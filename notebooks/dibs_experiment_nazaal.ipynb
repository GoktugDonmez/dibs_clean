{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e0e158",
   "metadata": {},
   "source": [
    "# Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d964658",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38285f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/dibs_experiment.ipynb\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Add project root to the Python path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from data.graph_data import generate_synthetic_data\n",
    "from models.dibs import grad_log_joint, log_joint, hard_gmat_from_z, bernoulli_soft_gmat, update_dibs_hparams\n",
    "from models.utils import acyclic_constr\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75147255",
   "metadata": {},
   "source": [
    "# CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28f613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:04:26,723 - INFO - Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    mlflow_experiment_name = \"DiBS Simple Experiment\"\n",
    "\n",
    "    # --- Data Generation ---\n",
    "    # 'simple_chain' or 'synthetic'\n",
    "    data_source = 'simple_chain'\n",
    "    \n",
    "    # Parameters for 'simple_chain'\n",
    "    num_samples = 50\n",
    "    obs_noise_std = 0.1\n",
    "\n",
    "    # Parameters for 'synthetic'\n",
    "    d_nodes = 4\n",
    "    graph_type = 'scale-free'\n",
    "    graph_params = {'p_edge': 0.70, 'm_edges':3}\n",
    "    synthetic_obs_noise_std = 0.1\n",
    "\n",
    "    # Particle and Model parameters\n",
    "    k_latent = 3\n",
    "    alpha_val = 0.05\n",
    "    beta_val = 1.0\n",
    "    tau_val = 1.0\n",
    "    theta_prior_sigma_val = 1.\n",
    "    n_grad_mc_samples = 64\n",
    "    n_nongrad_mc_samples = 64\n",
    "\n",
    "    # Training parameters\n",
    "    lr = 0.005\n",
    "    num_iterations = 1000\n",
    "    debug_print_iter = 100\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "\n",
    "log.info(f\"Running on device: {cfg.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed17e43",
   "metadata": {},
   "source": [
    "# Syntetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e74203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:04:26,760 - INFO - Using 'simple_chain' data source.\n",
      "2025-06-23 14:04:26,763 - INFO - Data generated with 3 nodes.\n"
     ]
    }
   ],
   "source": [
    "# notebooks/dibs_experiment.ipynb\n",
    "\n",
    "# ---- [Cell 3: Data Generation] ----\n",
    "# Generate data based on the selected `data_source` from the configuration.\n",
    "\n",
    "def generate_ground_truth_data_x1_x2_x3(num_samples, obs_noise_std, seed=None):\n",
    "    \"\"\"Generates data for the ground truth causal chain X1 -> X2 -> X3.\"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    D_nodes = 3\n",
    "    G_true = torch.zeros(D_nodes, D_nodes, dtype=torch.float32)\n",
    "    G_true[0, 1] = 1.0\n",
    "    G_true[1, 2] = 1.0\n",
    "    Theta_true = torch.zeros(D_nodes, D_nodes, dtype=torch.float32)\n",
    "    Theta_true[0, 1] = 2.0\n",
    "    Theta_true[1, 2] = -1.5\n",
    "    X_data = torch.zeros(num_samples, D_nodes)\n",
    "    X_data[:, 0] = torch.randn(num_samples) * obs_noise_std  # FIXME Should have variance obs_noise_std \n",
    "    noise_x2 = torch.randn(num_samples) * obs_noise_std\n",
    "    X_data[:, 1] = Theta_true[0, 1] * X_data[:, 0] + noise_x2\n",
    "    noise_x3 = torch.randn(num_samples) * obs_noise_std\n",
    "    X_data[:, 2] = Theta_true[1, 2] * X_data[:, 1] + noise_x3\n",
    "    return X_data, G_true, Theta_true\n",
    "\n",
    "if cfg.data_source == 'simple_chain':\n",
    "    log.info(\"Using 'simple_chain' data source.\")\n",
    "    data_x, graph_adj, graph_weights = generate_ground_truth_data_x1_x2_x3(\n",
    "        num_samples=cfg.num_samples,\n",
    "        obs_noise_std=cfg.obs_noise_std,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "    # Update d_nodes based on the simple chain's size\n",
    "    cfg.d_nodes = 3\n",
    "    \n",
    "elif cfg.data_source == 'synthetic':\n",
    "    log.info(\"Using 'synthetic' data source.\")\n",
    "    graph_adj, graph_weights, data_x = generate_synthetic_data(\n",
    "        n_samples=cfg.num_samples,\n",
    "        n_nodes=cfg.d_nodes,\n",
    "        graph_type=cfg.graph_type,\n",
    "        graph_params=cfg.graph_params,\n",
    "        noise_std=cfg.obs_noise_std\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown data_source: {cfg.data_source}\")\n",
    "\n",
    "data = {'x': data_x.to(cfg.device)}\n",
    "log.info(f\"Data generated with {cfg.d_nodes} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cf42b",
   "metadata": {},
   "source": [
    "# MLflow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4732d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:04:26,983 - INFO - Started MLflow run for experiment: 'DiBS Simple Experiment'\n"
     ]
    }
   ],
   "source": [
    "# End any existing active run before starting a new one\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "mlflow.set_experiment(cfg.mlflow_experiment_name)\n",
    "mlflow.start_run()\n",
    "\n",
    "# Log all hyperparameters from the Config class\n",
    "for param, value in vars(cfg).items():\n",
    "    if not param.startswith('__') and not callable(value):\n",
    "        mlflow.log_param(param, value)\n",
    "\n",
    "log.info(f\"Started MLflow run for experiment: '{cfg.mlflow_experiment_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604f2bc",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e13e0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/dibs_experiment.ipynb\n",
    "\n",
    "# ---- [Cell 4: Model and Particle Initialization] ----\n",
    "# We initialize the learnable parameters (particles) z and theta.\n",
    "\n",
    "def init_particle(d: int, k: int, device: str) -> dict:\n",
    "    return {\n",
    "        'z': torch.randn(d, k, 2, device=device),\n",
    "        'theta': torch.randn(d, d, device=device)\n",
    "    }\n",
    "\n",
    "particle = init_particle(cfg.d_nodes, cfg.k_latent, cfg.device)\n",
    "\n",
    "# Hparams dictionary, as used by the model functions\n",
    "sigma_z = (1.0 / math.sqrt(cfg.k_latent))\n",
    "hparams = {\n",
    "    \"alpha\": cfg.alpha_val,\n",
    "    \"beta\": cfg.beta_val,\n",
    "    \"alpha_base\":cfg.alpha_val,\n",
    "    \"beta_base\": cfg.beta_val,\n",
    "    \"tau\": cfg.tau_val,\n",
    "    \"sigma_z\": sigma_z,\n",
    "    \"sigma_obs_noise\": cfg.synthetic_obs_noise_std,\n",
    "    \"theta_prior_sigma\": cfg.theta_prior_sigma_val,\n",
    "    \"n_grad_mc_samples\": cfg.n_grad_mc_samples,\n",
    "    \"n_nongrad_mc_samples\": cfg.n_nongrad_mc_samples,\n",
    "    \"d\": cfg.d_nodes,\n",
    "    \"debug_print_iter\": cfg.debug_print_iter\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c598ab4",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Basic training loop using PyTorch optimizers with proper gradient hooking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b25ca0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5260, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4583, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4948, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4983, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5625, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5712, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5885, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6580, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5486, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4931, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5122, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5781, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4236, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6215, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5677, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5243, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5608, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5191, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4705, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3941, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6545, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5122, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4792, grad_fn=<MeanBackward1>)\n",
      "tensor(0.7569, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4774, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6476, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5052, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4497, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4323, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5590, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5226, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5625, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5764, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4201, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5208, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5729, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4462, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5347, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4878, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5660, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4757, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4844, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5885, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5590, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4948, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5469, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5590, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5573, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3681, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5000, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4688, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5799, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4896, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4913, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5052, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4271, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6424, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5156, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5087, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5608, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5469, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4705, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4618, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4809, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4236, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4497, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4306, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4931, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4028, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5538, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3767, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5278, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4705, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4583, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5451, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4306, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5260, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3785, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5677, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3993, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5434, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5642, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6597, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5069, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4670, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4965, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4948, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4809, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5417, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5087, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4601, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3472, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3819, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4444, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5590, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5660, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4427, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:26:48,889 - INFO - Iter 100: Z_norm=2.4661, Theta_norm=3.7427, log_joint=-167.4289, grad_Z_norm=3.5090e+00, grad_Theta_norm=6.1905e+00\n",
      "2025-06-23 14:26:48,890 - INFO -       grad_Theta (sample from iter 100):\n",
      "2025-06-23 14:26:48,892 - INFO -  tensor([[ 0.0000e+00,  6.0288e+00, -8.2731e-05],\n",
      "                 [ 8.9127e-03,  0.0000e+00,  0.0000e+00],\n",
      "                 [-4.6622e-05, -1.4055e+00,  0.0000e+00]], requires_grad=True)\n",
      "2025-06-23 14:26:48,893 - INFO -       Annealed: alpha=0.488, beta=190.325, tau=1.000\n",
      "2025-06-23 14:26:48,894 - INFO -       Current Edge Probs (from Z, alpha=0.488):\n",
      "2025-06-23 14:26:48,895 - INFO -  tensor([[0.0000, 0.4841, 0.6119],\n",
      "                 [0.5331, 0.0000, 0.2235],\n",
      "                 [0.5110, 0.5045, 0.0000]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5017)\n",
      "tensor(0.5330, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5313, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5434, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5885, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6406, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6441, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4410, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4358, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4549, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5191, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5660, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5156, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5000, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5642, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4948, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4375, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4149, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5208, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4861, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5139, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4792, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5000, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4392, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4601, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4757, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4184, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4427, grad_fn=<MeanBackward1>)\n",
      "tensor(0.2743, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6111, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4931, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4965, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4306, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4826, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3715, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3976, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4410, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4670, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4688, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6007, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5052, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3628, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5122, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5417, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5747, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4948, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4427, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4184, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4844, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3837, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5122, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3472, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4375, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4323, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5955, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4427, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5694, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4236, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4878, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5521, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5590, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5347, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5278, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4653, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5139, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5243, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4549, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5625, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5069, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5608, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4583, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4497, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3819, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5226, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4253, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5677, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4375, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5712, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3802, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6024, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4410, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5226, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4444, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3976, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4774, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4705, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5260, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4653, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4948, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4358, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4635, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5469, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4618, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4531, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5069, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4740, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5434, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4826, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:27:00,253 - INFO - Iter 200: Z_norm=2.7043, Theta_norm=3.7395, log_joint=-124.3563, grad_Z_norm=3.8265e+00, grad_Theta_norm=5.4608e+00\n",
      "2025-06-23 14:27:00,254 - INFO -       grad_Theta (sample from iter 200):\n",
      "2025-06-23 14:27:00,256 - INFO -  tensor([[ 0.0000e+00,  2.8589e+00, -1.4544e-05],\n",
      "                 [ 1.2834e-05,  0.0000e+00,  0.0000e+00],\n",
      "                 [-1.7558e-06,  4.6526e+00,  0.0000e+00]], requires_grad=True)\n",
      "2025-06-23 14:27:00,256 - INFO -       Annealed: alpha=0.952, beta=362.538, tau=1.000\n",
      "2025-06-23 14:27:00,257 - INFO -       Current Edge Probs (from Z, alpha=0.952):\n",
      "2025-06-23 14:27:00,258 - INFO -  tensor([[0.0000, 0.4755, 0.8494],\n",
      "                 [0.4637, 0.0000, 0.0523],\n",
      "                 [0.4970, 0.5048, 0.0000]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4601)\n",
      "tensor(0.4826, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6042, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5382, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5295, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3924, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4844, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4861, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6181, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4497, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6128, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5226, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5000, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3941, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4514, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4826, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4340, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4635, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5208, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5382, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4462, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5382, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4878, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4097, grad_fn=<MeanBackward1>)\n",
      "tensor(0.3993, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4410, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4392, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5573, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5087, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4983, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4219, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5069, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4358, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5573, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5035, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4219, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5087, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5017, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5642, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5122, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5503, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4601, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4427, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5035, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5278, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5260, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4184, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5017, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4740, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4497, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5347, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4774, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4757, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6788, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4201, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5625, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5694, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5938, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4253, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4844, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5000, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4913, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4635, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4583, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5243, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4635, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5816, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4740, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4410, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4583, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6285, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4566, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4931, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6302, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5712, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5260, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5781, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5503, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4427, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5330, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5278, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4410, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5122, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5156, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5972, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5382, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5208, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4601, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6007, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5365, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5260, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4618, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5712, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5087, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5104, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4931, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6215, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5781, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4861, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5694, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:27:11,280 - INFO - Iter 300: Z_norm=2.6345, Theta_norm=3.7548, log_joint=-192.0087, grad_Z_norm=1.7454e+00, grad_Theta_norm=1.5416e+00\n",
      "2025-06-23 14:27:11,282 - INFO -       grad_Theta (sample from iter 300):\n",
      "2025-06-23 14:27:11,283 - INFO -  tensor([[ 0.0000e+00,  1.1254e+00, -1.4544e-05],\n",
      "                 [ 4.4132e-06,  0.0000e+00,  0.0000e+00],\n",
      "                 [-7.5690e-07,  1.0536e+00,  0.0000e+00]], requires_grad=True)\n",
      "2025-06-23 14:27:11,283 - INFO -       Annealed: alpha=1.393, beta=518.364, tau=1.000\n",
      "2025-06-23 14:27:11,284 - INFO -       Current Edge Probs (from Z, alpha=1.393):\n",
      "2025-06-23 14:27:11,285 - INFO -  tensor([[0.0000, 0.6819, 0.9339],\n",
      "                 [0.5305, 0.0000, 0.0250],\n",
      "                 [0.4933, 0.5581, 0.0000]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4948)\n",
      "tensor(0.5729, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5712, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6858, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6372, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4167, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5677, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6042, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6736, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6302, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5156, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6875, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5764, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5521, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5712, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6111, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6076, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5382, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6094, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5434, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5035, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5903, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5955, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5382, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6302, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4861, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6406, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6528, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6354, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6337, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5226, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5868, grad_fn=<MeanBackward1>)\n",
      "tensor(0.7066, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6302, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6684, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5573, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5503, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5764, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4618, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5920, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5868, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5590, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5642, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5000, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5104, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6667, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5399, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5694, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5087, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6528, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6076, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5399, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5382, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5503, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5729, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5260, grad_fn=<MeanBackward1>)\n",
      "tensor(0.7031, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5642, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6701, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5417, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6181, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6354, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5208, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6372, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6111, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6233, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6076, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6233, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5486, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6094, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6059, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6215, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4948, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6875, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5069, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5503, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6233, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5747, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6146, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6580, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5885, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6823, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5729, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5903, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5417, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6875, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6458, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6042, grad_fn=<MeanBackward1>)\n",
      "tensor(0.7778, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6111, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6458, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6892, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4740, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5503, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5851, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5955, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4635, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6007, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6771, grad_fn=<MeanBackward1>)\n",
      "tensor(0.4722, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6823, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:27:22,386 - INFO - Iter 400: Z_norm=2.4669, Theta_norm=3.7646, log_joint=-331.9135, grad_Z_norm=5.3650e+00, grad_Theta_norm=1.5851e-01\n",
      "2025-06-23 14:27:22,387 - INFO -       grad_Theta (sample from iter 400):\n",
      "2025-06-23 14:27:22,389 - INFO -  tensor([[ 0.0000e+00,  1.1720e-01, -2.3842e-06],\n",
      "                 [ 4.7528e-06,  0.0000e+00,  0.0000e+00],\n",
      "                 [-5.2687e-07,  1.0673e-01,  0.0000e+00]], requires_grad=True)\n",
      "2025-06-23 14:27:22,390 - INFO -       Annealed: alpha=1.813, beta=659.360, tau=1.000\n",
      "2025-06-23 14:27:22,391 - INFO -       Current Edge Probs (from Z, alpha=1.813):\n",
      "2025-06-23 14:27:22,393 - INFO -  tensor([[0.0000, 0.8119, 0.9493],\n",
      "                 [0.6028, 0.0000, 0.0274],\n",
      "                 [0.4527, 0.6900, 0.0000]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6215)\n",
      "tensor(0.6024, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6128, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5868, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6181, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5816, grad_fn=<MeanBackward1>)\n",
      "tensor(0.7083, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5764, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6302, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6441, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6597, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5469, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6285, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5208, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5486, grad_fn=<MeanBackward1>)\n",
      "tensor(0.7778, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6354, grad_fn=<MeanBackward1>)\n",
      "tensor(0.7049, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5972, grad_fn=<MeanBackward1>)\n",
      "tensor(0.5990, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6215, grad_fn=<MeanBackward1>)\n",
      "tensor(0.6788, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 102\u001b[0m\n\u001b[1;32m     98\u001b[0m                 mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_joint\u001b[39m\u001b[38;5;124m\"\u001b[39m, lj_val, step\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m particle,hparams\n\u001b[0;32m--> 102\u001b[0m particle, hparams_final \u001b[38;5;241m=\u001b[39m \u001b[43mbasic_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m, in \u001b[0;36mbasic_training_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Get gradients of the log-joint\u001b[39;00m\n\u001b[1;32m     45\u001b[0m params_for_grad \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m\"\u001b[39m: particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(t))}\n\u001b[0;32m---> 46\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_log_joint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_for_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Hook gradients into PyTorch's gradient system\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# This is crucial: assign the computed gradients to .grad attributes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#particle['z'].grad = grads['z']\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#particle['theta'].grad = grads['theta']\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# use gradient ascent \u001b[39;00m\n\u001b[1;32m     54\u001b[0m particle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mgrads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/projects/dibs_clean/models/dibs.py:253\u001b[0m, in \u001b[0;36mgrad_log_joint\u001b[0;34m(params, data, hparams)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgrad_log_joint\u001b[39m(params: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], data: Dict[\u001b[38;5;28mstr\u001b[39m, Any], hparams: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    251\u001b[0m     z, theta \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 253\u001b[0m     grad_z \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_z_log_joint_gumbel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     grad_theta \u001b[38;5;241m=\u001b[39m grad_theta_log_joint(z\u001b[38;5;241m.\u001b[39mdetach(), theta, data, hparams)\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: grad_z, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m\"\u001b[39m: grad_theta}\n",
      "File \u001b[0;32m~/projects/dibs_clean/models/dibs.py:118\u001b[0m, in \u001b[0;36mgrad_z_log_joint_gumbel\u001b[0;34m(z, theta, data, hparams)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_grad_mc_samples\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    116\u001b[0m     g_soft \u001b[38;5;241m=\u001b[39m gumbel_soft_gmat(z, hparams)\n\u001b[0;32m--> 118\u001b[0m     log_lik_val \u001b[38;5;241m=\u001b[39m \u001b[43mlog_full_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_soft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_const\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     theta_eff_mc \u001b[38;5;241m=\u001b[39m theta_const \u001b[38;5;241m*\u001b[39m g_soft\n\u001b[1;32m    120\u001b[0m     log_theta_prior_val \u001b[38;5;241m=\u001b[39m log_theta_prior(theta_eff_mc, hparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta_prior_sigma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n",
      "File \u001b[0;32m~/projects/dibs_clean/models/dibs.py:65\u001b[0m, in \u001b[0;36mlog_full_likelihood\u001b[0;34m(data, soft_gmat, theta, hparams)\u001b[0m\n\u001b[1;32m     63\u001b[0m pred_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x_data, effective_W)\n\u001b[1;32m     64\u001b[0m sigma_obs \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma_obs_noise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlog_gaussian_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/dibs_clean/models/dibs.py:16\u001b[0m, in \u001b[0;36mlog_gaussian_likelihood\u001b[0;34m(x, pred_mean, sigma)\u001b[0m\n\u001b[1;32m     14\u001b[0m residuals \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m pred_mean\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# log_prob = -0.5 * (np.log(2 * np.pi) + 2 * torch.log(sigma_tensor) + (residuals / sigma_tensor) ** 2)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m sigma_tensor\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(log_prob)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def basic_training_loop():\n",
    "\n",
    "    def init_particle(d: int, k: int, device: str) -> dict:\n",
    "        return {\n",
    "            'z': torch.randn(d, k, 2, device=device),\n",
    "            'theta': torch.randn(d, d, device=device)\n",
    "        }\n",
    "\n",
    "    particle = init_particle(cfg.d_nodes, cfg.k_latent, cfg.device)\n",
    "\n",
    "    # Hparams dictionary, as used by the model functions\n",
    "    sigma_z = (1.0 / math.sqrt(cfg.k_latent))\n",
    "    hparams = {\n",
    "    \"alpha\": cfg.alpha_val,\n",
    "    \"beta\": cfg.beta_val,\n",
    "    \"alpha_base\":cfg.alpha_val,\n",
    "    \"beta_base\": cfg.beta_val,\n",
    "    \"tau\": cfg.tau_val,\n",
    "    \"sigma_z\": sigma_z,\n",
    "    \"sigma_obs_noise\": cfg.synthetic_obs_noise_std,\n",
    "    \"theta_prior_sigma\": cfg.theta_prior_sigma_val,\n",
    "    \"n_grad_mc_samples\": cfg.n_grad_mc_samples,\n",
    "    \"n_nongrad_mc_samples\": cfg.n_nongrad_mc_samples,\n",
    "    \"d\": cfg.d_nodes,\n",
    "    \"debug_print_iter\": cfg.debug_print_iter\n",
    "}\n",
    "\n",
    "    # Initialize PyTorch optimizers for z and theta parameters\n",
    "    optimizer_z = torch.optim.RMSprop([particle['z']], lr=cfg.lr) # CHANGED Added requires_grad to tensors here\n",
    "    optimizer_theta = torch.optim.RMSprop([particle['theta']], lr=cfg.lr)\n",
    "\n",
    "    # Training loop using PyTorch optimizers with gradient hooking\n",
    "    for t in range(1, cfg.num_iterations + 1):\n",
    "        hparams = update_dibs_hparams(hparams, t)\n",
    "        \n",
    "        # Clear gradients using optimizers\n",
    "        optimizer_z.zero_grad()\n",
    "        optimizer_theta.zero_grad()\n",
    "        \n",
    "        # Set requires_grad for the particles\n",
    "        particle['z'].requires_grad_(True)\n",
    "        particle['theta'].requires_grad_(True)\n",
    "\n",
    "        # Get gradients of the log-joint\n",
    "        params_for_grad = {\"z\": particle['z'], \"theta\": particle['theta'], \"t\": torch.tensor(float(t))}\n",
    "        grads = grad_log_joint(params_for_grad, data, hparams)\n",
    "\n",
    "        # Hook gradients into PyTorch's gradient system\n",
    "        # This is crucial: assign the computed gradients to .grad attributes\n",
    "        #particle['z'].grad = grads['z']\n",
    "        #particle['theta'].grad = grads['theta']\n",
    "        \n",
    "        # use gradient ascent \n",
    "        particle['z'].grad = -grads['z']\n",
    "        particle['theta'].grad = -grads['theta']\n",
    "\n",
    "        all_params = [particle['z'], particle['theta']]\n",
    "        #torch.nn.utils.clip_grad_norm_(all_params, max_norm=10.0) # max_norm is a hyperparameter to tune\n",
    "\n",
    "\n",
    "        # Use PyTorch optimizers to update parameters\n",
    "        optimizer_z.step()\n",
    "        optimizer_theta.step()\n",
    "\n",
    "        # Logging\n",
    "        if t % cfg.debug_print_iter == 0 or t == cfg.num_iterations:\n",
    "            with torch.no_grad():\n",
    "                # Calculate required values for logging\n",
    "                lj_val = log_joint(params_for_grad, data, hparams).item()\n",
    "                z_norm = torch.linalg.norm(particle['z']).item()\n",
    "                theta_norm = torch.linalg.norm(particle['theta']).item()\n",
    "                grad_z_norm = torch.linalg.norm(grads['z']).item()\n",
    "                grad_theta_norm = torch.linalg.norm(grads['theta']).item()\n",
    "                \n",
    "                # Main log entry\n",
    "                log.info(f\"Iter {t}: Z_norm={z_norm:.4f}, Theta_norm={theta_norm:.4f}, \"\n",
    "                         f\"log_joint={lj_val:.4f}, grad_Z_norm={grad_z_norm:.4e}, \"\n",
    "                         f\"grad_Theta_norm={grad_theta_norm:.4e}\")\n",
    "\n",
    "                # Gradient sample\n",
    "                log.info(f\"      grad_Theta (sample from iter {t}):\")\n",
    "                # Ensure the logged tensor fits the console nicely\n",
    "                grad_sample_str = str(grads['theta'][:3, :3]).replace(\"\\n\", \"\\n         \")\n",
    "                log.info(f\" {grad_sample_str}\")\n",
    "                \n",
    "                # Annealed parameters\n",
    "                log.info(f\"      Annealed: alpha={hparams['alpha']:.3f}, \"\n",
    "                         f\"beta={hparams['beta']:.3f}, tau={hparams['tau']:.3f}\")\n",
    "                         \n",
    "                # Edge probabilities\n",
    "                log.info(f\"      Current Edge Probs (from Z, alpha={hparams['alpha']:.3f}):\")\n",
    "                edge_probs = bernoulli_soft_gmat(particle['z'], hparams)\n",
    "                #Ensure the logged tensor fits the console nicely\n",
    "                edge_probs_str = str(edge_probs).replace(\"\\n\", \"\\n         \")\n",
    "                log.info(f\" {edge_probs_str}\")\n",
    "\n",
    "                # Log to MLflow\n",
    "                mlflow.log_metric(\"log_joint\", lj_val, step=t)\n",
    "    return particle,hparams\n",
    "\n",
    "\n",
    "particle, hparams_final = basic_training_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7555597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final soft matrix (2 decimals):\n",
      "tensor([[0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9200],\n",
      "        [0.0100, 0.0000, 0.0000]])\n",
      "\n",
      "Hard graph from Z:\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Learned Theta:\n",
      "tensor([[-1.4723,  1.9488,  0.5655],\n",
      "        [-0.9521, -0.2732, -1.4886],\n",
      "        [-0.2534, -0.2550, -0.4564]], requires_grad=True)\n",
      "\n",
      "Learned Theta * Learned Graph:\n",
      "tensor([[-0.0000,  1.9488,  0.0000],\n",
      "        [-0.0000, -0.0000, -1.4886],\n",
      "        [-0.0000, -0.0000, -0.0000]], grad_fn=<MulBackward0>)\n",
      "\n",
      "True graph (graph_adj):\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "True weights (graph_weights):\n",
      "tensor([[ 0.0000,  2.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -1.5000],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Final soft matrix (2 decimals):\")\n",
    "soft_gmat = bernoulli_soft_gmat(particle['z'], hparams_final)\n",
    "soft_gmat_rounded = torch.round(soft_gmat * 100) / 100\n",
    "print(soft_gmat_rounded)\n",
    "\n",
    "print(f\"\\nHard graph from Z:\")\n",
    "hard_graph = hard_gmat_from_z(particle['z'], hparams_final['alpha'])\n",
    "print(hard_graph)\n",
    "\n",
    "print(f\"\\nLearned Theta:\")\n",
    "print(particle['theta'])\n",
    "\n",
    "print(f\"\\nLearned Theta * Learned Graph:\")\n",
    "weighted_learned = hard_graph * particle['theta']\n",
    "print(weighted_learned)\n",
    "\n",
    "print(f\"\\nTrue graph (graph_adj):\")\n",
    "print(graph_adj)\n",
    "\n",
    "print(f\"\\nTrue weights (graph_weights):\")\n",
    "print(graph_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a95b7b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f7c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:06:10,054 - INFO - MLflow run finished and artifacts logged.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Final Results ===========\n",
      "Edge probabilities:\n",
      "tensor([[0.0000, 0.5199, 0.4791],\n",
      "        [0.4691, 0.0000, 0.5078],\n",
      "        [0.4866, 0.4709, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=========== Final Results ===========\")\n",
    "# print(\"Final probs:\")\n",
    "print(\"Edge probabilities:\")\n",
    "edge_probs = bernoulli_soft_gmat(particle['z'], hparams)\n",
    "print(edge_probs)\n",
    "\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n",
    "log.info(\"MLflow run finished and artifacts logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43d6e3",
   "metadata": {},
   "source": [
    "# Enhanced traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f2b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enchaced_training_loop():\n",
    "    # ========================================================================\n",
    "    # ENHANCED TRAINING LOOP WITH COMPREHENSIVE MONITORING\n",
    "    # ========================================================================\n",
    "    # This version extends the basic training loop with:\n",
    "    # 1. Edge probability tracking during training\n",
    "    # 2. Parameter norm monitoring (Z and Theta norms)\n",
    "    # 3. Gradient norm computation and optional clipping\n",
    "    # 4. Structural Hamming Distance (SHD) computation\n",
    "    # 5. Enhanced MLflow logging with multiple metrics\n",
    "    # 6. Numerical stability checks (NaN/Inf detection)\n",
    "    # 7. Weighted theta matrix visualization (G * Theta)\n",
    "    # 8. PyTorch optimizers with proper gradient hooking\n",
    "\n",
    "    log.info(\"\\n\" + \"=\"*80)\n",
    "    log.info(\"STARTING ENHANCED TRAINING LOOP WITH DETAILED MONITORING\")\n",
    "    log.info(\"=\"*80)\n",
    "\n",
    "    # --- Enhanced Training Configuration ---\n",
    "    # Reset particles for a clean experiment\n",
    "    particle_enhanced = init_particle(cfg.d_nodes, cfg.k_latent, cfg.device)\n",
    "\n",
    "    # Enhanced training hyperparameters\n",
    "    num_iterations_enhanced = cfg.num_iterations  # Match the final iteration from your output\n",
    "    lr_z_enhanced = cfg.lr          # Separate learning rate for Z\n",
    "    lr_theta_enhanced = cfg.lr      # Separate learning rate for Theta  \n",
    "    max_grad_norm = 10000       # Gradient clipping threshold\n",
    "    logging_interval = 50          # Log every N iterations (more frequent for better monitoring)\n",
    "\n",
    "    # Initialize PyTorch optimizers for enhanced training\n",
    "    optimizer_z_enhanced = torch.optim.Adam([particle_enhanced['z']], lr=lr_z_enhanced)\n",
    "    optimizer_theta_enhanced = torch.optim.Adam([particle_enhanced['theta']], lr=lr_theta_enhanced)\n",
    "\n",
    "    # Log initial configuration\n",
    "    log.info(f\"Enhanced Training Configuration:\")\n",
    "    log.info(f\"  Iterations: {num_iterations_enhanced}\")\n",
    "    log.info(f\"  Learning rates - Z: {lr_z_enhanced}, Theta: {lr_theta_enhanced}\")\n",
    "    log.info(f\"  Gradient clipping threshold: {max_grad_norm}\")\n",
    "    log.info(f\"  Logging interval: {logging_interval}\")\n",
    "    log.info(f\"  Initial Z norm: {particle_enhanced['z'].norm().item():.4f}\")\n",
    "    log.info(f\"  Initial Theta norm: {particle_enhanced['theta'].norm().item():.4f}\")\n",
    "\n",
    "    # --- Main Enhanced Training Loop ---\n",
    "    for t in range(1, num_iterations_enhanced + 1):\n",
    "        hparams = update_dibs_hparams(hparams, t)\n",
    "        \n",
    "        # ------------------------------------------------\n",
    "        # STEP 1: Parameter Setup and Gradient Clearing\n",
    "        # ------------------------------------------------\n",
    "        # Clear gradients using optimizers\n",
    "        optimizer_z_enhanced.zero_grad()\n",
    "        optimizer_theta_enhanced.zero_grad()\n",
    "        \n",
    "        # Enable gradient computation for both Z and Theta parameters\n",
    "        particle_enhanced['z'].requires_grad_(True)\n",
    "        particle_enhanced['theta'].requires_grad_(True)\n",
    "        \n",
    "        # ------------------------------------------------\n",
    "        # STEP 2: Forward Pass - Compute Log-Joint and Gradients\n",
    "        # ------------------------------------------------\n",
    "        # Prepare parameters dictionary for gradient computation\n",
    "        params_for_grad = {\n",
    "            \"z\": particle_enhanced['z'], \n",
    "            \"theta\": particle_enhanced['theta'], \n",
    "            \"t\": torch.tensor(float(t))  # Time step for annealing\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Compute log-joint probability (objective function)\n",
    "            lj_val = log_joint(params_for_grad, data, hparams).item()\n",
    "            \n",
    "            # Compute gradients of log-joint w.r.t. Z and Theta\n",
    "            grads = grad_log_joint(params_for_grad, data, hparams)\n",
    "            grad_z = grads['z']\n",
    "            grad_theta = grads['theta']\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 3: Gradient Analysis and Clipping\n",
    "            # ------------------------------------------------\n",
    "            # Compute gradient norms BEFORE clipping for monitoring\n",
    "            grad_z_norm_original = grad_z.norm().item()\n",
    "            grad_theta_norm_original = grad_theta.norm().item()\n",
    "            \n",
    "            # Apply gradient clipping if gradients exceed threshold\n",
    "            grad_z_clipped = False\n",
    "            grad_theta_clipped = False\n",
    "            \n",
    "            if grad_z_norm_original > max_grad_norm:\n",
    "                grad_z = grad_z * (max_grad_norm / grad_z_norm_original)\n",
    "                grad_z_clipped = True\n",
    "                \n",
    "            if grad_theta_norm_original > max_grad_norm:\n",
    "                grad_theta = grad_theta * (max_grad_norm / grad_theta_norm_original)\n",
    "                grad_theta_clipped = True\n",
    "                \n",
    "            # Compute final gradient norms AFTER clipping\n",
    "            grad_z_norm_final = grad_z.norm().item()\n",
    "            grad_theta_norm_final = grad_theta.norm().item()\n",
    "                \n",
    "        except Exception as e:\n",
    "            log.error(f\"Error in forward pass at iteration {t}: {e}\")\n",
    "            break\n",
    "        \n",
    "        # ------------------------------------------------\n",
    "        # STEP 4: Parameter Update using PyTorch Optimizers\n",
    "        # ------------------------------------------------\n",
    "        # Hook gradients into PyTorch's gradient system\n",
    "        # This is crucial: assign the computed gradients to .grad attributes\n",
    "        particle_enhanced['z'].grad = grad_z\n",
    "        particle_enhanced['theta'].grad = grad_theta\n",
    "        \n",
    "        # Use PyTorch optimizers to update parameters\n",
    "        optimizer_z_enhanced.step()\n",
    "        optimizer_theta_enhanced.step()\n",
    "        \n",
    "        # ------------------------------------------------\n",
    "        # STEP 5: Comprehensive Logging and Monitoring\n",
    "        # ------------------------------------------------\n",
    "        if t % logging_interval == 0 or t == 1 or t == num_iterations_enhanced:\n",
    "            \n",
    "            # Compute current parameter norms\n",
    "            z_norm = particle_enhanced['z'].norm().item()\n",
    "            theta_norm = particle_enhanced['theta'].norm().item()\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 5a: Edge Probability Analysis\n",
    "            # ------------------------------------------------\n",
    "            with torch.no_grad():\n",
    "                # Soft edge probabilities (continuous values)\n",
    "                edge_probs = bernoulli_soft_gmat(particle_enhanced['z'], hparams).detach().cpu()\n",
    "                \n",
    "                # Hard graph (binary adjacency matrix after thresholding)\n",
    "                hard_graph = hard_gmat_from_z(particle_enhanced['z'], hparams['alpha']).detach().cpu()\n",
    "                \n",
    "                # Weighted theta matrix (element-wise multiplication of G and Theta)\n",
    "                theta_cpu = particle_enhanced['theta'].detach().cpu()\n",
    "                weighted_theta = hard_graph * theta_cpu\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 5b: Ground Truth Comparison\n",
    "            # ------------------------------------------------\n",
    "            # Compute Structural Hamming Distance (SHD) if ground truth is available\n",
    "            shd = float('nan')\n",
    "            if 'graph_adj' in locals():\n",
    "                # SHD = number of edge differences between learned and true graph\n",
    "                shd = torch.sum(torch.abs(hard_graph.int() - graph_adj.int())).item()\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 5c: Concise Console Logging (Updated Format)\n",
    "            # ------------------------------------------------\n",
    "            # Implement beta annealing (beta increases with iteration for better convergence)\n",
    "            current_beta = cfg.beta_val + t * 0.001  # Annealing: beta increases over time\n",
    "            hparams['beta'] = current_beta  # Update beta in hparams\n",
    "            \n",
    "            # Concise logging format matching the target output\n",
    "            log.info(f\"Iter {t}: Z_norm={z_norm:.4f}, Theta_norm={theta_norm:.4f}, log_joint={lj_val:.4f}, grad_Z_norm={grad_z_norm_original:.4e}, grad_Theta_norm={grad_theta_norm_original:.4e}\")\n",
    "            \n",
    "            # Show a sample of the current grad_Theta matrix\n",
    "            log.info(f\"    grad_Theta (sample from iter {t}):\")\n",
    "            log.info(f\"{grad_theta.detach().cpu()}\")\n",
    "            \n",
    "            # Show annealed hyperparameters\n",
    "            current_alpha = hparams.get('alpha', cfg.alpha_val)\n",
    "            current_tau = hparams.get('tau', cfg.tau_val)\n",
    "            log.info(f\"    Annealed: alpha={current_alpha:.3f}, beta={current_beta:.3f}, tau={current_tau:.3f}\")\n",
    "            \n",
    "            # Show current edge probabilities\n",
    "            log.info(f\"    Current Edge Probs (from Z, alpha={current_alpha:.3f}):\")\n",
    "            log.info(f\"{edge_probs}\")\n",
    "            \n",
    "            # Edge probability statistics for MLflow logging\n",
    "            max_edge_prob = edge_probs.max().item()\n",
    "            mean_edge_prob = edge_probs.mean().item()\n",
    "            num_edges_hard = hard_graph.sum().item()\n",
    "            \n",
    "            # Ground truth comparison\n",
    "            if not math.isnan(shd):\n",
    "                pass  # Skip detailed SHD logging during training for cleaner output\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 5e: Enhanced MLflow Logging\n",
    "            # ------------------------------------------------\n",
    "            # Log all metrics to MLflow for experiment tracking\n",
    "            mlflow.log_metric(\"enhanced_log_joint\", lj_val, step=t)\n",
    "            mlflow.log_metric(\"z_norm\", z_norm, step=t)\n",
    "            mlflow.log_metric(\"theta_norm\", theta_norm, step=t)\n",
    "            mlflow.log_metric(\"grad_z_norm_original\", grad_z_norm_original, step=t)\n",
    "            mlflow.log_metric(\"grad_theta_norm_original\", grad_theta_norm_original, step=t)\n",
    "            mlflow.log_metric(\"grad_z_norm_final\", grad_z_norm_final, step=t)\n",
    "            mlflow.log_metric(\"grad_theta_norm_final\", grad_theta_norm_final, step=t)\n",
    "            mlflow.log_metric(\"max_edge_prob\", max_edge_prob, step=t)\n",
    "            mlflow.log_metric(\"mean_edge_prob\", mean_edge_prob, step=t)\n",
    "            mlflow.log_metric(\"num_hard_edges\", num_edges_hard, step=t)\n",
    "            \n",
    "            if not math.isnan(shd):\n",
    "                mlflow.log_metric(\"structural_hamming_distance\", shd, step=t)\n",
    "            \n",
    "            # Log boolean indicators as metrics\n",
    "            mlflow.log_metric(\"z_gradient_clipped\", float(grad_z_clipped), step=t)\n",
    "            mlflow.log_metric(\"theta_gradient_clipped\", float(grad_theta_clipped), step=t)\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # STEP 5f: Numerical Stability Checks\n",
    "            # ------------------------------------------------\n",
    "            # Check for NaN or Inf values that could break training\n",
    "            z_has_nan = torch.isnan(particle_enhanced['z']).any()\n",
    "            z_has_inf = torch.isinf(particle_enhanced['z']).any()\n",
    "            theta_has_nan = torch.isnan(particle_enhanced['theta']).any()\n",
    "            theta_has_inf = torch.isinf(particle_enhanced['theta']).any()\n",
    "            \n",
    "            if z_has_nan or theta_has_nan:\n",
    "                log.error(\"!!! NaN DETECTED IN PARAMETERS - STOPPING TRAINING !!!\")\n",
    "                log.error(f\"Z has NaN: {z_has_nan}, Theta has NaN: {theta_has_nan}\")\n",
    "                break\n",
    "                \n",
    "            if z_has_inf or theta_has_inf:\n",
    "                log.error(\"!!! INFINITY DETECTED IN PARAMETERS - STOPPING TRAINING !!!\")\n",
    "                log.error(f\"Z has Inf: {z_has_inf}, Theta has Inf: {theta_has_inf}\")\n",
    "                break\n",
    "\n",
    "    # ========================================================================\n",
    "    # ENHANCED TRAINING COMPLETION AND FINAL ANALYSIS\n",
    "    # ========================================================================\n",
    "\n",
    "    log.info(\"\\n\" + \"=\"*80)\n",
    "    log.info(\"ENHANCED TRAINING LOOP COMPLETED\")\n",
    "    log.info(\"=\"*80)\n",
    "\n",
    "    # Compute final graph structures\n",
    "    with torch.no_grad():\n",
    "        final_edge_probs_enhanced = bernoulli_soft_gmat(particle_enhanced['z'], hparams).detach().cpu()\n",
    "        final_hard_graph_enhanced = hard_gmat_from_z(particle_enhanced['z'], hparams['alpha']).detach().cpu()\n",
    "        final_theta_enhanced = particle_enhanced['theta'].detach().cpu()\n",
    "        final_weighted_theta_enhanced = final_hard_graph_enhanced * final_theta_enhanced\n",
    "\n",
    "    # Final comparison with ground truth - matching the target output format\n",
    "    log.info(\"\\n        --- Comparison with Ground Truth ---\")\n",
    "    log.info(f\"Final G_learned_hard:\")\n",
    "    log.info(f\"{final_hard_graph_enhanced.int()}\")\n",
    "    log.info(\"**************************************************\")\n",
    "\n",
    "    # Create final weighted matrix using learned hard graph and learned theta\n",
    "    log.info(f\"Final G_learned_hard * Theta_learned:\")\n",
    "    log.info(f\"{final_weighted_theta_enhanced}\")\n",
    "\n",
    "    # Additional analysis - show ground truth comparison if available\n",
    "    if 'graph_adj' in locals():\n",
    "        final_shd = torch.sum(torch.abs(final_hard_graph_enhanced.int() - graph_adj.int())).item()\n",
    "        log.info(f\"\\nFinal Structural Hamming Distance: {final_shd}\")\n",
    "        \n",
    "        log.info(f\"\\nDetailed Comparison:\")\n",
    "        log.info(f\"Ground Truth Graph:\")\n",
    "        log.info(f\"{graph_adj.int()}\")\n",
    "        log.info(f\"Learned Graph:\")\n",
    "        log.info(f\"{final_hard_graph_enhanced.int()}\")\n",
    "        \n",
    "        if 'graph_weights' in locals():\n",
    "            log.info(f\"Ground Truth Theta:\")\n",
    "            log.info(f\"{graph_weights}\")\n",
    "            log.info(f\"Learned Theta:\")\n",
    "            log.info(f\"{final_theta_enhanced}\")\n",
    "            \n",
    "            # Show weighted matrices comparison\n",
    "            ground_truth_weighted = graph_adj * graph_weights\n",
    "            log.info(f\"Ground Truth G * Theta:\")\n",
    "            log.info(f\"{ground_truth_weighted}\")\n",
    "\n",
    "    log.info(\"\\nEnhanced training analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2276644",
   "metadata": {},
   "source": [
    "# Training Methods Comparison\n",
    "Compare results between the basic training loop and the enhanced training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea688a7",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c723ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 14:06:10,164 - INFO - \n",
      "================================================================================\n",
      "2025-06-23 14:06:10,166 - INFO - FINAL RESULTS FROM ENHANCED TRAINING\n",
      "2025-06-23 14:06:10,167 - INFO - ================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'particle_enhanced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Use results from enhanced training loop\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m final_graph_enhanced \u001b[38;5;241m=\u001b[39m hard_gmat_from_z(\u001b[43mparticle_enhanced\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m], hparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     12\u001b[0m edge_probs_enhanced \u001b[38;5;241m=\u001b[39m bernoulli_soft_gmat(particle_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m], hparams)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     13\u001b[0m theta_enhanced \u001b[38;5;241m=\u001b[39m particle_enhanced[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'particle_enhanced' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# FINAL RESULTS: Matrix Outputs (Enhanced Training)\n",
    "# ========================================================================\n",
    "# Display the final learned matrices from the enhanced training loop\n",
    "\n",
    "log.info(\"\\n\" + \"=\"*80)\n",
    "log.info(\"FINAL RESULTS FROM ENHANCED TRAINING\")\n",
    "log.info(\"=\"*80)\n",
    "\n",
    "# Use results from enhanced training loop\n",
    "final_graph_enhanced = hard_gmat_from_z(particle_enhanced['z'], hparams['alpha']).detach().cpu()\n",
    "edge_probs_enhanced = bernoulli_soft_gmat(particle_enhanced['z'], hparams).detach().cpu()\n",
    "theta_enhanced = particle_enhanced['theta'].detach().cpu()\n",
    "weighted_theta_enhanced = final_graph_enhanced * theta_enhanced\n",
    "\n",
    "log.info(f\"\\n1. LEARNED EDGE PROBABILITIES (Soft Graph):\")\n",
    "log.info(f\"{edge_probs_enhanced}\")\n",
    "\n",
    "log.info(f\"\\n2. LEARNED HARD GRAPH (Binary Adjacency Matrix):\")\n",
    "log.info(f\"{final_graph_enhanced.int()}\")\n",
    "\n",
    "log.info(f\"\\n3. LEARNED THETA MATRIX (Edge Weights):\")\n",
    "log.info(f\"{theta_enhanced}\")\n",
    "\n",
    "log.info(f\"\\n4. FINAL WEIGHTED GRAPH (G ⊙ Theta):\")\n",
    "log.info(f\"{weighted_theta_enhanced}\")\n",
    "\n",
    "# Ground truth comparison\n",
    "if 'graph_adj' in locals() and 'graph_weights' in locals():\n",
    "    log.info(f\"\\n\" + \"-\"*60)\n",
    "    log.info(\"GROUND TRUTH COMPARISON\")\n",
    "    log.info(\"-\"*60)\n",
    "    \n",
    "    log.info(f\"\\nGround Truth Graph:\")\n",
    "    log.info(f\"{graph_adj.int()}\")\n",
    "    \n",
    "    log.info(f\"\\nGround Truth Weights:\")\n",
    "    log.info(f\"{graph_weights}\")\n",
    "    \n",
    "    ground_truth_weighted = graph_adj * graph_weights\n",
    "    log.info(f\"\\nGround Truth Weighted (G_true ⊙ Theta_true):\")\n",
    "    log.info(f\"{ground_truth_weighted}\")\n",
    "    \n",
    "    # Compute final metrics\n",
    "    final_shd = torch.sum(torch.abs(final_graph_enhanced.int() - graph_adj.int())).item()\n",
    "    log.info(f\"\\nStructural Hamming Distance (SHD): {final_shd}\")\n",
    "    \n",
    "    # Check if structure is correctly recovered\n",
    "    structure_match = torch.equal(final_graph_enhanced.int(), graph_adj.int())\n",
    "    log.info(f\"Perfect Structure Recovery: {structure_match}\")\n",
    "\n",
    "# MLflow logging for final results\n",
    "mlflow.log_metric(\"final_z_norm\", particle_enhanced['z'].norm().item())\n",
    "mlflow.log_metric(\"final_theta_norm\", particle_enhanced['theta'].norm().item())\n",
    "mlflow.log_metric(\"final_max_edge_prob\", edge_probs_enhanced.max().item())\n",
    "mlflow.log_metric(\"final_mean_edge_prob\", edge_probs_enhanced.mean().item())\n",
    "mlflow.log_metric(\"final_num_edges\", final_graph_enhanced.sum().item())\n",
    "\n",
    "if 'graph_adj' in locals():\n",
    "    mlflow.log_metric(\"final_shd\", final_shd)\n",
    "    mlflow.log_metric(\"perfect_structure_recovery\", float(structure_match))\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n",
    "log.info(\"\\nFinal results logging complete and MLflow run ended!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4eb5b6",
   "metadata": {},
   "source": [
    "# LOGSUMEXP ISSUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for manual_stable_gradient2 with Gaussian log densities\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def logsumexp_v1(log_tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\n",
    "    M = log_tensor.shape[0]\n",
    "    logM = torch.log(torch.tensor(M, dtype=log_tensor.dtype, device=log_tensor.device))\n",
    "\n",
    "    \n",
    "    log_sum_exp = torch.logsumexp(log_tensor, dim=0)\n",
    "\n",
    "    total = log_sum_exp - logM\n",
    "    return total # torch.exp(total)\n",
    "\n",
    "def manual_stable_gradient(log_p_tensor: torch.Tensor, grad_p_tensor: torch.Tensor) -> torch.Tensor:\n",
    "# uses the logsumexp_v1 function to compute the stable gradient\n",
    "\n",
    "    print(f'log density values and shape: {log_p_tensor}, {log_p_tensor.shape}')\n",
    "    log_density_lse = torch.exp(torch.logsumexp(log_p_tensor, dim=0) - log_p_tensor.shape[0])  # logsumexp_v1(log_p_tensor)\n",
    "    # logsumexp_v1(log_p_tensor)\n",
    "    print(f'log density lse value: {log_density_lse}, shape: {log_density_lse.shape}')\n",
    "\n",
    "    print('-' * 50)\n",
    "    print(f'grad density values and shape: {grad_p_tensor}, {grad_p_tensor.shape}')\n",
    "    grad_lse = logsumexp_v1(grad_p_tensor)\n",
    "    print(f'grad density lse value: {grad_lse}, shape: {grad_lse.shape}')\n",
    "\n",
    "    return torch.exp(logsumexp_v1(grad_p_tensor) - logsumexp_v1(log_p_tensor)) # grad_lse / log_density_lse[:, None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_softmax_vs_manual_logsumexp():\n",
    "    \"\"\"\n",
    "    Compare softmax weights with manual logsumexp computation to verify equivalence\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPARING SOFTMAX VS MANUAL LOGSUMEXP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Parameters\n",
    "    n_samples = 5\n",
    "    theta_shape = (3, 3)\n",
    "    \n",
    "    # Generate log densities from normal distribution\n",
    "    log_densities = torch.randn(n_samples) * 2.0 - 10.0  # Mean around -10, std=2\n",
    "    print(f\"Log densities: {log_densities}\")\n",
    "    \n",
    "    # Generate some gradients\n",
    "    grad_samples = torch.randn(n_samples, *theta_shape)\n",
    "    print(f\"Gradient samples shape: {grad_samples.shape}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"METHOD 1: USING PYTORCH SOFTMAX\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Method 1: Using PyTorch's built-in softmax\n",
    "    weights_softmax = F.softmax(log_densities, dim=0)\n",
    "    weighted_grad_softmax = torch.sum(weights_softmax.view(-1, 1, 1) * grad_samples, dim=0)\n",
    "    \n",
    "    print(f\"Softmax weights: {weights_softmax}\")\n",
    "    print(f\"Sum of softmax weights: {weights_softmax.sum()}\")\n",
    "    print(f\"Softmax weighted gradient:\\n{weighted_grad_softmax}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "\n",
    "    print(\"METHOD 2: USING LOGSUMEXP_V1 FUNCTION\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Method 3: Using your logsumexp_v1 function\n",
    "    log_sum_exp_v1 = logsumexp_v1(log_densities)\n",
    "    print(f\"logsumexp_v1 result: {log_sum_exp_v1}\")\n",
    "    \n",
    "    # Convert to weights\n",
    "    weights_v1 = torch.exp(log_densities - log_sum_exp_v1)\n",
    "    print(f\"Weights from logsumexp_v1: {weights_v1}\")\n",
    "    print(f\"Sum of logsumexp_v1 weights: {weights_v1.sum()}\")\n",
    "    \n",
    "    # Weighted gradient using logsumexp_v1\n",
    "    weighted_grad_v1 = torch.sum(weights_v1.view(-1, 1, 1) * grad_samples, dim=0)\n",
    "    print(f\"logsumexp_v1 weighted gradient:\\n{weighted_grad_v1}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Compare weights\n",
    "    weights_diff_softmax_manual = torch.abs(weights_softmax - weights_manual).max()\n",
    "    weights_diff_softmax_v1 = torch.abs(weights_softmax - weights_v1).max()\n",
    "    \n",
    "    print(f\"Max difference between softmax and manual weights: {weights_diff_softmax_manual:.10f}\")\n",
    "    print(f\"Max difference between softmax and logsumexp_v1 weights: {weights_diff_softmax_v1:.10f}\")\n",
    "    \n",
    "    # Compare gradients\n",
    "    grad_diff_softmax_manual = torch.abs(weighted_grad_softmax - weighted_grad_manual).max()\n",
    "    grad_diff_softmax_v1 = torch.abs(weighted_grad_softmax - weighted_grad_v1).max()\n",
    "    \n",
    "    print(f\"Max difference between softmax and manual gradients: {grad_diff_softmax_manual:.10f}\")\n",
    "    print(f\"Max difference between softmax and logsumexp_v1 gradients: {grad_diff_softmax_v1:.10f}\")\n",
    "    \n",
    "    # Check if they're essentially equal\n",
    "    tolerance = 1e-6\n",
    "    softmax_manual_equal = weights_diff_softmax_manual < tolerance\n",
    "    softmax_v1_equal = weights_diff_softmax_v1 < tolerance\n",
    "    \n",
    "    print(f\"\\nAre softmax and manual weights equal (tol={tolerance})? {softmax_manual_equal}\")\n",
    "    print(f\"Are softmax and logsumexp_v1 weights equal (tol={tolerance})? {softmax_v1_equal}\")\n",
    "\n",
    "# Run the comparison test\n",
    "#test_softmax_vs_manual_logsumexp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- numerically stable, vector-valued estimator ----------\n",
    "def weighted_grad(log_p: torch.Tensor,\n",
    "                  grad_p: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return  ∑ softmax(log_p)_m * grad_p[m]\n",
    "    Shapes\n",
    "        log_p  : (M,)\n",
    "        grad_p : (M, …)\n",
    "    \"\"\"\n",
    "    w = torch.softmax(log_p, dim=0)           # (M,)\n",
    "    while w.dim() < grad_p.dim():             # make w broadcastable\n",
    "        w = w.unsqueeze(-1)\n",
    "    return (w * grad_p).sum(dim=0)\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------ quick comparison against softmax ------------\n",
    "torch.manual_seed(42)\n",
    "M, shape = 5, (3, 3)\n",
    "\n",
    "log_p   = torch.randn(M) * 2.0 - 10.0               # log densities\n",
    "grad_p  = torch.randn(M, *shape)                    # per-sample grads\n",
    "\n",
    "w_soft  = F.softmax(log_p, dim=0)                   # reference weights\n",
    "grad_sm = (w_soft.unsqueeze(-1).unsqueeze(-1) * grad_p).sum(0)\n",
    "\n",
    "grad_wg = weighted_grad(log_p, grad_p)\n",
    "\n",
    "print(\"log densities:\\n\", log_p, \"\\n\")\n",
    "print(\"softmax weights  :\", w_soft)\n",
    "print(\"weighted_grad wts:\", torch.softmax(log_p, 0))        # same\n",
    "print(\"‖grad diff‖_∞     :\", (grad_sm - grad_wg).abs().max())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
