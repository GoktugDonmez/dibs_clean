{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the logsumexp function\n",
    "Try with mock data to see how it works, try to compute the exp(logp), define a mock data for the logp it should have small values. it doesnt even need to be a real prob density just a array with small numbers. \n",
    "\n",
    "for score estimator we need expectation of ( p_joint(we have logp so we do exp(logp)) * grad_z p(G|Z)) we can seperate the expectation terms (try!)\n",
    "\n",
    "after having the score estimator grad for p_joint,\n",
    "calculate numerator (score estimator grad_z) divided by expectation_p_joint ( we again have logp) \n",
    "\n",
    "doing num / den is hard again so do lognum - log den and then exponentiate\n",
    "\n",
    "start first how to calculate stably the logp to p \n",
    "\n",
    "then by using the function for logp to p stable logsumexp trick \n",
    "\n",
    "try to use for the pjoint in denominator too \n",
    "\n",
    "and also for the finally for the ratio num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logp: tensor([-1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000])\n",
      "exp_logp: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "sum of exp_logp: 0.0\n",
      "log of sum of exp_logp: -inf\n",
      "logsumexp of logp: -993.785400390625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "## negative values for logp \n",
    "# variables are, values of the entries and the shape of the tensor\n",
    "\n",
    "val = -1000\n",
    "size = 500\n",
    "\n",
    "logp = torch.tensor([val] * size)\n",
    "print(f'logp: {logp}')\n",
    "\n",
    "\n",
    "# directly exponentiate the logp \n",
    "exp_logp = torch.exp(logp)\n",
    "print(f'exp_logp: {exp_logp}')\n",
    "\n",
    "# print the sum of the exp_logp\n",
    "print(f'sum of exp_logp: {exp_logp.sum()}')\n",
    "# print the log of the sum of the exp_logp\n",
    "print(f'log of sum of exp_logp: {exp_logp.sum().log()}')\n",
    "\n",
    "# which is equal to \n",
    "log_sum_exp = torch.logsumexp(logp,dim=0)\n",
    "print(f'logsumexp of logp: {log_sum_exp}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log of a negative number: tensor([nan])\n",
      "sign of a negative number: tensor([-1])\n",
      "negative of a negative number: tensor([4])\n",
      "log of a negative number: tensor([1.3863])\n",
      "log of a negative number with reverse sign: tensor([-1.3863])\n"
     ]
    }
   ],
   "source": [
    "## but here where is the part if the value is negative what is log of a negative v\n",
    "\n",
    "# log of a negative number is undefined\n",
    "dummy_tensor = torch.tensor([-4])\n",
    "print(f'log of a negative number: {torch.log(dummy_tensor)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# How about if we just reverse the sign and then do the logsumexp and then reverse the sign again?\n",
    "\n",
    "sign_dummy_tensor = torch.sign(dummy_tensor)\n",
    "print(f'sign of a negative number: {sign_dummy_tensor}')\n",
    "\n",
    "neg_dummy_tensor = -1 * dummy_tensor\n",
    "print(f'negative of a negative number: {neg_dummy_tensor}')\n",
    "\n",
    "# final neg value \n",
    "log_dummy_tensor = torch.log(neg_dummy_tensor)\n",
    "print(f'log of a negative number: {log_dummy_tensor}')\n",
    "\n",
    "log_neg_dummy_tensor_reverse = sign_dummy_tensor * log_dummy_tensor\n",
    "print(f'log of a negative number with reverse sign: {log_neg_dummy_tensor_reverse}')\n",
    "\n",
    "# so some sign trick is needed to make it work \n",
    "# continue with this idea for the logsumexp of the numerator / denominator \n",
    "# (especially since numerator is a gradient with negative values)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try with the expectation of a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logp: tensor([-1001.0804,  -998.3104,  -999.5331, -1000.1557, -1000.3253, -1000.9654,\n",
      "        -1000.6677,  -999.9363, -1000.0333,  -999.7894,  -999.3666, -1000.4529,\n",
      "        -1000.6130,  -999.6716,  -999.4961, -1000.4133,  -998.3268,  -999.3607,\n",
      "         -999.3165,  -999.3884, -1000.0940,  -998.6558, -1000.4788,  -998.0553,\n",
      "         -999.9222,  -999.3783,  -998.3589,  -999.2471, -1000.4186, -1000.1543,\n",
      "        -1001.0948,  -999.6008,  -999.2224,  -999.6808, -1000.1816, -1001.9860,\n",
      "         -999.8506,  -999.4153,  -999.3735, -1001.0857, -1000.4762,  -998.7845,\n",
      "         -999.3394,  -999.2870,  -999.3116,  -998.5846, -1002.2172,  -999.0669,\n",
      "         -999.6246,  -998.6852,  -998.3542, -1001.7344,  -999.7136, -1001.9893,\n",
      "         -999.4598, -1001.2610, -1002.1068, -1001.2159, -1001.2245, -1001.0909,\n",
      "        -1000.7559, -1001.5068, -1000.5159, -1000.4258, -1000.3645, -1000.4149,\n",
      "         -999.0513,  -999.8271,  -999.7773,  -997.3482, -1000.3988,  -999.9426,\n",
      "         -999.2041, -1000.4899, -1001.8509,  -998.0611,  -999.9514,  -999.9780,\n",
      "        -1001.3359,  -998.4218, -1000.2765,  -999.9352, -1000.4351, -1001.2850,\n",
      "         -999.2338, -1001.3517,  -997.9457, -1000.3693,  -999.1210, -1001.5935,\n",
      "         -999.9111,  -998.6367, -1000.3665,  -999.9440,  -999.8423,  -998.4760,\n",
      "        -1002.2011, -1000.3917, -1000.4025,  -999.6940,  -998.3099, -1000.5358,\n",
      "         -999.9019,  -998.0094,  -999.1859,  -998.6762, -1000.9030,  -999.5739,\n",
      "         -998.4401, -1000.6458,  -999.2639, -1000.6326,  -999.6786,  -999.7816,\n",
      "        -1000.6902, -1000.3422,  -998.3148, -1000.2239, -1001.3160,  -998.2251,\n",
      "         -998.1675, -1000.6204, -1000.3880, -1000.9752, -1001.2659,  -999.7712,\n",
      "         -999.6154, -1000.4760, -1000.1989,  -999.4987,  -998.9944, -1000.2338,\n",
      "         -999.6682,  -999.8643, -1000.4980, -1000.8109, -1000.0355, -1000.0875,\n",
      "        -1000.1483, -1002.3022, -1000.5579,  -997.5599, -1000.7509,  -999.0375,\n",
      "        -1000.3450,  -999.7248, -1000.1579,  -999.8909, -1001.5439, -1001.2177,\n",
      "        -1000.7029,  -998.5955,  -999.2391,  -998.3864,  -997.7487, -1000.9816,\n",
      "         -998.7924, -1001.2571,  -998.8260, -1001.2670,  -999.5690,  -998.7157,\n",
      "         -999.9529, -1001.1542, -1000.3336, -1000.8853,  -999.3688,  -998.0386,\n",
      "        -1000.5850,  -999.9375,  -999.4703,  -999.9777,  -999.6982,  -999.5771,\n",
      "         -999.1614, -1001.8607,  -999.2252, -1001.7600, -1000.2770, -1001.3286,\n",
      "        -1000.9808, -1000.0136, -1000.5840, -1000.7603, -1000.9973,  -999.6977,\n",
      "         -999.8299, -1001.8282, -1001.8156, -1001.7095,  -998.6729, -1000.6673,\n",
      "         -999.4667, -1001.8288,  -999.9081,  -998.0164, -1000.1750,  -998.9002,\n",
      "        -1000.6404,  -999.5042,  -999.5800,  -999.9282, -1000.2900,  -998.9811,\n",
      "        -1000.1604,  -999.4995,  -999.7834, -1000.7762, -1000.3664, -1000.0602,\n",
      "         -998.5976, -1000.7137, -1000.4472, -1000.1863,  -998.4703, -1000.0093,\n",
      "         -999.3853, -1001.6962, -1000.3622, -1000.0452,  -999.5375,  -999.2789,\n",
      "         -999.0503,  -999.5140,  -999.5463,  -999.6410,  -999.8062,  -999.3337,\n",
      "         -999.8297, -1000.5598, -1001.7549, -1000.4963, -1002.6603, -1000.1799,\n",
      "         -999.7691, -1000.6658, -1001.3522, -1000.7967,  -998.9783,  -999.8658,\n",
      "        -1000.1474,  -999.0865, -1000.5601,  -999.5319, -1002.2901, -1000.9509,\n",
      "         -999.6830, -1000.8085,  -999.5192, -1000.6804, -1000.6794, -1000.4657,\n",
      "         -998.7005, -1000.6482, -1000.7057, -1000.6967,  -999.2673, -1001.4248,\n",
      "        -1000.7026, -1000.2173, -1001.0264,  -999.7845,  -999.5159, -1000.5806,\n",
      "         -999.6395, -1001.5303, -1000.0795, -1001.6904, -1001.9037, -1000.6452,\n",
      "         -999.6772,  -999.7440, -1000.3914,  -999.8948, -1001.0471,  -999.0560,\n",
      "        -1000.9604,  -998.8643,  -999.4192,  -999.9200,  -999.6475,  -999.5398,\n",
      "         -998.6946,  -999.4290, -1000.1583,  -999.2374,  -999.6577, -1000.4133,\n",
      "        -1000.5674,  -998.9541, -1001.6093, -1000.9302, -1000.6539, -1000.4282,\n",
      "         -999.6260, -1000.6773, -1000.9988,  -999.8769,  -999.0586, -1000.2438,\n",
      "         -998.6188,  -999.7947, -1000.6060,  -999.7160, -1000.0678,  -999.9111,\n",
      "        -1000.6599,  -999.8772, -1001.6024,  -999.9929, -1000.4659,  -999.6436,\n",
      "        -1000.5530, -1000.3845,  -998.5654,  -999.8530, -1001.6366, -1000.1143,\n",
      "         -999.9611, -1000.0089, -1001.3551, -1000.9088,  -998.9494, -1000.7506,\n",
      "         -998.6897,  -999.5714,  -999.1550, -1001.6442,  -998.3807,  -998.0057,\n",
      "         -999.0206, -1001.1599, -1001.5166, -1001.0402,  -998.5091,  -999.6593,\n",
      "        -1000.7374,  -998.5140,  -999.4319, -1001.1768, -1001.6023, -1000.2125,\n",
      "        -1000.6011, -1000.3085, -1002.6166,  -999.1411,  -999.5993,  -999.1334,\n",
      "        -1000.2369, -1001.0257,  -999.1438, -1000.5085,  -997.7526, -1000.6472,\n",
      "         -998.5662, -1000.5367, -1000.4912, -1001.3168,  -999.0176,  -998.8477,\n",
      "         -999.5095,  -999.9120,  -998.9496,  -998.6833, -1001.2725, -1001.8443,\n",
      "        -1000.2809,  -998.8912,  -999.3083,  -999.8475, -1001.0553,  -998.2780,\n",
      "         -999.1682, -1000.6547, -1000.4507, -1001.7666, -1000.0716,  -998.9133,\n",
      "         -999.8848,  -998.6863, -1000.9118,  -999.6597, -1000.8272,  -998.1872,\n",
      "        -1000.0362, -1000.1403, -1000.5439, -1000.1364,  -998.2107, -1001.0821,\n",
      "        -1000.3330, -1000.4001, -1001.1710, -1001.2867, -1001.6822, -1000.3013,\n",
      "        -1001.3938, -1000.3594,  -999.7979,  -999.1332,  -999.4023, -1001.6734,\n",
      "         -999.9663, -1000.8010,  -999.3898,  -999.3874, -1000.6620, -1000.1408,\n",
      "        -1000.1315,  -999.3537, -1001.9169, -1000.2800, -1000.9627, -1000.4772,\n",
      "        -1000.3846, -1001.4643,  -999.9359,  -998.1244, -1001.9128, -1000.0927,\n",
      "         -999.8931, -1000.2296,  -999.6462,  -999.9364,  -999.7145,  -999.6120,\n",
      "        -1002.0274, -1000.5903, -1001.1111,  -999.0593,  -999.4104,  -999.5371,\n",
      "        -1000.2339,  -998.6854,  -999.5737, -1001.1073,  -998.5527, -1001.5893,\n",
      "        -1000.6141, -1000.2505,  -999.8956,  -999.3586, -1001.0172, -1000.2668,\n",
      "         -999.8616,  -999.3119,  -999.7085,  -997.8560,  -999.3217,  -998.0732,\n",
      "         -999.3346, -1001.0753,  -999.9811, -1000.1981,  -998.6423,  -999.4768,\n",
      "        -1001.2878, -1001.7411,  -999.7413,  -998.3124, -1001.4761, -1001.1147,\n",
      "        -1000.0209,  -998.6795, -1000.5560,  -999.8852, -1000.9509, -1000.7828,\n",
      "        -1000.7451,  -999.6650,  -999.6961, -1000.2187,  -999.5540,  -999.7123,\n",
      "         -998.7631, -1003.1508,  -999.8779, -1000.6367,  -999.9756, -1000.8585,\n",
      "         -999.3842, -1000.7097,  -999.8276, -1001.8730,  -999.7781,  -998.7130,\n",
      "        -1001.4852, -1000.2605, -1000.6102, -1000.6027, -1001.0166,  -999.9117,\n",
      "         -999.6976, -1000.7726, -1000.2668,  -999.8945, -1000.3660,  -998.9520,\n",
      "         -998.6655,  -998.6978])\n",
      "log_p_max: -997.3482055664062\n",
      "log_p_shift: tensor([-3.7322, -0.9622, -2.1849, -2.8075, -2.9771, -3.6172, -3.3195, -2.5881,\n",
      "        -2.6851, -2.4412, -2.0184, -3.1047, -3.2648, -2.3234, -2.1479, -3.0651,\n",
      "        -0.9786, -2.0125, -1.9683, -2.0402, -2.7458, -1.3076, -3.1306, -0.7071,\n",
      "        -2.5740, -2.0301, -1.0107, -1.8989, -3.0704, -2.8061, -3.7466, -2.2526,\n",
      "        -1.8741, -2.3326, -2.8334, -4.6378, -2.5024, -2.0671, -2.0253, -3.7375,\n",
      "        -3.1280, -1.4363, -1.9911, -1.9388, -1.9634, -1.2364, -4.8690, -1.7187,\n",
      "        -2.2764, -1.3370, -1.0060, -4.3862, -2.3654, -4.6411, -2.1116, -3.9128,\n",
      "        -4.7585, -3.8677, -3.8763, -3.7427, -3.4077, -4.1586, -3.1677, -3.0776,\n",
      "        -3.0163, -3.0667, -1.7031, -2.4789, -2.4291,  0.0000, -3.0506, -2.5944,\n",
      "        -1.8559, -3.1417, -4.5027, -0.7129, -2.6031, -2.6298, -3.9877, -1.0736,\n",
      "        -2.9283, -2.5870, -3.0869, -3.9368, -1.8856, -4.0035, -0.5975, -3.0211,\n",
      "        -1.7728, -4.2453, -2.5629, -1.2885, -3.0182, -2.5958, -2.4941, -1.1278,\n",
      "        -4.8529, -3.0435, -3.0543, -2.3458, -0.9617, -3.1876, -2.5537, -0.6612,\n",
      "        -1.8376, -1.3280, -3.5547, -2.2256, -1.0919, -3.2976, -1.9157, -3.2844,\n",
      "        -2.3304, -2.4333, -3.3420, -2.9940, -0.9666, -2.8757, -3.9678, -0.8769,\n",
      "        -0.8193, -3.2722, -3.0398, -3.6270, -3.9177, -2.4230, -2.2672, -3.1277,\n",
      "        -2.8506, -2.1505, -1.6462, -2.8856, -2.3200, -2.5161, -3.1498, -3.4626,\n",
      "        -2.6873, -2.7393, -2.8000, -4.9540, -3.2097, -0.2117, -3.4027, -1.6893,\n",
      "        -2.9968, -2.3766, -2.8097, -2.5427, -4.1957, -3.8695, -3.3547, -1.2473,\n",
      "        -1.8909, -1.0381, -0.4005, -3.6334, -1.4442, -3.9089, -1.4778, -3.9188,\n",
      "        -2.2208, -1.3675, -2.6047, -3.8060, -2.9854, -3.5371, -2.0206, -0.6904,\n",
      "        -3.2368, -2.5893, -2.1221, -2.6295, -2.3500, -2.2289, -1.8132, -4.5125,\n",
      "        -1.8770, -4.4118, -2.9288, -3.9803, -3.6326, -2.6654, -3.2358, -3.4120,\n",
      "        -3.6490, -2.3495, -2.4817, -4.4800, -4.4673, -4.3613, -1.3246, -3.3191,\n",
      "        -2.1185, -4.4806, -2.5599, -0.6682, -2.8268, -1.5520, -3.2922, -2.1559,\n",
      "        -2.2318, -2.5800, -2.9418, -1.6329, -2.8122, -2.1512, -2.4352, -3.4280,\n",
      "        -3.0182, -2.7120, -1.2494, -3.3655, -3.0990, -2.8381, -1.1221, -2.6611,\n",
      "        -2.0371, -4.3480, -3.0140, -2.6970, -2.1893, -1.9307, -1.7021, -2.1658,\n",
      "        -2.1981, -2.2928, -2.4580, -1.9855, -2.4814, -3.2116, -4.4067, -3.1481,\n",
      "        -5.3121, -2.8317, -2.4209, -3.3176, -4.0040, -3.4485, -1.6301, -2.5176,\n",
      "        -2.7992, -1.7383, -3.2119, -2.1837, -4.9419, -3.6027, -2.3348, -3.4603,\n",
      "        -2.1710, -3.3322, -3.3312, -3.1175, -1.3523, -3.3000, -3.3575, -3.3485,\n",
      "        -1.9191, -4.0766, -3.3544, -2.8691, -3.6782, -2.4363, -2.1677, -3.2324,\n",
      "        -2.2913, -4.1821, -2.7313, -4.3422, -4.5555, -3.2970, -2.3290, -2.3958,\n",
      "        -3.0432, -2.5466, -3.6989, -1.7078, -3.6122, -1.5161, -2.0710, -2.5718,\n",
      "        -2.2993, -2.1916, -1.3464, -2.0808, -2.8101, -1.8892, -2.3095, -3.0651,\n",
      "        -3.2192, -1.6059, -4.2610, -3.5820, -3.3057, -3.0800, -2.2778, -3.3291,\n",
      "        -3.6506, -2.5287, -1.7104, -2.8956, -1.2706, -2.4465, -3.2578, -2.3678,\n",
      "        -2.7196, -2.5629, -3.3117, -2.5290, -4.2542, -2.6447, -3.1177, -2.2954,\n",
      "        -3.2048, -3.0363, -1.2172, -2.5048, -4.2884, -2.7661, -2.6129, -2.6606,\n",
      "        -4.0069, -3.5606, -1.6012, -3.4024, -1.3415, -2.2232, -1.8068, -4.2960,\n",
      "        -1.0325, -0.6575, -1.6724, -3.8117, -4.1684, -3.6920, -1.1609, -2.3111,\n",
      "        -3.3892, -1.1658, -2.0837, -3.8286, -4.2541, -2.8643, -3.2529, -2.9603,\n",
      "        -5.2684, -1.7929, -2.2511, -1.7852, -2.8887, -3.6775, -1.7956, -3.1603,\n",
      "        -0.4044, -3.2990, -1.2180, -3.1885, -3.1430, -3.9686, -1.6694, -1.4995,\n",
      "        -2.1613, -2.5638, -1.6014, -1.3351, -3.9243, -4.4961, -2.9327, -1.5430,\n",
      "        -1.9601, -2.4993, -3.7071, -0.9297, -1.8199, -3.3065, -3.1025, -4.4184,\n",
      "        -2.7234, -1.5651, -2.5366, -1.3381, -3.5636, -2.3115, -3.4790, -0.8390,\n",
      "        -2.6880, -2.7921, -3.1957, -2.7882, -0.8625, -3.7339, -2.9848, -3.0519,\n",
      "        -3.8228, -3.9385, -4.3340, -2.9531, -4.0456, -3.0112, -2.4496, -1.7850,\n",
      "        -2.0541, -4.3252, -2.6181, -3.4528, -2.0416, -2.0392, -3.3138, -2.7926,\n",
      "        -2.7833, -2.0055, -4.5687, -2.9318, -3.6145, -3.1290, -3.0364, -4.1161,\n",
      "        -2.5877, -0.7762, -4.5646, -2.7444, -2.5449, -2.8814, -2.2980, -2.5882,\n",
      "        -2.3663, -2.2638, -4.6792, -3.2421, -3.7629, -1.7111, -2.0622, -2.1889,\n",
      "        -2.8857, -1.3372, -2.2255, -3.7591, -1.2045, -4.2411, -3.2659, -2.9023,\n",
      "        -2.5474, -2.0104, -3.6689, -2.9186, -2.5134, -1.9637, -2.3603, -0.5078,\n",
      "        -1.9734, -0.7250, -1.9864, -3.7271, -2.6329, -2.8499, -1.2941, -2.1286,\n",
      "        -3.9396, -4.3929, -2.3931, -0.9642, -4.1279, -3.7665, -2.6727, -1.3313,\n",
      "        -3.2078, -2.5370, -3.6027, -3.4346, -3.3969, -2.3168, -2.3479, -2.8705,\n",
      "        -2.2057, -2.3641, -1.4149, -5.8026, -2.5297, -3.2885, -2.6274, -3.5103,\n",
      "        -2.0360, -3.3615, -2.4794, -4.5248, -2.4299, -1.3648, -4.1370, -2.9123,\n",
      "        -3.2620, -3.2545, -3.6684, -2.5635, -2.3494, -3.4244, -2.9186, -2.5463,\n",
      "        -3.0178, -1.6038, -1.3173, -1.3496])\n",
      "p_shift: tensor([0.0239, 0.3821, 0.1125, 0.0604, 0.0509, 0.0269, 0.0362, 0.0752, 0.0682,\n",
      "        0.0871, 0.1329, 0.0448, 0.0382, 0.0979, 0.1167, 0.0466, 0.3758, 0.1337,\n",
      "        0.1397, 0.1300, 0.0642, 0.2705, 0.0437, 0.4931, 0.0762, 0.1313, 0.3640,\n",
      "        0.1497, 0.0464, 0.0604, 0.0236, 0.1051, 0.1535, 0.0970, 0.0588, 0.0097,\n",
      "        0.0819, 0.1265, 0.1320, 0.0238, 0.0438, 0.2378, 0.1365, 0.1439, 0.1404,\n",
      "        0.2904, 0.0077, 0.1793, 0.1027, 0.2626, 0.3657, 0.0124, 0.0939, 0.0096,\n",
      "        0.1210, 0.0200, 0.0086, 0.0209, 0.0207, 0.0237, 0.0331, 0.0156, 0.0421,\n",
      "        0.0461, 0.0490, 0.0466, 0.1821, 0.0838, 0.0881, 1.0000, 0.0473, 0.0747,\n",
      "        0.1563, 0.0432, 0.0111, 0.4902, 0.0740, 0.0721, 0.0185, 0.3418, 0.0535,\n",
      "        0.0752, 0.0456, 0.0195, 0.1517, 0.0183, 0.5502, 0.0487, 0.1699, 0.0143,\n",
      "        0.0771, 0.2757, 0.0489, 0.0746, 0.0826, 0.3237, 0.0078, 0.0477, 0.0472,\n",
      "        0.0958, 0.3823, 0.0413, 0.0778, 0.5162, 0.1592, 0.2650, 0.0286, 0.1080,\n",
      "        0.3356, 0.0370, 0.1472, 0.0375, 0.0973, 0.0877, 0.0354, 0.0501, 0.3804,\n",
      "        0.0564, 0.0189, 0.4161, 0.4407, 0.0379, 0.0478, 0.0266, 0.0199, 0.0887,\n",
      "        0.1036, 0.0438, 0.0578, 0.1164, 0.1928, 0.0558, 0.0983, 0.0808, 0.0429,\n",
      "        0.0313, 0.0681, 0.0646, 0.0608, 0.0071, 0.0404, 0.8092, 0.0333, 0.1846,\n",
      "        0.0499, 0.0929, 0.0602, 0.0787, 0.0151, 0.0209, 0.0349, 0.2873, 0.1509,\n",
      "        0.3541, 0.6700, 0.0264, 0.2359, 0.0201, 0.2281, 0.0199, 0.1085, 0.2547,\n",
      "        0.0739, 0.0222, 0.0505, 0.0291, 0.1326, 0.5014, 0.0393, 0.0751, 0.1198,\n",
      "        0.0721, 0.0954, 0.1076, 0.1631, 0.0110, 0.1530, 0.0121, 0.0535, 0.0187,\n",
      "        0.0264, 0.0696, 0.0393, 0.0330, 0.0260, 0.0954, 0.0836, 0.0113, 0.0115,\n",
      "        0.0128, 0.2659, 0.0362, 0.1202, 0.0113, 0.0773, 0.5127, 0.0592, 0.2118,\n",
      "        0.0372, 0.1158, 0.1073, 0.0758, 0.0528, 0.1954, 0.0601, 0.1163, 0.0876,\n",
      "        0.0325, 0.0489, 0.0664, 0.2867, 0.0345, 0.0451, 0.0585, 0.3256, 0.0699,\n",
      "        0.1304, 0.0129, 0.0491, 0.0674, 0.1120, 0.1450, 0.1823, 0.1147, 0.1110,\n",
      "        0.1010, 0.0856, 0.1373, 0.0836, 0.0403, 0.0122, 0.0429, 0.0049, 0.0589,\n",
      "        0.0888, 0.0362, 0.0182, 0.0318, 0.1959, 0.0806, 0.0609, 0.1758, 0.0403,\n",
      "        0.1126, 0.0071, 0.0273, 0.0968, 0.0314, 0.1141, 0.0357, 0.0358, 0.0443,\n",
      "        0.2586, 0.0369, 0.0348, 0.0351, 0.1467, 0.0170, 0.0349, 0.0568, 0.0253,\n",
      "        0.0875, 0.1144, 0.0395, 0.1011, 0.0153, 0.0651, 0.0130, 0.0105, 0.0370,\n",
      "        0.0974, 0.0911, 0.0477, 0.0783, 0.0248, 0.1813, 0.0270, 0.2196, 0.1261,\n",
      "        0.0764, 0.1003, 0.1117, 0.2602, 0.1248, 0.0602, 0.1512, 0.0993, 0.0467,\n",
      "        0.0400, 0.2007, 0.0141, 0.0278, 0.0367, 0.0460, 0.1025, 0.0358, 0.0260,\n",
      "        0.0798, 0.1808, 0.0553, 0.2807, 0.0866, 0.0385, 0.0937, 0.0659, 0.0771,\n",
      "        0.0365, 0.0797, 0.0142, 0.0710, 0.0443, 0.1007, 0.0406, 0.0480, 0.2961,\n",
      "        0.0817, 0.0137, 0.0629, 0.0733, 0.0699, 0.0182, 0.0284, 0.2017, 0.0333,\n",
      "        0.2615, 0.1083, 0.1642, 0.0136, 0.3561, 0.5182, 0.1878, 0.0221, 0.0155,\n",
      "        0.0249, 0.3132, 0.0992, 0.0337, 0.3117, 0.1245, 0.0217, 0.0142, 0.0570,\n",
      "        0.0387, 0.0518, 0.0052, 0.1665, 0.1053, 0.1678, 0.0557, 0.0253, 0.1660,\n",
      "        0.0424, 0.6674, 0.0369, 0.2958, 0.0412, 0.0432, 0.0189, 0.1884, 0.2232,\n",
      "        0.1152, 0.0770, 0.2016, 0.2631, 0.0198, 0.0112, 0.0533, 0.2137, 0.1408,\n",
      "        0.0821, 0.0245, 0.3947, 0.1620, 0.0366, 0.0449, 0.0121, 0.0657, 0.2091,\n",
      "        0.0791, 0.2623, 0.0283, 0.0991, 0.0308, 0.4321, 0.0680, 0.0613, 0.0409,\n",
      "        0.0615, 0.4221, 0.0239, 0.0505, 0.0473, 0.0219, 0.0195, 0.0131, 0.0522,\n",
      "        0.0175, 0.0492, 0.0863, 0.1678, 0.1282, 0.0132, 0.0729, 0.0317, 0.1298,\n",
      "        0.1301, 0.0364, 0.0613, 0.0618, 0.1346, 0.0104, 0.0533, 0.0269, 0.0438,\n",
      "        0.0480, 0.0163, 0.0752, 0.4602, 0.0104, 0.0643, 0.0785, 0.0561, 0.1005,\n",
      "        0.0752, 0.0938, 0.1040, 0.0093, 0.0391, 0.0232, 0.1807, 0.1272, 0.1120,\n",
      "        0.0558, 0.2626, 0.1080, 0.0233, 0.2999, 0.0144, 0.0382, 0.0549, 0.0783,\n",
      "        0.1339, 0.0255, 0.0540, 0.0810, 0.1403, 0.0944, 0.6018, 0.1390, 0.4843,\n",
      "        0.1372, 0.0241, 0.0719, 0.0578, 0.2742, 0.1190, 0.0195, 0.0124, 0.0913,\n",
      "        0.3813, 0.0161, 0.0231, 0.0691, 0.2641, 0.0404, 0.0791, 0.0272, 0.0322,\n",
      "        0.0335, 0.0986, 0.0956, 0.0567, 0.1102, 0.0940, 0.2430, 0.0030, 0.0797,\n",
      "        0.0373, 0.0723, 0.0299, 0.1305, 0.0347, 0.0838, 0.0108, 0.0880, 0.2554,\n",
      "        0.0160, 0.0544, 0.0383, 0.0386, 0.0255, 0.0770, 0.0954, 0.0326, 0.0540,\n",
      "        0.0784, 0.0489, 0.2011, 0.2679, 0.2593])\n",
      "p total: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "val = -1000\n",
    "size = 500\n",
    "\n",
    "logp = torch.tensor([val + torch.randn(1) for _ in range(size)])\n",
    "print(f'logp: {logp}')\n",
    "\n",
    "log_p_max = torch.max(logp)\n",
    "print(f'log_p_max: {log_p_max}')\n",
    "\n",
    "log_p_shift = logp - log_p_max\n",
    "print(f'log_p_shift: {log_p_shift}')\n",
    "p_shift = torch.exp(log_p_shift)\n",
    "print(f'p_shift: {p_shift}')\n",
    "\n",
    "p = torch.exp(log_p_shift) * torch.exp(log_p_max)\n",
    "print(f'p total: {p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the formula to calculate the score estimator \n",
    "\n",
    "# smth like expectation under (p G|Z) of = p(G|Z) * grad_z p(G|Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\n",
      "============================================================\n",
      "Original log probabilities: tensor([-1000.5000, -1001.2000,  -999.8000, -1002.1000, -1000.9000])\n",
      "Gradient values shape: torch.Size([5, 3, 2, 2])\n",
      "Sample gradient values:\n",
      "tensor([[[ 0.1927,  0.1487],\n",
      "         [ 0.0901, -0.2106]],\n",
      "\n",
      "        [[ 0.0678, -0.1235],\n",
      "         [-0.0043, -0.1605]],\n",
      "\n",
      "        [[-0.0752,  0.1649],\n",
      "         [-0.0392, -0.1404]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 1: NUMERICALLY UNSTABLE\n",
      "--------------------------------------------------\n",
      "Direct exp(log_p): tensor([0., 0., 0., 0., 0.])\n",
      "Sum of exp(log_p): 0.0\n",
      "Weights (unstable): tensor([nan, nan, nan, nan, nan])\n",
      "Sum of weights: nan\n",
      "Weighted gradient (unstable):\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 2: NUMERICALLY STABLE\n",
      "--------------------------------------------------\n",
      "Max log probability: -999.7999877929688\n",
      "Shifted log probabilities: tensor([-0.7000, -1.4000,  0.0000, -2.3000, -1.1000])\n",
      "Exp of shifted log_p: tensor([0.4966, 0.2466, 1.0000, 0.1003, 0.3329])\n",
      "Weights (stable): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 1.0\n",
      "Weighted gradient (stable):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 3: USING PYTORCH'S LOGSUMEXP\n",
      "--------------------------------------------------\n",
      "Logsumexp result: -999.0223388671875\n",
      "Weights (logsumexp): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 0.9999722838401794\n",
      "Weighted gradient (logsumexp):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "COMPARISON\n",
      "--------------------------------------------------\n",
      "Weight comparison:\n",
      "Stable weights:      tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Logsumexp weights:   tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Unstable weights:    tensor([nan, nan, nan, nan, nan])\n",
      "\n",
      "Gradient comparison:\n",
      "Stable vs Logsumexp gradient difference: 5.937592504778877e-06\n",
      "Stable vs Unstable gradient difference: nan\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\n",
      "============================================================\n",
      "z shape: torch.Size([3, 2, 2])\n",
      "theta shape: torch.Size([3, 3])\n",
      "data['x'] shape: torch.Size([100, 3])\n",
      "\n",
      "Log probabilities range: [-1004.72, -995.40]\n",
      "Gradient tensor shape: torch.Size([64, 3, 2, 2])\n",
      "\n",
      "--------------------------------------------------\n",
      "STABLE SCORE ESTIMATOR COMPUTATION\n",
      "--------------------------------------------------\n",
      "Max log probability: -995.40\n",
      "Weights sum: 1.000000\n",
      "Weight range: [0.000030, 0.330849]\n",
      "Final weighted gradient shape: torch.Size([3, 2, 2])\n",
      "Gradient norm: 0.123335\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dummy_score_estimator_stable_vs_unstable():\n",
    "    \"\"\"\n",
    "    Demonstrates numerically stable vs unstable score estimator computation\n",
    "    using dummy values to show the difference.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    n_samples = 5\n",
    "    d = 3  # dimension of z\n",
    "    z_dim = (d, 2, 2)  # shape of z tensor\n",
    "    \n",
    "    # Generate dummy log probabilities (very small values to demonstrate instability)\n",
    "    log_p_values = torch.tensor([-1000.5, -1001.2, -999.8, -1002.1, -1000.9])\n",
    "    print(f\"Original log probabilities: {log_p_values}\")\n",
    "    \n",
    "    # Generate dummy gradient values\n",
    "    grad_values = torch.randn(n_samples, *z_dim) * 0.1\n",
    "    print(f\"Gradient values shape: {grad_values.shape}\")\n",
    "    print(f\"Sample gradient values:\\n{grad_values[0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 1: NUMERICALLY UNSTABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Unstable method: direct exponentiation\n",
    "    try:\n",
    "        p_unstable = torch.exp(log_p_values)\n",
    "        print(f\"Direct exp(log_p): {p_unstable}\")\n",
    "        print(f\"Sum of exp(log_p): {p_unstable.sum()}\")\n",
    "        \n",
    "        # Normalize to get weights\n",
    "        weights_unstable = p_unstable / p_unstable.sum()\n",
    "        print(f\"Weights (unstable): {weights_unstable}\")\n",
    "        print(f\"Sum of weights: {weights_unstable.sum()}\")\n",
    "        \n",
    "        # Compute weighted gradient\n",
    "        weighted_grad_unstable = torch.zeros_like(grad_values[0])\n",
    "        for i in range(n_samples):\n",
    "            weighted_grad_unstable += weights_unstable[i] * grad_values[i]\n",
    "        \n",
    "        print(f\"Weighted gradient (unstable):\\n{weighted_grad_unstable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unstable method failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 2: NUMERICALLY STABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Stable method: logsumexp trick\n",
    "    log_p_max = torch.max(log_p_values)\n",
    "    print(f\"Max log probability: {log_p_max}\")\n",
    "    \n",
    "    log_p_shifted = log_p_values - log_p_max\n",
    "    print(f\"Shifted log probabilities: {log_p_shifted}\")\n",
    "    \n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    print(f\"Exp of shifted log_p: {p_shifted}\")\n",
    "    \n",
    "    # Normalize to get weights\n",
    "    weights_stable = p_shifted / p_shifted.sum()\n",
    "    print(f\"Weights (stable): {weights_stable}\")\n",
    "    print(f\"Sum of weights: {weights_stable.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_stable = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_stable += weights_stable[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (stable):\\n{weighted_grad_stable}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 3: USING PYTORCH'S LOGSUMEXP\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Using PyTorch's built-in logsumexp\n",
    "    log_sum_exp = torch.logsumexp(log_p_values, dim=0)\n",
    "    print(f\"Logsumexp result: {log_sum_exp}\")\n",
    "    \n",
    "    # Compute weights using logsumexp\n",
    "    weights_logsumexp = torch.exp(log_p_values - log_sum_exp)\n",
    "    print(f\"Weights (logsumexp): {weights_logsumexp}\")\n",
    "    print(f\"Sum of weights: {weights_logsumexp.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_logsumexp = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_logsumexp += weights_logsumexp[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (logsumexp):\\n{weighted_grad_logsumexp}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compare the methods\n",
    "    print(\"Weight comparison:\")\n",
    "    print(f\"Stable weights:      {weights_stable}\")\n",
    "    print(f\"Logsumexp weights:   {weights_logsumexp}\")\n",
    "    \n",
    "    if 'weights_unstable' in locals():\n",
    "        print(f\"Unstable weights:    {weights_unstable}\")\n",
    "    \n",
    "    print(\"\\nGradient comparison:\")\n",
    "    print(f\"Stable vs Logsumexp gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_logsumexp)}\")\n",
    "    \n",
    "    if 'weighted_grad_unstable' in locals():\n",
    "        print(f\"Stable vs Unstable gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_unstable)}\")\n",
    "    \n",
    "    return {\n",
    "        'weights_stable': weights_stable,\n",
    "        'weights_logsumexp': weights_logsumexp,\n",
    "        'grad_stable': weighted_grad_stable,\n",
    "        'grad_logsumexp': weighted_grad_logsumexp\n",
    "    }\n",
    "\n",
    "def score_estimator_with_dummy_data():\n",
    "    \"\"\"\n",
    "    Complete score estimator function using dummy data to demonstrate\n",
    "    the stable computation method.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    torch.manual_seed(42)\n",
    "    n_samples = 64\n",
    "    d = 3\n",
    "    z = torch.randn(d, 2, 2) * 0.1\n",
    "    theta = torch.randn(d, d) * 0.1\n",
    "    data = {'x': torch.randn(100, d)}\n",
    "    hparams = {\n",
    "        'sigma_z': 1.0,\n",
    "        'beta': 1.0,\n",
    "        'alpha': 1.0,\n",
    "        'n_mc_samples': n_samples\n",
    "    }\n",
    "    \n",
    "    print(f\"z shape: {z.shape}\")\n",
    "    print(f\"theta shape: {theta.shape}\")\n",
    "    print(f\"data['x'] shape: {data['x'].shape}\")\n",
    "    \n",
    "    # Simulate the score estimator computation\n",
    "    log_p_samples = []\n",
    "    grad_z_samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Dummy log joint probability (very small values)\n",
    "        log_p = -1000 + torch.randn(1) * 2.0\n",
    "        log_p_samples.append(log_p)\n",
    "        \n",
    "        # Dummy gradient of z\n",
    "        grad_z = torch.randn_like(z) * 0.1\n",
    "        grad_z_samples.append(grad_z)\n",
    "    \n",
    "    log_p_tensor = torch.stack(log_p_samples).squeeze()\n",
    "    grad_z_tensor = torch.stack(grad_z_samples)\n",
    "    \n",
    "    print(f\"\\nLog probabilities range: [{log_p_tensor.min():.2f}, {log_p_tensor.max():.2f}]\")\n",
    "    print(f\"Gradient tensor shape: {grad_z_tensor.shape}\")\n",
    "    \n",
    "    # STABLE COMPUTATION\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"STABLE SCORE ESTIMATOR COMPUTATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Compute stable weights using logsumexp\n",
    "    log_p_max = torch.max(log_p_tensor)\n",
    "    log_p_shifted = log_p_tensor - log_p_max\n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    weights = p_shifted / p_shifted.sum()\n",
    "    \n",
    "    print(f\"Max log probability: {log_p_max:.2f}\")\n",
    "    print(f\"Weights sum: {weights.sum():.6f}\")\n",
    "    print(f\"Weight range: [{weights.min():.6f}, {weights.max():.6f}]\")\n",
    "    \n",
    "    # 2. Compute weighted gradient\n",
    "    weighted_grad = torch.zeros_like(grad_z_tensor[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad += weights[i] * grad_z_tensor[i]\n",
    "    \n",
    "    print(f\"Final weighted gradient shape: {weighted_grad.shape}\")\n",
    "    print(f\"Gradient norm: {torch.norm(weighted_grad):.6f}\")\n",
    "    \n",
    "    return weighted_grad\n",
    "\n",
    "# Run the demonstrations\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the comparison\n",
    "    results = dummy_score_estimator_stable_vs_unstable()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Run the complete estimator\n",
    "    final_grad = score_estimator_with_dummy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\n",
      "============================================================\n",
      "Original log probabilities: tensor([-1000.5000, -1001.2000,  -999.8000, -1002.1000, -1000.9000])\n",
      "Gradient values shape: torch.Size([5, 3, 2, 2])\n",
      "Sample gradient values:\n",
      "tensor([[[ 0.1927,  0.1487],\n",
      "         [ 0.0901, -0.2106]],\n",
      "\n",
      "        [[ 0.0678, -0.1235],\n",
      "         [-0.0043, -0.1605]],\n",
      "\n",
      "        [[-0.0752,  0.1649],\n",
      "         [-0.0392, -0.1404]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 1: NUMERICALLY UNSTABLE\n",
      "--------------------------------------------------\n",
      "Direct exp(log_p): tensor([0., 0., 0., 0., 0.])\n",
      "Sum of exp(log_p): 0.0\n",
      "Weights (unstable): tensor([nan, nan, nan, nan, nan])\n",
      "Sum of weights: nan\n",
      "Weighted gradient (unstable):\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 2: NUMERICALLY STABLE\n",
      "--------------------------------------------------\n",
      "Max log probability: -999.7999877929688\n",
      "Shifted log probabilities: tensor([-0.7000, -1.4000,  0.0000, -2.3000, -1.1000])\n",
      "Exp of shifted log_p: tensor([0.4966, 0.2466, 1.0000, 0.1003, 0.3329])\n",
      "Weights (stable): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 1.0\n",
      "Weighted gradient (stable):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 3: USING PYTORCH'S LOGSUMEXP\n",
      "--------------------------------------------------\n",
      "Logsumexp result: -999.0223388671875\n",
      "Weights (logsumexp): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 0.9999722838401794\n",
      "Weighted gradient (logsumexp):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "COMPARISON\n",
      "--------------------------------------------------\n",
      "Weight comparison:\n",
      "Stable weights:      tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Logsumexp weights:   tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Unstable weights:    tensor([nan, nan, nan, nan, nan])\n",
      "\n",
      "Gradient comparison:\n",
      "Stable vs Logsumexp gradient difference: 5.937592504778877e-06\n",
      "Stable vs Unstable gradient difference: nan\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\n",
      "============================================================\n",
      "z shape: torch.Size([3, 2, 2])\n",
      "theta shape: torch.Size([3, 3])\n",
      "data['x'] shape: torch.Size([100, 3])\n",
      "\n",
      "Log probabilities range: [-1004.72, -995.40]\n",
      "Gradient tensor shape: torch.Size([64, 3, 2, 2])\n",
      "\n",
      "--------------------------------------------------\n",
      "STABLE SCORE ESTIMATOR COMPUTATION\n",
      "--------------------------------------------------\n",
      "Max log probability: -995.40\n",
      "Weights sum: 1.000000\n",
      "Weight range: [0.000030, 0.330849]\n",
      "Final weighted gradient shape: torch.Size([3, 2, 2])\n",
      "Gradient norm: 0.123335\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "VERIFYING GRADIENT AVERAGING CORRECTNESS\n",
      "============================================================\n",
      "Log probabilities: tensor([-1.0000, -2.0000, -1.5000, -3.0000, -1.2000, -2.5000, -1.8000, -2.2000,\n",
      "        -1.3000, -2.8000])\n",
      "Gradient shape: torch.Size([10, 2, 2])\n",
      "\n",
      "Simple average gradient:\n",
      "tensor([[ 0.3118,  0.3731],\n",
      "        [-0.0852, -0.2451]])\n",
      "\n",
      "Weights: tensor([0.2080, 0.0765, 0.1261, 0.0281, 0.1703, 0.0464, 0.0934, 0.0626, 0.1541,\n",
      "        0.0344])\n",
      "Sum of weights: 1.000000\n",
      "Weighted average gradient:\n",
      "tensor([[ 0.4893,  0.6262],\n",
      "        [ 0.1042, -0.3597]])\n",
      "\n",
      "Softmax weights: tensor([0.2080, 0.0765, 0.1261, 0.0281, 0.1703, 0.0464, 0.0934, 0.0626, 0.1541,\n",
      "        0.0344])\n",
      "Weighted average (softmax):\n",
      "tensor([[ 0.4893,  0.6262],\n",
      "        [ 0.1042, -0.3597]])\n",
      "\n",
      "Consistency checks:\n",
      "Weights vs Softmax weights difference: 3.54e-08\n",
      "Weighted avg vs Softmax avg difference: 1.88e-07\n",
      "\n",
      "Weight sums:\n",
      "Stable weights sum: 1.000000\n",
      "Softmax weights sum: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dummy_score_estimator_stable_vs_unstable():\n",
    "    \"\"\"\n",
    "    Demonstrates numerically stable vs unstable score estimator computation\n",
    "    with mathematically correct gradient averaging.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    n_samples = 5\n",
    "    d = 3  # dimension of z\n",
    "    z_dim = (d, 2, 2)  # shape of z tensor\n",
    "    \n",
    "    # Generate dummy log probabilities (very small values to demonstrate instability)\n",
    "    log_p_values = torch.tensor([-1000.5, -1001.2, -999.8, -1002.1, -1000.9])\n",
    "    print(f\"Original log probabilities: {log_p_values}\")\n",
    "    \n",
    "    # Generate dummy gradient values\n",
    "    grad_values = torch.randn(n_samples, *z_dim) * 0.1\n",
    "    print(f\"Gradient values shape: {grad_values.shape}\")\n",
    "    print(f\"Sample gradient values:\\n{grad_values[0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 1: NUMERICALLY UNSTABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Unstable method: direct exponentiation\n",
    "    try:\n",
    "        p_unstable = torch.exp(log_p_values)\n",
    "        print(f\"Direct exp(log_p): {p_unstable}\")\n",
    "        print(f\"Sum of exp(log_p): {p_unstable.sum()}\")\n",
    "        \n",
    "        # Normalize to get weights\n",
    "        weights_unstable = p_unstable / p_unstable.sum()\n",
    "        print(f\"Weights (unstable): {weights_unstable}\")\n",
    "        print(f\"Sum of weights: {weights_unstable.sum()}\")\n",
    "        \n",
    "        # Compute weighted gradient\n",
    "        weighted_grad_unstable = torch.zeros_like(grad_values[0])\n",
    "        for i in range(n_samples):\n",
    "            weighted_grad_unstable += weights_unstable[i] * grad_values[i]\n",
    "        \n",
    "        print(f\"Weighted gradient (unstable):\\n{weighted_grad_unstable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unstable method failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 2: NUMERICALLY STABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Stable method: logsumexp trick\n",
    "    log_p_max = torch.max(log_p_values)\n",
    "    print(f\"Max log probability: {log_p_max}\")\n",
    "    \n",
    "    log_p_shifted = log_p_values - log_p_max\n",
    "    print(f\"Shifted log probabilities: {log_p_shifted}\")\n",
    "    \n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    print(f\"Exp of shifted log_p: {p_shifted}\")\n",
    "    \n",
    "    # Normalize to get weights\n",
    "    weights_stable = p_shifted / p_shifted.sum()\n",
    "    print(f\"Weights (stable): {weights_stable}\")\n",
    "    print(f\"Sum of weights: {weights_stable.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_stable = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_stable += weights_stable[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (stable):\\n{weighted_grad_stable}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 3: USING PYTORCH'S LOGSUMEXP\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Using PyTorch's built-in logsumexp\n",
    "    log_sum_exp = torch.logsumexp(log_p_values, dim=0)\n",
    "    print(f\"Logsumexp result: {log_sum_exp}\")\n",
    "    \n",
    "    # Compute weights using logsumexp\n",
    "    weights_logsumexp = torch.exp(log_p_values - log_sum_exp)\n",
    "    print(f\"Weights (logsumexp): {weights_logsumexp}\")\n",
    "    print(f\"Sum of weights: {weights_logsumexp.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_logsumexp = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_logsumexp += weights_logsumexp[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (logsumexp):\\n{weighted_grad_logsumexp}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compare the methods\n",
    "    print(\"Weight comparison:\")\n",
    "    print(f\"Stable weights:      {weights_stable}\")\n",
    "    print(f\"Logsumexp weights:   {weights_logsumexp}\")\n",
    "    \n",
    "    if 'weights_unstable' in locals():\n",
    "        print(f\"Unstable weights:    {weights_unstable}\")\n",
    "    \n",
    "    print(\"\\nGradient comparison:\")\n",
    "    print(f\"Stable vs Logsumexp gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_logsumexp)}\")\n",
    "    \n",
    "    if 'weighted_grad_unstable' in locals():\n",
    "        print(f\"Stable vs Unstable gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_unstable)}\")\n",
    "    \n",
    "    return {\n",
    "        'weights_stable': weights_stable,\n",
    "        'weights_logsumexp': weights_logsumexp,\n",
    "        'grad_stable': weighted_grad_stable,\n",
    "        'grad_logsumexp': weighted_grad_logsumexp\n",
    "    }\n",
    "\n",
    "def score_estimator_with_dummy_data():\n",
    "    \"\"\"\n",
    "    Complete score estimator function using dummy data to demonstrate\n",
    "    the stable computation method with correct gradient averaging.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    torch.manual_seed(42)\n",
    "    n_samples = 64\n",
    "    d = 3\n",
    "    z = torch.randn(d, 2, 2) * 0.1\n",
    "    theta = torch.randn(d, d) * 0.1\n",
    "    data = {'x': torch.randn(100, d)}\n",
    "    hparams = {\n",
    "        'sigma_z': 1.0,\n",
    "        'beta': 1.0,\n",
    "        'alpha': 1.0,\n",
    "        'n_mc_samples': n_samples\n",
    "    }\n",
    "    \n",
    "    print(f\"z shape: {z.shape}\")\n",
    "    print(f\"theta shape: {theta.shape}\")\n",
    "    print(f\"data['x'] shape: {data['x'].shape}\")\n",
    "    \n",
    "    # Simulate the score estimator computation\n",
    "    log_p_samples = []\n",
    "    grad_z_samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Dummy log joint probability (very small values)\n",
    "        log_p = -1000 + torch.randn(1) * 2.0\n",
    "        log_p_samples.append(log_p)\n",
    "        \n",
    "        # Dummy gradient of z\n",
    "        grad_z = torch.randn_like(z) * 0.1\n",
    "        grad_z_samples.append(grad_z)\n",
    "    \n",
    "    log_p_tensor = torch.stack(log_p_samples).squeeze()\n",
    "    grad_z_tensor = torch.stack(grad_z_samples)\n",
    "    \n",
    "    print(f\"\\nLog probabilities range: [{log_p_tensor.min():.2f}, {log_p_tensor.max():.2f}]\")\n",
    "    print(f\"Gradient tensor shape: {grad_z_tensor.shape}\")\n",
    "    \n",
    "    # STABLE COMPUTATION\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"STABLE SCORE ESTIMATOR COMPUTATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Compute stable weights using logsumexp\n",
    "    log_p_max = torch.max(log_p_tensor)\n",
    "    log_p_shifted = log_p_tensor - log_p_max\n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    weights = p_shifted / p_shifted.sum()\n",
    "    \n",
    "    print(f\"Max log probability: {log_p_max:.2f}\")\n",
    "    print(f\"Weights sum: {weights.sum():.6f}\")\n",
    "    print(f\"Weight range: [{weights.min():.6f}, {weights.max():.6f}]\")\n",
    "    \n",
    "    # 2. Compute weighted gradient\n",
    "    weighted_grad = torch.zeros_like(grad_z_tensor[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad += weights[i] * grad_z_tensor[i]\n",
    "    \n",
    "    print(f\"Final weighted gradient shape: {weighted_grad.shape}\")\n",
    "    print(f\"Gradient norm: {torch.norm(weighted_grad):.6f}\")\n",
    "    \n",
    "    return weighted_grad\n",
    "\n",
    "def verify_gradient_averaging():\n",
    "    \"\"\"\n",
    "    Verify that the gradient averaging is mathematically correct\n",
    "    by comparing with simple average and weighted average.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"VERIFYING GRADIENT AVERAGING CORRECTNESS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    n_samples = 10\n",
    "    d = 2\n",
    "    z_dim = (d, 2)\n",
    "    \n",
    "    # Create simple test case with known probabilities\n",
    "    log_p_values = torch.tensor([-1.0, -2.0, -1.5, -3.0, -1.2, -2.5, -1.8, -2.2, -1.3, -2.8])\n",
    "    grad_values = torch.randn(n_samples, *z_dim)\n",
    "    \n",
    "    print(f\"Log probabilities: {log_p_values}\")\n",
    "    print(f\"Gradient shape: {grad_values.shape}\")\n",
    "    \n",
    "    # Method 1: Simple average (equal weights)\n",
    "    simple_avg = grad_values.mean(dim=0)\n",
    "    print(f\"\\nSimple average gradient:\\n{simple_avg}\")\n",
    "    \n",
    "    # Method 2: Weighted average using stable method\n",
    "    log_p_max = torch.max(log_p_values)\n",
    "    log_p_shifted = log_p_values - log_p_max\n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    weights = p_shifted / p_shifted.sum()\n",
    "    \n",
    "    print(f\"\\nWeights: {weights}\")\n",
    "    print(f\"Sum of weights: {weights.sum():.6f}\")\n",
    "    \n",
    "    weighted_avg = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_avg += weights[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted average gradient:\\n{weighted_avg}\")\n",
    "    \n",
    "    # Method 3: Using PyTorch's softmax\n",
    "    weights_softmax = F.softmax(log_p_values, dim=0)\n",
    "    weighted_avg_softmax = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_avg_softmax += weights_softmax[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"\\nSoftmax weights: {weights_softmax}\")\n",
    "    print(f\"Weighted average (softmax):\\n{weighted_avg_softmax}\")\n",
    "    \n",
    "    # Verify consistency\n",
    "    print(f\"\\nConsistency checks:\")\n",
    "    print(f\"Weights vs Softmax weights difference: {torch.norm(weights - weights_softmax):.2e}\")\n",
    "    print(f\"Weighted avg vs Softmax avg difference: {torch.norm(weighted_avg - weighted_avg_softmax):.2e}\")\n",
    "    \n",
    "    # Show that weights sum to 1\n",
    "    print(f\"\\nWeight sums:\")\n",
    "    print(f\"Stable weights sum: {weights.sum():.6f}\")\n",
    "    print(f\"Softmax weights sum: {weights_softmax.sum():.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'simple_avg': simple_avg,\n",
    "        'weighted_avg': weighted_avg,\n",
    "        'weighted_avg_softmax': weighted_avg_softmax,\n",
    "        'weights': weights,\n",
    "        'weights_softmax': weights_softmax\n",
    "    }\n",
    "\n",
    "# Run the demonstrations\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the comparison\n",
    "    results = dummy_score_estimator_stable_vs_unstable()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Run the complete estimator\n",
    "    final_grad = score_estimator_with_dummy_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Verify gradient averaging\n",
    "    verification = verify_gradient_averaging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's assume the calculation for the score estimator is done as above\n",
    "THe numerator and the denominator is needed to calculate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z_score_with_ratio_stable(z: torch.Tensor, data: Dict[str, Any], theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes _z log p(Z,|D) using equation (10) with stable ratio computation.\n",
    "    \n",
    "    Equation (10): _Z log p(Z,|D) = _Z log p(Z) + [_Z E[p(,D|G)]] / [E[p(,D|G)]]\n",
    "    \"\"\"\n",
    "    d = z.shape[0]\n",
    "    n_samples = hparams.get('n_mc_samples', 64)\n",
    "\n",
    "    # --- Gradient of the Prior ---\n",
    "    grad_z_prior = -(z / hparams['sigma_z']**2)\n",
    "\n",
    "    # --- Collect samples for both numerator and denominator ---\n",
    "    log_joint_rewards = []  # log p(,D|G) for each sample\n",
    "    scores_list = []        # _z log q(G|z) for each sample\n",
    "    \n",
    "    # Pre-compute probabilities to sample from\n",
    "    with torch.no_grad():\n",
    "        edge_probs = torch.sigmoid(scores(z, hparams))\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # 1. Sample a hard graph G\n",
    "        g_hard = torch.bernoulli(edge_probs)\n",
    "\n",
    "        # 2. Calculate the score _z log q(G|z)\n",
    "        score = score_autograd_g_given_z(z, g_hard, hparams)\n",
    "\n",
    "        # 3. Calculate log joint reward: log p(,D|G)\n",
    "        with torch.no_grad():\n",
    "            log_joint_reward = (log_full_likelihood(data, g_hard, theta, hparams) + \n",
    "                               log_theta_prior(theta * g_hard, hparams.get('theta_prior_sigma')))\n",
    "            \n",
    "        log_joint_rewards.append(log_joint_reward)\n",
    "        scores_list.append(score)\n",
    "\n",
    "    # Convert to tensors\n",
    "    log_joint_tensor = torch.stack(log_joint_rewards)  # (n_samples,)\n",
    "    scores_tensor = torch.stack(scores_list)           # (n_samples, d, k, 2)\n",
    "    \n",
    "    # --- STABLE COMPUTATION OF THE RATIO ---\n",
    "    \n",
    "    # Step 1: Compute NUMERATOR = _Z E_p(G|Z) [p(,D|G)]\n",
    "    # This is the score function estimator result\n",
    "    # We need: _i p(G_i|Z) * _z log p(G_i|Z) * p(,D|G_i)\n",
    "    # Since we sampled from p(G|Z), this becomes: (1/M) * _i _z log p(G_i|Z) * p(,D|G_i)\n",
    "    \n",
    "    # Convert log probabilities to actual probabilities using stable method\n",
    "    log_max = torch.max(log_joint_tensor)\n",
    "    log_shifted = log_joint_tensor - log_max\n",
    "    p_shifted = torch.exp(log_shifted)  # These are p(,D|G_i) / exp(log_max)\n",
    "    \n",
    "    # Numerator: weighted sum of scores by probabilities\n",
    "    numerator = torch.zeros_like(scores_tensor[0])\n",
    "    for i in range(n_samples):\n",
    "        numerator += p_shifted[i] * scores_tensor[i]\n",
    "    numerator = numerator / n_samples  # Average over samples\n",
    "    \n",
    "    # Step 2: Compute DENOMINATOR = E_p(G|Z) [p(,D|G)]\n",
    "    # This is: (1/M) * _i p(,D|G_i)\n",
    "    denominator = p_shifted.sum() / n_samples\n",
    "    \n",
    "    # Step 3: Compute the ratio stably\n",
    "    # We have: numerator/denominator, but both are scaled by exp(log_max)\n",
    "    # So the ratio is: (numerator_scaled / exp(log_max)) / (denominator_scaled / exp(log_max))\n",
    "    #                = numerator_scaled / denominator_scaled\n",
    "    grad_likelihood_ratio = numerator / denominator\n",
    "    \n",
    "    # --- Final result ---\n",
    "    total_grad = grad_z_prior + grad_likelihood_ratio\n",
    "    \n",
    "    return total_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable mean \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def stable_mean(fxs: torch.Tensor, dim: int = 0, keepdim: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically stable mean for arrays with mixed positive/negative values.\n",
    "    \n",
    "    Args:\n",
    "        fxs: Input tensor\n",
    "        dim: Dimension along which to compute the mean\n",
    "        keepdim: Whether to keep the reduced dimension\n",
    "    \n",
    "    Returns:\n",
    "        Stable mean of the input tensor\n",
    "    \"\"\"\n",
    "    jitter = 1e-30\n",
    "    \n",
    "    def stable_mean_positive_only(fs: torch.Tensor, n: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Helper function for positive-only values\"\"\"\n",
    "        # Add jitter to avoid log(0)\n",
    "        fs_safe = fs + jitter\n",
    "        log_sum = torch.logsumexp(torch.log(fs_safe), dim=dim, keepdim=True)\n",
    "        log_n = torch.log(n + jitter)\n",
    "        return torch.exp(log_sum - log_n)\n",
    "    \n",
    "    # Separate positive and negative values\n",
    "    positive_mask = fxs > 0.0\n",
    "    negative_mask = fxs < 0.0\n",
    "    \n",
    "    f_xs_positive = fxs * positive_mask.float()\n",
    "    f_xs_negative = -fxs * negative_mask.float()  # Make negative values positive\n",
    "    \n",
    "    # Count positive and negative elements\n",
    "    n_positive = positive_mask.sum(dim=dim, keepdim=True).float()\n",
    "    n_negative = negative_mask.sum(dim=dim, keepdim=True).float()\n",
    "    \n",
    "    # Total number of elements\n",
    "    if dim is None:\n",
    "        n_total = torch.tensor(fxs.numel(), dtype=fxs.dtype, device=fxs.device)\n",
    "    else:\n",
    "        n_total = torch.tensor(fxs.shape[dim], dtype=fxs.dtype, device=fxs.device)\n",
    "    \n",
    "    # Compute stable means for positive and negative parts\n",
    "    avg_positive = stable_mean_positive_only(f_xs_positive, n_positive)\n",
    "    avg_negative = stable_mean_positive_only(f_xs_negative, n_negative)\n",
    "    \n",
    "    # Combine with proper weighting\n",
    "    result = (n_positive / n_total) * avg_positive - (n_negative / n_total) * avg_negative\n",
    "    \n",
    "    return result if keepdim else result.squeeze(dim)\n",
    "\n",
    "\n",
    "def log_stable_mean_from_logs(fsx: torch.Tensor, dim: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log(mean(exp(fsx))) stably when fsx contains log values.\n",
    "    \n",
    "    This is equivalent to computing the log of the mean of probabilities\n",
    "    when fsx contains log probabilities.\n",
    "    \n",
    "    Args:\n",
    "        fsx: Input tensor containing log values\n",
    "        dim: Dimension along which to compute the mean\n",
    "    \n",
    "    Returns:\n",
    "        Log of the mean of exp(fsx)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = fsx.shape[dim]\n",
    "    \n",
    "    # Compute logsumexp and subtract log(n) for the mean\n",
    "    lse = torch.logsumexp(fsx, dim=dim)\n",
    "    log_n = torch.log(torch.tensor(n, dtype=fsx.dtype, device=fsx.device))\n",
    "    \n",
    "    return lse - log_n\n",
    "\n",
    "\n",
    "def stable_mean_simple(x: torch.Tensor, dim: int = 0, keepdim: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Simplified stable mean that works well for most cases.\n",
    "    Similar to the one in your models/utils.py but cleaner.\n",
    "    \"\"\"\n",
    "    jitter = 1e-30\n",
    "    \n",
    "    if not x.is_floating_point():\n",
    "        x = x.float()\n",
    "    \n",
    "    # Separate positive and negative parts\n",
    "    pos = x.clamp(min=0) + jitter\n",
    "    neg = (-x).clamp(min=0) + jitter\n",
    "    \n",
    "    # Compute stable sums using logsumexp\n",
    "    sum_pos = torch.exp(torch.logsumexp(torch.log(pos), dim=dim, keepdim=True))\n",
    "    sum_neg = torch.exp(torch.logsumexp(torch.log(neg), dim=dim, keepdim=True))\n",
    "    \n",
    "    # Get the count\n",
    "    n = torch.tensor(x.shape[dim] if dim < x.dim() else x.numel(), \n",
    "                     dtype=x.dtype, device=x.device)\n",
    "    \n",
    "    # Compute mean\n",
    "    mean = (sum_pos - sum_neg) / (n + jitter)\n",
    "    \n",
    "    return mean if keepdim else mean.squeeze(dim)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_tensor: tensor([-1000, -1000, -1000, -1000, -1000])\n",
      "stable_mean: -999.9996337890625\n",
      "stable_mean exp: 0.0\n"
     ]
    }
   ],
   "source": [
    "## try the stable mean function \n",
    "\n",
    "dummy_tensor = torch.tensor([-1000, -1000, -1000, -1000, -1000])\n",
    "\n",
    "print(f'dummy_tensor: {dummy_tensor}')\n",
    "\n",
    "print(f'stable_mean: {stable_mean(dummy_tensor)}')\n",
    "\n",
    "print(f'stable_mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def logsumexp_with_sign(a, b, dim=-1, keepdim=False, eps=1e-40):\n",
    "    \"\"\"\n",
    "    Stable log| bexp(a)|   and   sign( bexp(a))\n",
    "    ------------------------------------------------\n",
    "    a   : tensor  the log-terms (_j)\n",
    "    b   : tensor  the weights   (b_j)  (can be )\n",
    "          `a` and `b` must be broadcastcompatible.\n",
    "    dim : dimension over which to sum\n",
    "    \"\"\"\n",
    "    a_max = torch.max(a, dim=dim, keepdim=True)[0]           # shift\n",
    "    scaled = b * torch.exp(a - a_max)                        # weights in safe range\n",
    "    s      = scaled.sum(dim=dim, keepdim=keepdim)            # may be \n",
    "    sign   = torch.sign(s).detach()                          # 1 (detach  no grad through sign)\n",
    "    logabs = torch.log(torch.abs(s) + eps) + a_max.squeeze(dim) \\\n",
    "             if not keepdim else torch.log(torch.abs(s) + eps) + a_max\n",
    "    return logabs, sign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_num: tensor([-2.0601, -0.6257, -2.0684, -0.0838,  1.4336, -0.1034,  0.1488,  0.1566,\n",
      "         1.1128, -1.3828,  1.5209,  0.6923])\n",
      "sign_num: tensor([ 1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.])\n",
      "grad_z: tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487],\n",
      "        [ 0.6920, -0.3160, -2.1152,  0.3223, -1.2633],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168],\n",
      "        [-0.2473, -1.3527, -1.6959,  0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502],\n",
      "        [-0.5855, -0.1734,  0.1835,  1.3894,  1.5863],\n",
      "        [ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927],\n",
      "        [ 0.2484,  0.4397,  0.1124,  0.6408,  0.4412],\n",
      "        [-0.1023,  0.7924, -0.2897,  0.0525,  0.5943],\n",
      "        [ 1.5419,  0.5073, -0.5910, -0.5692,  0.9200],\n",
      "        [ 1.1108,  1.2899, -1.4959, -0.1938,  0.4455],\n",
      "        [ 1.3253, -1.6293, -0.5497, -0.4798, -0.4997]])\n",
      "torch.Size([3, 2, 2])\n",
      "tensor([[[ 0.0134, -0.0562],\n",
      "         [-0.0133, -0.0967]],\n",
      "\n",
      "        [[-0.4408, -0.0948],\n",
      "         [-0.1220,  0.1229]],\n",
      "\n",
      "        [[ 0.3199,  0.0264],\n",
      "         [ 0.4811, -0.2101]]])\n"
     ]
    }
   ],
   "source": [
    "# --- dummy data for demonstration ---\n",
    "K          = 5                          # number of graph samples\n",
    "flat_dim   = 12                         # d*k*2  after you reshape\n",
    "\n",
    "torch.manual_seed(0)\n",
    "grad_z     = torch.randn(flat_dim, K)   # [flat_dim, K]  each row = score_row\n",
    "log_lik    = torch.randn(K) * -2.0      # [K]\n",
    "\n",
    "# --- denominator: plain logsumexp because weights are +1 ---\n",
    "log_den    = torch.logsumexp(log_lik, dim=0)          # scalar  log D\n",
    "\n",
    "# --- numerator: row-wise signed logsumexp ---\n",
    "log_num, sign_num = [], []\n",
    "for r in range(flat_dim):\n",
    "    ln, sg = logsumexp_with_sign(grad_z[r], log_lik)\n",
    "    log_num.append(ln)\n",
    "    sign_num.append(sg)\n",
    "\n",
    "log_num = torch.stack(log_num)    # [flat_dim]\n",
    "sign_num= torch.stack(sign_num)   # [flat_dim]  (+1/-1/0)\n",
    "print(f'log_num: {log_num}')\n",
    "print(f'sign_num: {sign_num}')\n",
    "print(f'grad_z: {grad_z}')\n",
    "\n",
    "# --- weighted average  N/D  (still on-graph wrt log_lik and grad_z) ---\n",
    "weighted_score_flat = sign_num * torch.exp(log_num - log_den)   # [flat_dim]\n",
    "\n",
    "# reshape back to [d, k, 2] if you like\n",
    "d, k = 3, 2\n",
    "weighted_score = weighted_score_flat.view(d, k, 2)\n",
    "\n",
    "print(weighted_score.shape)   # torch.Size([3, 2, 2])\n",
    "print(weighted_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stable_mean(fxs):\n",
    "    # assumes fs are only positive\n",
    "    jitter = 1e-30\n",
    "\n",
    "    # Taking n separately we need non-zero\n",
    "    stable_mean_psve_only = lambda fs, n: np.exp(\n",
    "        np.logsumexp(np.log(fs)) - np.log(n + jitter)\n",
    "    )\n",
    "\n",
    "    f_xs_psve = fxs * (fxs > 0.0)\n",
    "    f_xs_ngve = -fxs * (fxs < 0.0)\n",
    "\n",
    "    n_psve = np.sum((fxs > 0.0))\n",
    "    n_ngve = fxs.size - n_psve\n",
    "\n",
    "    avg_psve = stable_mean_psve_only(f_xs_psve, n_psve)\n",
    "    avg_ngve = stable_mean_psve_only(f_xs_ngve, n_ngve)\n",
    "    return (n_psve / fxs.size) * avg_psve - (n_ngve / fxs.size) * avg_ngve\n",
    "\n",
    "\n",
    "def log_stable_mean_from_logs(fsx):\n",
    "    lse = np.logsumexp(fsx)\n",
    "    return lse - fsx.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pos: 17, n_neg: 33\n",
      "log_grads_abs: tensor([ 2.9957,  2.9646,  2.9326,  2.8994,  2.8651,  2.8296,  2.7928,  2.7546,\n",
      "         2.7148,  2.6734,  2.6303,  2.5852,  2.5379,  2.4883,  2.4361,  2.3811,\n",
      "         2.3228,  2.2609,  2.1950,  2.1243,  2.0484,  1.9661,  1.8765,  1.7781,\n",
      "         1.6689,  1.5463,  1.4065,  1.2440,  1.0498,  0.8087,  0.4902,  0.0202,\n",
      "        -0.8961, -1.5892, -0.2029,  0.3567,  0.7133,  0.9757,  1.1834,  1.3552,\n",
      "         1.5018,  1.6296,  1.7430,  1.8448,  1.9371,  2.0217,  2.0996,  2.1720,\n",
      "         2.2394,  2.3026])\n",
      "log_grads_abs_pos: tensor([   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf, -1.5892, -0.2029,  0.3567,  0.7133,  0.9757,  1.1834,  1.3552,\n",
      "         1.5018,  1.6296,  1.7430,  1.8448,  1.9371,  2.0217,  2.0996,  2.1720,\n",
      "         2.2394,  2.3026])\n",
      "log_grads_abs_neg: tensor([ 2.9957,  2.9646,  2.9326,  2.8994,  2.8651,  2.8296,  2.7928,  2.7546,\n",
      "         2.7148,  2.6734,  2.6303,  2.5852,  2.5379,  2.4883,  2.4361,  2.3811,\n",
      "         2.3228,  2.2609,  2.1950,  2.1243,  2.0484,  1.9661,  1.8765,  1.7781,\n",
      "         1.6689,  1.5463,  1.4065,  1.2440,  1.0498,  0.8087,  0.4902,  0.0202,\n",
      "        -0.8961,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf])\n",
      "log_numerator_positive: 1.6296405792236328\n",
      "log_numerator_negative: 2.322787284851074\n",
      "log_den: -983.3275756835938\n",
      "log_den_shifted: -983.3275146484375\n",
      "log_den_shifted_exp: 0.0\n",
      "final_grad: nan\n",
      "final_grad_shifted: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def stable_ratio(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    \n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grads = torch.stack(grad_samples)\n",
    "\n",
    "    log_grads_abs = torch.log(grads.abs() + eps)\n",
    "\n",
    "    log_grads_abs += log_p\n",
    "\n",
    "    pos_mask = grads >= 0\n",
    "    neg_mask = grads < 0\n",
    "\n",
    "    n_pos = pos_mask.sum().clamp(min=1)\n",
    "    n_neg = neg_mask.sum().clamp(min=1)\n",
    "\n",
    "    print(f'n_pos: {n_pos}, n_neg: {n_neg}')\n",
    "\n",
    "    print(f'log_grads_abs: {log_grads_abs}')\n",
    "\n",
    "    log_grads_abs_pos = log_grads_abs.masked_fill(~pos_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_pos: {log_grads_abs_pos}')\n",
    "    log_grads_abs_neg = log_grads_abs.masked_fill(~neg_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_neg: {log_grads_abs_neg}')\n",
    "\n",
    "    log_numerator_positive = torch.logsumexp(log_grads_abs_pos, dim=0) - torch.log(n_pos.float())\n",
    "    print(f'log_numerator_positive: {log_numerator_positive}')\n",
    "    log_numerator_negative = torch.logsumexp(log_grads_abs_neg, dim=0) - torch.log(n_neg.float())\n",
    "    print(f'log_numerator_negative: {log_numerator_negative}')\n",
    "    log_den = torch.logsumexp(log_p, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "    print(f'log_den: {log_den}')\n",
    "\n",
    "\n",
    "    # can we use the log_p - log_p_max for the denominator\n",
    "    log_p_max = log_p.max()\n",
    "    log_p_shifted = log_p - log_p_max\n",
    "    log_den_shifted = torch.logsumexp(log_p_shifted, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "    log_den_shifted = log_den_shifted + log_p_max\n",
    "    print(f'log_den_shifted: {log_den_shifted}')\n",
    "\n",
    "    log_den_shifted_exp = torch.exp(log_den_shifted)\n",
    "    print(f'log_den_shifted_exp: {log_den_shifted_exp}')\n",
    "\n",
    "\n",
    "\n",
    "    final_grad = torch.exp(log_numerator_positive - log_den) - torch.exp(log_numerator_negative - log_den)\n",
    "    print(f'final_grad: {final_grad}')\n",
    "\n",
    "    final_grad_shifted = torch.exp(log_numerator_positive - log_den_shifted) - torch.exp(log_numerator_negative - log_den_shifted)\n",
    "    print(f'final_grad_shifted: {final_grad_shifted}')\n",
    "\n",
    "    #print(f'n_pos: {n_pos}, n_neg: {n_neg}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    #print(f\"grad_samples: {grad_samples}\")\n",
    "    #print(f'grads: {grads}')\n",
    "    #print(f'pos_mask: {pos_mask}')\n",
    "    #print(f'pos_mask logs: {pos_mask.log()}')\n",
    "\n",
    "\n",
    "    #print(f'neg_mask: {neg_mask}')\n",
    "    #print(f'neg_mask logs: {neg_mask.log()}')\n",
    "\n",
    "\n",
    "\n",
    "    #print(f'log_grads_abs: {log_grads_abs}')\n",
    "\n",
    "\n",
    "    #print(f'end of grads')x\n",
    "    #print(f'--------------------------------')\n",
    "\n",
    "\n",
    "    #print(f'log_p: {log_p}')\n",
    "\n",
    "    #print(f'grads: {grads}')\n",
    "\n",
    "    return final_grad\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_samples = 50\n",
    "\n",
    "# Create grad samples with net positive mean\n",
    "grad_samples = [x.clone().detach() for x in torch.linspace(-20, 10, num_samples)]  # More positive values than negative\n",
    "\n",
    "# Create log density samples from N(-1000, 100)\n",
    "log_density_samples = [torch.normal(mean=-1000.0, std=10.0, size=()) for _ in range(num_samples)]\n",
    "\n",
    "result = stable_ratio(grad_samples, log_density_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stable_ratio(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    \n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grads = torch.stack(grad_samples)\n",
    "\n",
    "    log_grads_abs = torch.log(grads.abs() + eps)\n",
    "\n",
    "    pos_mask = grads >= 0\n",
    "    neg_mask = grads < 0\n",
    "\n",
    "    n_pos = pos_mask.sum().clamp(min=1)\n",
    "    n_neg = neg_mask.sum().clamp(min=1)\n",
    "\n",
    "    print(f'n_pos: {n_pos}, n_neg: {n_neg}')\n",
    "\n",
    "    print(f'log_grads_abs: {log_grads_abs}')\n",
    "\n",
    "    log_grads_abs_pos = log_grads_abs.masked_fill(~pos_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_pos: {log_grads_abs_pos}')\n",
    "    log_grads_abs_neg = log_grads_abs.masked_fill(~neg_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_neg: {log_grads_abs_neg}')\n",
    "\n",
    "    log_numerator_positive = torch.logsumexp(log_grads_abs_pos, dim=0) - torch.log(n_pos.float())\n",
    "    print(f'log_numerator_positive: {log_numerator_positive}')\n",
    "    log_numerator_negative = torch.logsumexp(log_grads_abs_neg, dim=0) - torch.log(n_neg.float())\n",
    "    print(f'log_numerator_negative: {log_numerator_negative}')\n",
    "    log_den = torch.logsumexp(log_p, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "    print(f'log_den: {log_den}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    final_grad = torch.exp(log_numerator_positive - log_den) - torch.exp(log_numerator_negative - log_den)\n",
    "    print(f'final_grad: {final_grad}')\n",
    "\n",
    "\n",
    "    return final_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m grad_samples[:\u001b[38;5;241m20\u001b[39m] \u001b[38;5;241m=\u001b[39m grad_samples[:\u001b[38;5;241m20\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     53\u001b[0m log_density_samples \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnormal(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000.0\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m, size\u001b[38;5;241m=\u001b[39m()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[0;32m---> 55\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mstable_ratio_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_density_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected Final Gradient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[101], line 7\u001b[0m, in \u001b[0;36mstable_ratio_fixed\u001b[0;34m(grad_samples, log_density_samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-30\u001b[39m\n\u001b[1;32m      6\u001b[0m log_p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(log_density_samples)\n\u001b[0;32m----> 7\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m log_grads_abs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(grads\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m     11\u001b[0m pos_mask \u001b[38;5;241m=\u001b[39m grads \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def stable_ratio_fixed(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    \n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grads = torch.stack(grad_samples)\n",
    "\n",
    "    log_grads_abs = torch.log(grads.abs() + eps)\n",
    "\n",
    "    pos_mask = grads >= 0\n",
    "    neg_mask = grads < 0\n",
    "\n",
    "    n_pos = pos_mask.sum().clamp(min=1)\n",
    "    n_neg = neg_mask.sum().clamp(min=1)\n",
    "\n",
    "    log_grads_abs_pos = log_grads_abs.masked_fill(~pos_mask, float('-inf'))\n",
    "    log_grads_abs_neg = log_grads_abs.masked_fill(~neg_mask, float('-inf'))\n",
    "\n",
    "    # Numerator terms in log space\n",
    "    log_num_pos = torch.logsumexp(log_grads_abs_pos, dim=0) - torch.log(n_pos.float())\n",
    "    log_num_neg = torch.logsumexp(log_grads_abs_neg, dim=0) - torch.log(n_neg.float())\n",
    "    \n",
    "    # Denominator in log space\n",
    "    log_den = torch.logsumexp(log_p, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "\n",
    "    # Perform subtraction in log-space to avoid overflow/underflow\n",
    "    # This computes log(num_pos - num_neg)\n",
    "    # We must handle the case where the result is negative or zero\n",
    "    if log_num_pos > log_num_neg:\n",
    "        # Using log(exp(a) - exp(b)) = a + log(1 - exp(b-a))\n",
    "        log_numerator = log_num_pos + torch.log1p(-torch.exp(log_num_neg - log_num_pos))\n",
    "        sign = 1.0\n",
    "    else:\n",
    "        # To handle num_neg > num_pos\n",
    "        log_numerator = log_num_neg + torch.log1p(-torch.exp(log_num_pos - log_num_neg))\n",
    "        sign = -1.0\n",
    "\n",
    "    # Final calculation: log(numerator) - log(denominator)\n",
    "    final_log_grad = log_numerator - log_den\n",
    "    \n",
    "    return sign * torch.exp(final_log_grad)\n",
    "\n",
    "# Using your test case\n",
    "torch.manual_seed(0)\n",
    "num_samples = 50\n",
    "grad_samples = [torch.tensor(x) for x in torch.linspace(-4, 10, num_samples)]\n",
    "## change first 20 to more *2\n",
    "\n",
    "\n",
    "log_density_samples = [torch.normal(mean=-1000.0, std=100.0, size=()) for _ in range(num_samples)]\n",
    "\n",
    "result = stable_ratio_fixed(grad_samples, log_density_samples)\n",
    "print(f'Corrected Final Gradient: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_density_samples: tensor([ -923.8889,  -938.1699, -1029.9385, -1018.7778,  -808.4102]) ...\n",
      "grad_samples: tensor([-4.0000, -3.7143, -3.4286, -3.1429, -2.8571]) ...\n",
      "num: 3.000000238418579, den: 0.019999999552965164\n",
      "den: 0.019999999552965164 max p exp: 0.0\n",
      "\n",
      "Final Result: 150.00001525878906\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def stable_mean(x: torch.Tensor, eps: float = 1e-30) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically-stable mean of `x` (any shape, any sign).\n",
    "    Works by treating positive and negative parts separately and\n",
    "    using log-sum-exp for each part.\n",
    "    Returns a scalar.\n",
    "    \"\"\"\n",
    "    x = x.flatten()\n",
    "\n",
    "    pos_mask = x >= 0\n",
    "    neg_mask = ~pos_mask                # same as x < 0\n",
    "\n",
    "    pos_vals = x[pos_mask]              #  0\n",
    "    neg_vals = -x[neg_mask]             # magnitudes of the negatives\n",
    "\n",
    "    n_pos = pos_vals.numel()\n",
    "    n_neg = neg_vals.numel()\n",
    "    n_tot = n_pos + n_neg\n",
    "    if n_tot == 0:\n",
    "        raise ValueError(\"`x` is empty!\")\n",
    "\n",
    "    # ----  log-mean of the positive part  ---------------------------------\n",
    "    if n_pos:\n",
    "        log_pos_mean = (\n",
    "            torch.logsumexp(torch.log(pos_vals + eps), dim=0) -\n",
    "            math.log(n_pos)\n",
    "        )\n",
    "        pos_mean = log_pos_mean.exp()\n",
    "    else:\n",
    "        pos_mean = torch.tensor(0., dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # ----  log-mean of the |negative| part  -------------------------------\n",
    "    if n_neg:\n",
    "        log_neg_mean = (\n",
    "            torch.logsumexp(torch.log(neg_vals + eps), dim=0) -\n",
    "            math.log(n_neg)\n",
    "        )\n",
    "        neg_mean = log_neg_mean.exp()\n",
    "    else:\n",
    "        neg_mean = torch.tensor(0., dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # unconditional mean =  P(+)E[+ ]    P()E[||]\n",
    "    return (n_pos / n_tot) * pos_mean - (n_neg / n_tot) * neg_mean\n",
    "\n",
    "\n",
    "def log_mean_from_logs(log_x, eps: float = 1e-30):\n",
    "    \"\"\"log E[x] where `log_x = log(x)` and x0.\"\"\"\n",
    "    return torch.logsumexp(log_x, dim=0) - math.log(log_x.shape[0])\n",
    "\n",
    "def stable_ratio_with_stable_mean(grad_samples, log_density_samples):\n",
    "    grads = torch.stack(grad_samples).flatten()\n",
    "\n",
    "    # numerator   ordinary stable mean (takes  values)\n",
    "    num = stable_mean(grads)\n",
    "\n",
    "    # denominator  stay in log-space until the very end\n",
    "    log_p = torch.stack(log_density_samples).flatten()\n",
    "    log_p_max = log_p.max()\n",
    "    log_p = log_p - log_p_max\n",
    "    log_den = log_mean_from_logs(log_p)     # log E[p]\n",
    "    den = torch.exp(log_den)               # E[p] in \n",
    "\n",
    "    print(f'num: {num}, den: {den}')\n",
    "    print(f'den: {den} max p exp: {torch.exp(log_p_max)}')\n",
    "\n",
    "    return num / den\n",
    "\n",
    "# 50 log-density samples ~ N(-1000, 100)\n",
    "log_density_samples = [torch.normal(mean=-1000.0, std=100.0, size=(50,))]\n",
    "\n",
    "# 50 gradient samples (some positive, some negative)\n",
    "grad_samples = [torch.linspace(-4, 10, steps=50)]  # evenly spaced, centered at 0\n",
    "\n",
    "# Print and test\n",
    "\n",
    "# Print and test\n",
    "print(f'log_density_samples: {log_density_samples[0][:5]} ...')\n",
    "print(f'grad_samples: {grad_samples[0][:5]} ...')\n",
    "\n",
    "result = stable_ratio_with_stable_mean(grad_samples, log_density_samples)\n",
    "print(f'\\nFinal Result: {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING STABLE RATIO FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "Test Case 1: Mixed gradients with small log densities\n",
      "--------------------------------------------------\n",
      "Gradient shapes: [torch.Size([5]), torch.Size([5]), torch.Size([5])]\n",
      "Log density ranges: ['[-1002.1, -999.8]', '[-1001.8, -998.7]', '[-1001.2, -998.3]']\n",
      "stable_ratio result: tensor([nan, nan, nan, nan, nan])\n",
      "stable_ratio result shape: torch.Size([5])\n",
      "stable_ratio_with_stable_mean result: inf\n",
      "stable_ratio_with_stable_mean result shape: torch.Size([])\n",
      "Difference between methods: nan\n",
      "Results are close: False\n",
      "\n",
      "Test Case 2: Comparison with naive method\n",
      "--------------------------------------------------\n",
      "Naive numerator: 0.7733333706855774\n",
      "Naive denominator: 0.0\n",
      "Naive result: inf\n",
      "Stable vs Naive difference: nan\n",
      "\n",
      "Test Case 3: Simple validation case\n",
      "--------------------------------------------------\n",
      "Simple case - stable_ratio: tensor([ 1.3591, -7.3891,  4.4817])\n",
      "Simple case - stable_mean version: 2.7535126209259033\n",
      "Simple case difference: 10.382801055908203\n",
      "Manual calculation: 2.7535128593444824\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def test_stable_ratio_functions():\n",
    "    \"\"\"\n",
    "    Quick test to verify both stable ratio functions work correctly\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING STABLE RATIO FUNCTIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test case 1: Mixed positive/negative gradients with very small log densities\n",
    "    print(\"\\nTest Case 1: Mixed gradients with small log densities\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    grad_samples = [\n",
    "        torch.tensor([15.5, -8.2, 3.1, -12.7, 0.5]),\n",
    "        torch.tensor([-2.3, 9.8, -15.1, 4.6, -0.8]),\n",
    "        torch.tensor([7.2, -5.4, 11.3, -1.9, 6.0])\n",
    "    ]\n",
    "    \n",
    "    log_density_samples = [\n",
    "        torch.tensor([-1001.5, -999.8, -1000.2, -1002.1, -1000.9]),\n",
    "        torch.tensor([-1000.3, -998.7, -1001.8, -999.5, -1000.6]),\n",
    "        torch.tensor([-999.9, -1001.2, -998.3, -1000.7, -999.4])\n",
    "    ]\n",
    "    \n",
    "    print(f\"Gradient shapes: {[g.shape for g in grad_samples]}\")\n",
    "    print(f\"Log density ranges: {[f'[{lg.min():.1f}, {lg.max():.1f}]' for lg in log_density_samples]}\")\n",
    "    \n",
    "    # Test both functions\n",
    "    try:\n",
    "        result1 = stable_ratio(grad_samples, log_density_samples)\n",
    "        print(f\"stable_ratio result: {result1}\")\n",
    "        print(f\"stable_ratio result shape: {result1.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"stable_ratio failed: {e}\")\n",
    "        result1 = None\n",
    "    \n",
    "    try:\n",
    "        result2 = stable_ratio_with_stable_mean(grad_samples, log_density_samples)\n",
    "        print(f\"stable_ratio_with_stable_mean result: {result2}\")\n",
    "        print(f\"stable_ratio_with_stable_mean result shape: {result2.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"stable_ratio_with_stable_mean failed: {e}\")\n",
    "        result2 = None\n",
    "    \n",
    "    if result1 is not None and result2 is not None:\n",
    "        diff = torch.norm(result1 - result2)\n",
    "        print(f\"Difference between methods: {diff}\")\n",
    "        print(f\"Results are close: {diff < 1e-5}\")\n",
    "    \n",
    "    # Test case 2: Naive comparison (will likely fail due to numerical issues)\n",
    "    print(\"\\nTest Case 2: Comparison with naive method\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Naive unstable computation\n",
    "        grads_flat = torch.cat([g.flatten() for g in grad_samples])\n",
    "        log_densities_flat = torch.cat([ld.flatten() for ld in log_density_samples])\n",
    "        densities_flat = torch.exp(log_densities_flat)\n",
    "        \n",
    "        naive_num = grads_flat.mean()\n",
    "        naive_den = densities_flat.mean()\n",
    "        naive_result = naive_num / naive_den if naive_den != 0 else torch.tensor(float('inf'))\n",
    "        \n",
    "        print(f\"Naive numerator: {naive_num}\")\n",
    "        print(f\"Naive denominator: {naive_den}\")\n",
    "        print(f\"Naive result: {naive_result}\")\n",
    "        \n",
    "        if result1 is not None:\n",
    "            print(f\"Stable vs Naive difference: {torch.norm(result1.flatten().mean() - naive_result)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Naive method failed (expected): {e}\")\n",
    "    \n",
    "    # Test case 3: Simple case for validation\n",
    "    print(\"\\nTest Case 3: Simple validation case\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    simple_grads = [torch.tensor([1.0, -1.0, 2.0])]\n",
    "    simple_log_densities = [torch.tensor([-1.0, -2.0, -1.5])]  # Less extreme values\n",
    "    \n",
    "    try:\n",
    "        simple_result1 = stable_ratio(simple_grads, simple_log_densities)\n",
    "        simple_result2 = stable_ratio_with_stable_mean(simple_grads, simple_log_densities)\n",
    "        \n",
    "        print(f\"Simple case - stable_ratio: {simple_result1}\")\n",
    "        print(f\"Simple case - stable_mean version: {simple_result2}\")\n",
    "        print(f\"Simple case difference: {torch.norm(simple_result1 - simple_result2)}\")\n",
    "        \n",
    "        # Manual calculation for verification\n",
    "        grads = torch.tensor([1.0, -1.0, 2.0])\n",
    "        log_p = torch.tensor([-1.0, -2.0, -1.5])\n",
    "        p = torch.exp(log_p)\n",
    "        \n",
    "        manual_num = grads.mean()\n",
    "        manual_den = p.mean()\n",
    "        manual_result = manual_num / manual_den\n",
    "        \n",
    "        print(f\"Manual calculation: {manual_result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Simple case failed: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_stable_ratio_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dibs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
