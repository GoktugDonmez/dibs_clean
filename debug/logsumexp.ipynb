{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the logsumexp function\n",
    "Try with mock data to see how it works, try to compute the exp(logp), define a mock data for the logp it should have small values. it doesnt even need to be a real prob density just a array with small numbers. \n",
    "\n",
    "for score estimator we need expectation of ( p_joint(we have logp so we do exp(logp)) * grad_z p(G|Z)) we can seperate the expectation terms (try!)\n",
    "\n",
    "after having the score estimator grad for p_joint,\n",
    "calculate numerator (score estimator grad_z) divided by expectation_p_joint ( we again have logp) \n",
    "\n",
    "doing num / den is hard again so do lognum - log den and then exponentiate\n",
    "\n",
    "start first how to calculate stably the logp to p \n",
    "\n",
    "then by using the function for logp to p stable logsumexp trick \n",
    "\n",
    "try to use for the pjoint in denominator too \n",
    "\n",
    "and also for the finally for the ratio num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logp: tensor([-1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000,\n",
      "        -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000, -1000])\n",
      "exp_logp: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "sum of exp_logp: 0.0\n",
      "log of sum of exp_logp: -inf\n",
      "logsumexp of logp: -993.785400390625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "## negative values for logp \n",
    "# variables are, values of the entries and the shape of the tensor\n",
    "\n",
    "val = -1000\n",
    "size = 500\n",
    "\n",
    "logp = torch.tensor([val] * size)\n",
    "print(f'logp: {logp}')\n",
    "\n",
    "\n",
    "# directly exponentiate the logp \n",
    "exp_logp = torch.exp(logp)\n",
    "print(f'exp_logp: {exp_logp}')\n",
    "\n",
    "# print the sum of the exp_logp\n",
    "print(f'sum of exp_logp: {exp_logp.sum()}')\n",
    "# print the log of the sum of the exp_logp\n",
    "print(f'log of sum of exp_logp: {exp_logp.sum().log()}')\n",
    "\n",
    "# which is equal to \n",
    "log_sum_exp = torch.logsumexp(logp,dim=0)\n",
    "print(f'logsumexp of logp: {log_sum_exp}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log of a negative number: tensor([nan])\n",
      "sign of a negative number: tensor([-1])\n",
      "negative of a negative number: tensor([4])\n",
      "log of a negative number: tensor([1.3863])\n",
      "log of a negative number with reverse sign: tensor([-1.3863])\n"
     ]
    }
   ],
   "source": [
    "## but here where is the part if the value is negative what is log of a negative v\n",
    "\n",
    "# log of a negative number is undefined\n",
    "dummy_tensor = torch.tensor([-4])\n",
    "print(f'log of a negative number: {torch.log(dummy_tensor)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# How about if we just reverse the sign and then do the logsumexp and then reverse the sign again?\n",
    "\n",
    "sign_dummy_tensor = torch.sign(dummy_tensor)\n",
    "print(f'sign of a negative number: {sign_dummy_tensor}')\n",
    "\n",
    "neg_dummy_tensor = -1 * dummy_tensor\n",
    "print(f'negative of a negative number: {neg_dummy_tensor}')\n",
    "\n",
    "# final neg value \n",
    "log_dummy_tensor = torch.log(neg_dummy_tensor)\n",
    "print(f'log of a negative number: {log_dummy_tensor}')\n",
    "\n",
    "log_neg_dummy_tensor_reverse = sign_dummy_tensor * log_dummy_tensor\n",
    "print(f'log of a negative number with reverse sign: {log_neg_dummy_tensor_reverse}')\n",
    "\n",
    "# so some sign trick is needed to make it work \n",
    "# continue with this idea for the logsumexp of the numerator / denominator \n",
    "# (especially since numerator is a gradient with negative values)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try with the expectation of a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logp: tensor([-1001.0804,  -998.3104,  -999.5331, -1000.1557, -1000.3253, -1000.9654,\n",
      "        -1000.6677,  -999.9363, -1000.0333,  -999.7894,  -999.3666, -1000.4529,\n",
      "        -1000.6130,  -999.6716,  -999.4961, -1000.4133,  -998.3268,  -999.3607,\n",
      "         -999.3165,  -999.3884, -1000.0940,  -998.6558, -1000.4788,  -998.0553,\n",
      "         -999.9222,  -999.3783,  -998.3589,  -999.2471, -1000.4186, -1000.1543,\n",
      "        -1001.0948,  -999.6008,  -999.2224,  -999.6808, -1000.1816, -1001.9860,\n",
      "         -999.8506,  -999.4153,  -999.3735, -1001.0857, -1000.4762,  -998.7845,\n",
      "         -999.3394,  -999.2870,  -999.3116,  -998.5846, -1002.2172,  -999.0669,\n",
      "         -999.6246,  -998.6852,  -998.3542, -1001.7344,  -999.7136, -1001.9893,\n",
      "         -999.4598, -1001.2610, -1002.1068, -1001.2159, -1001.2245, -1001.0909,\n",
      "        -1000.7559, -1001.5068, -1000.5159, -1000.4258, -1000.3645, -1000.4149,\n",
      "         -999.0513,  -999.8271,  -999.7773,  -997.3482, -1000.3988,  -999.9426,\n",
      "         -999.2041, -1000.4899, -1001.8509,  -998.0611,  -999.9514,  -999.9780,\n",
      "        -1001.3359,  -998.4218, -1000.2765,  -999.9352, -1000.4351, -1001.2850,\n",
      "         -999.2338, -1001.3517,  -997.9457, -1000.3693,  -999.1210, -1001.5935,\n",
      "         -999.9111,  -998.6367, -1000.3665,  -999.9440,  -999.8423,  -998.4760,\n",
      "        -1002.2011, -1000.3917, -1000.4025,  -999.6940,  -998.3099, -1000.5358,\n",
      "         -999.9019,  -998.0094,  -999.1859,  -998.6762, -1000.9030,  -999.5739,\n",
      "         -998.4401, -1000.6458,  -999.2639, -1000.6326,  -999.6786,  -999.7816,\n",
      "        -1000.6902, -1000.3422,  -998.3148, -1000.2239, -1001.3160,  -998.2251,\n",
      "         -998.1675, -1000.6204, -1000.3880, -1000.9752, -1001.2659,  -999.7712,\n",
      "         -999.6154, -1000.4760, -1000.1989,  -999.4987,  -998.9944, -1000.2338,\n",
      "         -999.6682,  -999.8643, -1000.4980, -1000.8109, -1000.0355, -1000.0875,\n",
      "        -1000.1483, -1002.3022, -1000.5579,  -997.5599, -1000.7509,  -999.0375,\n",
      "        -1000.3450,  -999.7248, -1000.1579,  -999.8909, -1001.5439, -1001.2177,\n",
      "        -1000.7029,  -998.5955,  -999.2391,  -998.3864,  -997.7487, -1000.9816,\n",
      "         -998.7924, -1001.2571,  -998.8260, -1001.2670,  -999.5690,  -998.7157,\n",
      "         -999.9529, -1001.1542, -1000.3336, -1000.8853,  -999.3688,  -998.0386,\n",
      "        -1000.5850,  -999.9375,  -999.4703,  -999.9777,  -999.6982,  -999.5771,\n",
      "         -999.1614, -1001.8607,  -999.2252, -1001.7600, -1000.2770, -1001.3286,\n",
      "        -1000.9808, -1000.0136, -1000.5840, -1000.7603, -1000.9973,  -999.6977,\n",
      "         -999.8299, -1001.8282, -1001.8156, -1001.7095,  -998.6729, -1000.6673,\n",
      "         -999.4667, -1001.8288,  -999.9081,  -998.0164, -1000.1750,  -998.9002,\n",
      "        -1000.6404,  -999.5042,  -999.5800,  -999.9282, -1000.2900,  -998.9811,\n",
      "        -1000.1604,  -999.4995,  -999.7834, -1000.7762, -1000.3664, -1000.0602,\n",
      "         -998.5976, -1000.7137, -1000.4472, -1000.1863,  -998.4703, -1000.0093,\n",
      "         -999.3853, -1001.6962, -1000.3622, -1000.0452,  -999.5375,  -999.2789,\n",
      "         -999.0503,  -999.5140,  -999.5463,  -999.6410,  -999.8062,  -999.3337,\n",
      "         -999.8297, -1000.5598, -1001.7549, -1000.4963, -1002.6603, -1000.1799,\n",
      "         -999.7691, -1000.6658, -1001.3522, -1000.7967,  -998.9783,  -999.8658,\n",
      "        -1000.1474,  -999.0865, -1000.5601,  -999.5319, -1002.2901, -1000.9509,\n",
      "         -999.6830, -1000.8085,  -999.5192, -1000.6804, -1000.6794, -1000.4657,\n",
      "         -998.7005, -1000.6482, -1000.7057, -1000.6967,  -999.2673, -1001.4248,\n",
      "        -1000.7026, -1000.2173, -1001.0264,  -999.7845,  -999.5159, -1000.5806,\n",
      "         -999.6395, -1001.5303, -1000.0795, -1001.6904, -1001.9037, -1000.6452,\n",
      "         -999.6772,  -999.7440, -1000.3914,  -999.8948, -1001.0471,  -999.0560,\n",
      "        -1000.9604,  -998.8643,  -999.4192,  -999.9200,  -999.6475,  -999.5398,\n",
      "         -998.6946,  -999.4290, -1000.1583,  -999.2374,  -999.6577, -1000.4133,\n",
      "        -1000.5674,  -998.9541, -1001.6093, -1000.9302, -1000.6539, -1000.4282,\n",
      "         -999.6260, -1000.6773, -1000.9988,  -999.8769,  -999.0586, -1000.2438,\n",
      "         -998.6188,  -999.7947, -1000.6060,  -999.7160, -1000.0678,  -999.9111,\n",
      "        -1000.6599,  -999.8772, -1001.6024,  -999.9929, -1000.4659,  -999.6436,\n",
      "        -1000.5530, -1000.3845,  -998.5654,  -999.8530, -1001.6366, -1000.1143,\n",
      "         -999.9611, -1000.0089, -1001.3551, -1000.9088,  -998.9494, -1000.7506,\n",
      "         -998.6897,  -999.5714,  -999.1550, -1001.6442,  -998.3807,  -998.0057,\n",
      "         -999.0206, -1001.1599, -1001.5166, -1001.0402,  -998.5091,  -999.6593,\n",
      "        -1000.7374,  -998.5140,  -999.4319, -1001.1768, -1001.6023, -1000.2125,\n",
      "        -1000.6011, -1000.3085, -1002.6166,  -999.1411,  -999.5993,  -999.1334,\n",
      "        -1000.2369, -1001.0257,  -999.1438, -1000.5085,  -997.7526, -1000.6472,\n",
      "         -998.5662, -1000.5367, -1000.4912, -1001.3168,  -999.0176,  -998.8477,\n",
      "         -999.5095,  -999.9120,  -998.9496,  -998.6833, -1001.2725, -1001.8443,\n",
      "        -1000.2809,  -998.8912,  -999.3083,  -999.8475, -1001.0553,  -998.2780,\n",
      "         -999.1682, -1000.6547, -1000.4507, -1001.7666, -1000.0716,  -998.9133,\n",
      "         -999.8848,  -998.6863, -1000.9118,  -999.6597, -1000.8272,  -998.1872,\n",
      "        -1000.0362, -1000.1403, -1000.5439, -1000.1364,  -998.2107, -1001.0821,\n",
      "        -1000.3330, -1000.4001, -1001.1710, -1001.2867, -1001.6822, -1000.3013,\n",
      "        -1001.3938, -1000.3594,  -999.7979,  -999.1332,  -999.4023, -1001.6734,\n",
      "         -999.9663, -1000.8010,  -999.3898,  -999.3874, -1000.6620, -1000.1408,\n",
      "        -1000.1315,  -999.3537, -1001.9169, -1000.2800, -1000.9627, -1000.4772,\n",
      "        -1000.3846, -1001.4643,  -999.9359,  -998.1244, -1001.9128, -1000.0927,\n",
      "         -999.8931, -1000.2296,  -999.6462,  -999.9364,  -999.7145,  -999.6120,\n",
      "        -1002.0274, -1000.5903, -1001.1111,  -999.0593,  -999.4104,  -999.5371,\n",
      "        -1000.2339,  -998.6854,  -999.5737, -1001.1073,  -998.5527, -1001.5893,\n",
      "        -1000.6141, -1000.2505,  -999.8956,  -999.3586, -1001.0172, -1000.2668,\n",
      "         -999.8616,  -999.3119,  -999.7085,  -997.8560,  -999.3217,  -998.0732,\n",
      "         -999.3346, -1001.0753,  -999.9811, -1000.1981,  -998.6423,  -999.4768,\n",
      "        -1001.2878, -1001.7411,  -999.7413,  -998.3124, -1001.4761, -1001.1147,\n",
      "        -1000.0209,  -998.6795, -1000.5560,  -999.8852, -1000.9509, -1000.7828,\n",
      "        -1000.7451,  -999.6650,  -999.6961, -1000.2187,  -999.5540,  -999.7123,\n",
      "         -998.7631, -1003.1508,  -999.8779, -1000.6367,  -999.9756, -1000.8585,\n",
      "         -999.3842, -1000.7097,  -999.8276, -1001.8730,  -999.7781,  -998.7130,\n",
      "        -1001.4852, -1000.2605, -1000.6102, -1000.6027, -1001.0166,  -999.9117,\n",
      "         -999.6976, -1000.7726, -1000.2668,  -999.8945, -1000.3660,  -998.9520,\n",
      "         -998.6655,  -998.6978])\n",
      "log_p_max: -997.3482055664062\n",
      "log_p_shift: tensor([-3.7322, -0.9622, -2.1849, -2.8075, -2.9771, -3.6172, -3.3195, -2.5881,\n",
      "        -2.6851, -2.4412, -2.0184, -3.1047, -3.2648, -2.3234, -2.1479, -3.0651,\n",
      "        -0.9786, -2.0125, -1.9683, -2.0402, -2.7458, -1.3076, -3.1306, -0.7071,\n",
      "        -2.5740, -2.0301, -1.0107, -1.8989, -3.0704, -2.8061, -3.7466, -2.2526,\n",
      "        -1.8741, -2.3326, -2.8334, -4.6378, -2.5024, -2.0671, -2.0253, -3.7375,\n",
      "        -3.1280, -1.4363, -1.9911, -1.9388, -1.9634, -1.2364, -4.8690, -1.7187,\n",
      "        -2.2764, -1.3370, -1.0060, -4.3862, -2.3654, -4.6411, -2.1116, -3.9128,\n",
      "        -4.7585, -3.8677, -3.8763, -3.7427, -3.4077, -4.1586, -3.1677, -3.0776,\n",
      "        -3.0163, -3.0667, -1.7031, -2.4789, -2.4291,  0.0000, -3.0506, -2.5944,\n",
      "        -1.8559, -3.1417, -4.5027, -0.7129, -2.6031, -2.6298, -3.9877, -1.0736,\n",
      "        -2.9283, -2.5870, -3.0869, -3.9368, -1.8856, -4.0035, -0.5975, -3.0211,\n",
      "        -1.7728, -4.2453, -2.5629, -1.2885, -3.0182, -2.5958, -2.4941, -1.1278,\n",
      "        -4.8529, -3.0435, -3.0543, -2.3458, -0.9617, -3.1876, -2.5537, -0.6612,\n",
      "        -1.8376, -1.3280, -3.5547, -2.2256, -1.0919, -3.2976, -1.9157, -3.2844,\n",
      "        -2.3304, -2.4333, -3.3420, -2.9940, -0.9666, -2.8757, -3.9678, -0.8769,\n",
      "        -0.8193, -3.2722, -3.0398, -3.6270, -3.9177, -2.4230, -2.2672, -3.1277,\n",
      "        -2.8506, -2.1505, -1.6462, -2.8856, -2.3200, -2.5161, -3.1498, -3.4626,\n",
      "        -2.6873, -2.7393, -2.8000, -4.9540, -3.2097, -0.2117, -3.4027, -1.6893,\n",
      "        -2.9968, -2.3766, -2.8097, -2.5427, -4.1957, -3.8695, -3.3547, -1.2473,\n",
      "        -1.8909, -1.0381, -0.4005, -3.6334, -1.4442, -3.9089, -1.4778, -3.9188,\n",
      "        -2.2208, -1.3675, -2.6047, -3.8060, -2.9854, -3.5371, -2.0206, -0.6904,\n",
      "        -3.2368, -2.5893, -2.1221, -2.6295, -2.3500, -2.2289, -1.8132, -4.5125,\n",
      "        -1.8770, -4.4118, -2.9288, -3.9803, -3.6326, -2.6654, -3.2358, -3.4120,\n",
      "        -3.6490, -2.3495, -2.4817, -4.4800, -4.4673, -4.3613, -1.3246, -3.3191,\n",
      "        -2.1185, -4.4806, -2.5599, -0.6682, -2.8268, -1.5520, -3.2922, -2.1559,\n",
      "        -2.2318, -2.5800, -2.9418, -1.6329, -2.8122, -2.1512, -2.4352, -3.4280,\n",
      "        -3.0182, -2.7120, -1.2494, -3.3655, -3.0990, -2.8381, -1.1221, -2.6611,\n",
      "        -2.0371, -4.3480, -3.0140, -2.6970, -2.1893, -1.9307, -1.7021, -2.1658,\n",
      "        -2.1981, -2.2928, -2.4580, -1.9855, -2.4814, -3.2116, -4.4067, -3.1481,\n",
      "        -5.3121, -2.8317, -2.4209, -3.3176, -4.0040, -3.4485, -1.6301, -2.5176,\n",
      "        -2.7992, -1.7383, -3.2119, -2.1837, -4.9419, -3.6027, -2.3348, -3.4603,\n",
      "        -2.1710, -3.3322, -3.3312, -3.1175, -1.3523, -3.3000, -3.3575, -3.3485,\n",
      "        -1.9191, -4.0766, -3.3544, -2.8691, -3.6782, -2.4363, -2.1677, -3.2324,\n",
      "        -2.2913, -4.1821, -2.7313, -4.3422, -4.5555, -3.2970, -2.3290, -2.3958,\n",
      "        -3.0432, -2.5466, -3.6989, -1.7078, -3.6122, -1.5161, -2.0710, -2.5718,\n",
      "        -2.2993, -2.1916, -1.3464, -2.0808, -2.8101, -1.8892, -2.3095, -3.0651,\n",
      "        -3.2192, -1.6059, -4.2610, -3.5820, -3.3057, -3.0800, -2.2778, -3.3291,\n",
      "        -3.6506, -2.5287, -1.7104, -2.8956, -1.2706, -2.4465, -3.2578, -2.3678,\n",
      "        -2.7196, -2.5629, -3.3117, -2.5290, -4.2542, -2.6447, -3.1177, -2.2954,\n",
      "        -3.2048, -3.0363, -1.2172, -2.5048, -4.2884, -2.7661, -2.6129, -2.6606,\n",
      "        -4.0069, -3.5606, -1.6012, -3.4024, -1.3415, -2.2232, -1.8068, -4.2960,\n",
      "        -1.0325, -0.6575, -1.6724, -3.8117, -4.1684, -3.6920, -1.1609, -2.3111,\n",
      "        -3.3892, -1.1658, -2.0837, -3.8286, -4.2541, -2.8643, -3.2529, -2.9603,\n",
      "        -5.2684, -1.7929, -2.2511, -1.7852, -2.8887, -3.6775, -1.7956, -3.1603,\n",
      "        -0.4044, -3.2990, -1.2180, -3.1885, -3.1430, -3.9686, -1.6694, -1.4995,\n",
      "        -2.1613, -2.5638, -1.6014, -1.3351, -3.9243, -4.4961, -2.9327, -1.5430,\n",
      "        -1.9601, -2.4993, -3.7071, -0.9297, -1.8199, -3.3065, -3.1025, -4.4184,\n",
      "        -2.7234, -1.5651, -2.5366, -1.3381, -3.5636, -2.3115, -3.4790, -0.8390,\n",
      "        -2.6880, -2.7921, -3.1957, -2.7882, -0.8625, -3.7339, -2.9848, -3.0519,\n",
      "        -3.8228, -3.9385, -4.3340, -2.9531, -4.0456, -3.0112, -2.4496, -1.7850,\n",
      "        -2.0541, -4.3252, -2.6181, -3.4528, -2.0416, -2.0392, -3.3138, -2.7926,\n",
      "        -2.7833, -2.0055, -4.5687, -2.9318, -3.6145, -3.1290, -3.0364, -4.1161,\n",
      "        -2.5877, -0.7762, -4.5646, -2.7444, -2.5449, -2.8814, -2.2980, -2.5882,\n",
      "        -2.3663, -2.2638, -4.6792, -3.2421, -3.7629, -1.7111, -2.0622, -2.1889,\n",
      "        -2.8857, -1.3372, -2.2255, -3.7591, -1.2045, -4.2411, -3.2659, -2.9023,\n",
      "        -2.5474, -2.0104, -3.6689, -2.9186, -2.5134, -1.9637, -2.3603, -0.5078,\n",
      "        -1.9734, -0.7250, -1.9864, -3.7271, -2.6329, -2.8499, -1.2941, -2.1286,\n",
      "        -3.9396, -4.3929, -2.3931, -0.9642, -4.1279, -3.7665, -2.6727, -1.3313,\n",
      "        -3.2078, -2.5370, -3.6027, -3.4346, -3.3969, -2.3168, -2.3479, -2.8705,\n",
      "        -2.2057, -2.3641, -1.4149, -5.8026, -2.5297, -3.2885, -2.6274, -3.5103,\n",
      "        -2.0360, -3.3615, -2.4794, -4.5248, -2.4299, -1.3648, -4.1370, -2.9123,\n",
      "        -3.2620, -3.2545, -3.6684, -2.5635, -2.3494, -3.4244, -2.9186, -2.5463,\n",
      "        -3.0178, -1.6038, -1.3173, -1.3496])\n",
      "p_shift: tensor([0.0239, 0.3821, 0.1125, 0.0604, 0.0509, 0.0269, 0.0362, 0.0752, 0.0682,\n",
      "        0.0871, 0.1329, 0.0448, 0.0382, 0.0979, 0.1167, 0.0466, 0.3758, 0.1337,\n",
      "        0.1397, 0.1300, 0.0642, 0.2705, 0.0437, 0.4931, 0.0762, 0.1313, 0.3640,\n",
      "        0.1497, 0.0464, 0.0604, 0.0236, 0.1051, 0.1535, 0.0970, 0.0588, 0.0097,\n",
      "        0.0819, 0.1265, 0.1320, 0.0238, 0.0438, 0.2378, 0.1365, 0.1439, 0.1404,\n",
      "        0.2904, 0.0077, 0.1793, 0.1027, 0.2626, 0.3657, 0.0124, 0.0939, 0.0096,\n",
      "        0.1210, 0.0200, 0.0086, 0.0209, 0.0207, 0.0237, 0.0331, 0.0156, 0.0421,\n",
      "        0.0461, 0.0490, 0.0466, 0.1821, 0.0838, 0.0881, 1.0000, 0.0473, 0.0747,\n",
      "        0.1563, 0.0432, 0.0111, 0.4902, 0.0740, 0.0721, 0.0185, 0.3418, 0.0535,\n",
      "        0.0752, 0.0456, 0.0195, 0.1517, 0.0183, 0.5502, 0.0487, 0.1699, 0.0143,\n",
      "        0.0771, 0.2757, 0.0489, 0.0746, 0.0826, 0.3237, 0.0078, 0.0477, 0.0472,\n",
      "        0.0958, 0.3823, 0.0413, 0.0778, 0.5162, 0.1592, 0.2650, 0.0286, 0.1080,\n",
      "        0.3356, 0.0370, 0.1472, 0.0375, 0.0973, 0.0877, 0.0354, 0.0501, 0.3804,\n",
      "        0.0564, 0.0189, 0.4161, 0.4407, 0.0379, 0.0478, 0.0266, 0.0199, 0.0887,\n",
      "        0.1036, 0.0438, 0.0578, 0.1164, 0.1928, 0.0558, 0.0983, 0.0808, 0.0429,\n",
      "        0.0313, 0.0681, 0.0646, 0.0608, 0.0071, 0.0404, 0.8092, 0.0333, 0.1846,\n",
      "        0.0499, 0.0929, 0.0602, 0.0787, 0.0151, 0.0209, 0.0349, 0.2873, 0.1509,\n",
      "        0.3541, 0.6700, 0.0264, 0.2359, 0.0201, 0.2281, 0.0199, 0.1085, 0.2547,\n",
      "        0.0739, 0.0222, 0.0505, 0.0291, 0.1326, 0.5014, 0.0393, 0.0751, 0.1198,\n",
      "        0.0721, 0.0954, 0.1076, 0.1631, 0.0110, 0.1530, 0.0121, 0.0535, 0.0187,\n",
      "        0.0264, 0.0696, 0.0393, 0.0330, 0.0260, 0.0954, 0.0836, 0.0113, 0.0115,\n",
      "        0.0128, 0.2659, 0.0362, 0.1202, 0.0113, 0.0773, 0.5127, 0.0592, 0.2118,\n",
      "        0.0372, 0.1158, 0.1073, 0.0758, 0.0528, 0.1954, 0.0601, 0.1163, 0.0876,\n",
      "        0.0325, 0.0489, 0.0664, 0.2867, 0.0345, 0.0451, 0.0585, 0.3256, 0.0699,\n",
      "        0.1304, 0.0129, 0.0491, 0.0674, 0.1120, 0.1450, 0.1823, 0.1147, 0.1110,\n",
      "        0.1010, 0.0856, 0.1373, 0.0836, 0.0403, 0.0122, 0.0429, 0.0049, 0.0589,\n",
      "        0.0888, 0.0362, 0.0182, 0.0318, 0.1959, 0.0806, 0.0609, 0.1758, 0.0403,\n",
      "        0.1126, 0.0071, 0.0273, 0.0968, 0.0314, 0.1141, 0.0357, 0.0358, 0.0443,\n",
      "        0.2586, 0.0369, 0.0348, 0.0351, 0.1467, 0.0170, 0.0349, 0.0568, 0.0253,\n",
      "        0.0875, 0.1144, 0.0395, 0.1011, 0.0153, 0.0651, 0.0130, 0.0105, 0.0370,\n",
      "        0.0974, 0.0911, 0.0477, 0.0783, 0.0248, 0.1813, 0.0270, 0.2196, 0.1261,\n",
      "        0.0764, 0.1003, 0.1117, 0.2602, 0.1248, 0.0602, 0.1512, 0.0993, 0.0467,\n",
      "        0.0400, 0.2007, 0.0141, 0.0278, 0.0367, 0.0460, 0.1025, 0.0358, 0.0260,\n",
      "        0.0798, 0.1808, 0.0553, 0.2807, 0.0866, 0.0385, 0.0937, 0.0659, 0.0771,\n",
      "        0.0365, 0.0797, 0.0142, 0.0710, 0.0443, 0.1007, 0.0406, 0.0480, 0.2961,\n",
      "        0.0817, 0.0137, 0.0629, 0.0733, 0.0699, 0.0182, 0.0284, 0.2017, 0.0333,\n",
      "        0.2615, 0.1083, 0.1642, 0.0136, 0.3561, 0.5182, 0.1878, 0.0221, 0.0155,\n",
      "        0.0249, 0.3132, 0.0992, 0.0337, 0.3117, 0.1245, 0.0217, 0.0142, 0.0570,\n",
      "        0.0387, 0.0518, 0.0052, 0.1665, 0.1053, 0.1678, 0.0557, 0.0253, 0.1660,\n",
      "        0.0424, 0.6674, 0.0369, 0.2958, 0.0412, 0.0432, 0.0189, 0.1884, 0.2232,\n",
      "        0.1152, 0.0770, 0.2016, 0.2631, 0.0198, 0.0112, 0.0533, 0.2137, 0.1408,\n",
      "        0.0821, 0.0245, 0.3947, 0.1620, 0.0366, 0.0449, 0.0121, 0.0657, 0.2091,\n",
      "        0.0791, 0.2623, 0.0283, 0.0991, 0.0308, 0.4321, 0.0680, 0.0613, 0.0409,\n",
      "        0.0615, 0.4221, 0.0239, 0.0505, 0.0473, 0.0219, 0.0195, 0.0131, 0.0522,\n",
      "        0.0175, 0.0492, 0.0863, 0.1678, 0.1282, 0.0132, 0.0729, 0.0317, 0.1298,\n",
      "        0.1301, 0.0364, 0.0613, 0.0618, 0.1346, 0.0104, 0.0533, 0.0269, 0.0438,\n",
      "        0.0480, 0.0163, 0.0752, 0.4602, 0.0104, 0.0643, 0.0785, 0.0561, 0.1005,\n",
      "        0.0752, 0.0938, 0.1040, 0.0093, 0.0391, 0.0232, 0.1807, 0.1272, 0.1120,\n",
      "        0.0558, 0.2626, 0.1080, 0.0233, 0.2999, 0.0144, 0.0382, 0.0549, 0.0783,\n",
      "        0.1339, 0.0255, 0.0540, 0.0810, 0.1403, 0.0944, 0.6018, 0.1390, 0.4843,\n",
      "        0.1372, 0.0241, 0.0719, 0.0578, 0.2742, 0.1190, 0.0195, 0.0124, 0.0913,\n",
      "        0.3813, 0.0161, 0.0231, 0.0691, 0.2641, 0.0404, 0.0791, 0.0272, 0.0322,\n",
      "        0.0335, 0.0986, 0.0956, 0.0567, 0.1102, 0.0940, 0.2430, 0.0030, 0.0797,\n",
      "        0.0373, 0.0723, 0.0299, 0.1305, 0.0347, 0.0838, 0.0108, 0.0880, 0.2554,\n",
      "        0.0160, 0.0544, 0.0383, 0.0386, 0.0255, 0.0770, 0.0954, 0.0326, 0.0540,\n",
      "        0.0784, 0.0489, 0.2011, 0.2679, 0.2593])\n",
      "p total: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "val = -1000\n",
    "size = 500\n",
    "\n",
    "logp = torch.tensor([val + torch.randn(1) for _ in range(size)])\n",
    "print(f'logp: {logp}')\n",
    "\n",
    "log_p_max = torch.max(logp)\n",
    "print(f'log_p_max: {log_p_max}')\n",
    "\n",
    "log_p_shift = logp - log_p_max\n",
    "print(f'log_p_shift: {log_p_shift}')\n",
    "p_shift = torch.exp(log_p_shift)\n",
    "print(f'p_shift: {p_shift}')\n",
    "\n",
    "p = torch.exp(log_p_shift) * torch.exp(log_p_max)\n",
    "print(f'p total: {p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the formula to calculate the score estimator \n",
    "\n",
    "# smth like expectation under (p G|Z) of = p(G|Z) * grad_z p(G|Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\n",
      "============================================================\n",
      "Original log probabilities: tensor([-1000.5000, -1001.2000,  -999.8000, -1002.1000, -1000.9000])\n",
      "Gradient values shape: torch.Size([5, 3, 2, 2])\n",
      "Sample gradient values:\n",
      "tensor([[[ 0.1927,  0.1487],\n",
      "         [ 0.0901, -0.2106]],\n",
      "\n",
      "        [[ 0.0678, -0.1235],\n",
      "         [-0.0043, -0.1605]],\n",
      "\n",
      "        [[-0.0752,  0.1649],\n",
      "         [-0.0392, -0.1404]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 1: NUMERICALLY UNSTABLE\n",
      "--------------------------------------------------\n",
      "Direct exp(log_p): tensor([0., 0., 0., 0., 0.])\n",
      "Sum of exp(log_p): 0.0\n",
      "Weights (unstable): tensor([nan, nan, nan, nan, nan])\n",
      "Sum of weights: nan\n",
      "Weighted gradient (unstable):\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 2: NUMERICALLY STABLE\n",
      "--------------------------------------------------\n",
      "Max log probability: -999.7999877929688\n",
      "Shifted log probabilities: tensor([-0.7000, -1.4000,  0.0000, -2.3000, -1.1000])\n",
      "Exp of shifted log_p: tensor([0.4966, 0.2466, 1.0000, 0.1003, 0.3329])\n",
      "Weights (stable): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 1.0\n",
      "Weighted gradient (stable):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 3: USING PYTORCH'S LOGSUMEXP\n",
      "--------------------------------------------------\n",
      "Logsumexp result: -999.0223388671875\n",
      "Weights (logsumexp): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 0.9999722838401794\n",
      "Weighted gradient (logsumexp):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "COMPARISON\n",
      "--------------------------------------------------\n",
      "Weight comparison:\n",
      "Stable weights:      tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Logsumexp weights:   tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Unstable weights:    tensor([nan, nan, nan, nan, nan])\n",
      "\n",
      "Gradient comparison:\n",
      "Stable vs Logsumexp gradient difference: 5.937592504778877e-06\n",
      "Stable vs Unstable gradient difference: nan\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\n",
      "============================================================\n",
      "z shape: torch.Size([3, 2, 2])\n",
      "theta shape: torch.Size([3, 3])\n",
      "data['x'] shape: torch.Size([100, 3])\n",
      "\n",
      "Log probabilities range: [-1004.72, -995.40]\n",
      "Gradient tensor shape: torch.Size([64, 3, 2, 2])\n",
      "\n",
      "--------------------------------------------------\n",
      "STABLE SCORE ESTIMATOR COMPUTATION\n",
      "--------------------------------------------------\n",
      "Max log probability: -995.40\n",
      "Weights sum: 1.000000\n",
      "Weight range: [0.000030, 0.330849]\n",
      "Final weighted gradient shape: torch.Size([3, 2, 2])\n",
      "Gradient norm: 0.123335\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dummy_score_estimator_stable_vs_unstable():\n",
    "    \"\"\"\n",
    "    Demonstrates numerically stable vs unstable score estimator computation\n",
    "    using dummy values to show the difference.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    n_samples = 5\n",
    "    d = 3  # dimension of z\n",
    "    z_dim = (d, 2, 2)  # shape of z tensor\n",
    "    \n",
    "    # Generate dummy log probabilities (very small values to demonstrate instability)\n",
    "    log_p_values = torch.tensor([-1000.5, -1001.2, -999.8, -1002.1, -1000.9])\n",
    "    print(f\"Original log probabilities: {log_p_values}\")\n",
    "    \n",
    "    # Generate dummy gradient values\n",
    "    grad_values = torch.randn(n_samples, *z_dim) * 0.1\n",
    "    print(f\"Gradient values shape: {grad_values.shape}\")\n",
    "    print(f\"Sample gradient values:\\n{grad_values[0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 1: NUMERICALLY UNSTABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Unstable method: direct exponentiation\n",
    "    try:\n",
    "        p_unstable = torch.exp(log_p_values)\n",
    "        print(f\"Direct exp(log_p): {p_unstable}\")\n",
    "        print(f\"Sum of exp(log_p): {p_unstable.sum()}\")\n",
    "        \n",
    "        # Normalize to get weights\n",
    "        weights_unstable = p_unstable / p_unstable.sum()\n",
    "        print(f\"Weights (unstable): {weights_unstable}\")\n",
    "        print(f\"Sum of weights: {weights_unstable.sum()}\")\n",
    "        \n",
    "        # Compute weighted gradient\n",
    "        weighted_grad_unstable = torch.zeros_like(grad_values[0])\n",
    "        for i in range(n_samples):\n",
    "            weighted_grad_unstable += weights_unstable[i] * grad_values[i]\n",
    "        \n",
    "        print(f\"Weighted gradient (unstable):\\n{weighted_grad_unstable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unstable method failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 2: NUMERICALLY STABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Stable method: logsumexp trick\n",
    "    log_p_max = torch.max(log_p_values)\n",
    "    print(f\"Max log probability: {log_p_max}\")\n",
    "    \n",
    "    log_p_shifted = log_p_values - log_p_max\n",
    "    print(f\"Shifted log probabilities: {log_p_shifted}\")\n",
    "    \n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    print(f\"Exp of shifted log_p: {p_shifted}\")\n",
    "    \n",
    "    # Normalize to get weights\n",
    "    weights_stable = p_shifted / p_shifted.sum()\n",
    "    print(f\"Weights (stable): {weights_stable}\")\n",
    "    print(f\"Sum of weights: {weights_stable.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_stable = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_stable += weights_stable[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (stable):\\n{weighted_grad_stable}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 3: USING PYTORCH'S LOGSUMEXP\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Using PyTorch's built-in logsumexp\n",
    "    log_sum_exp = torch.logsumexp(log_p_values, dim=0)\n",
    "    print(f\"Logsumexp result: {log_sum_exp}\")\n",
    "    \n",
    "    # Compute weights using logsumexp\n",
    "    weights_logsumexp = torch.exp(log_p_values - log_sum_exp)\n",
    "    print(f\"Weights (logsumexp): {weights_logsumexp}\")\n",
    "    print(f\"Sum of weights: {weights_logsumexp.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_logsumexp = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_logsumexp += weights_logsumexp[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (logsumexp):\\n{weighted_grad_logsumexp}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compare the methods\n",
    "    print(\"Weight comparison:\")\n",
    "    print(f\"Stable weights:      {weights_stable}\")\n",
    "    print(f\"Logsumexp weights:   {weights_logsumexp}\")\n",
    "    \n",
    "    if 'weights_unstable' in locals():\n",
    "        print(f\"Unstable weights:    {weights_unstable}\")\n",
    "    \n",
    "    print(\"\\nGradient comparison:\")\n",
    "    print(f\"Stable vs Logsumexp gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_logsumexp)}\")\n",
    "    \n",
    "    if 'weighted_grad_unstable' in locals():\n",
    "        print(f\"Stable vs Unstable gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_unstable)}\")\n",
    "    \n",
    "    return {\n",
    "        'weights_stable': weights_stable,\n",
    "        'weights_logsumexp': weights_logsumexp,\n",
    "        'grad_stable': weighted_grad_stable,\n",
    "        'grad_logsumexp': weighted_grad_logsumexp\n",
    "    }\n",
    "\n",
    "def score_estimator_with_dummy_data():\n",
    "    \"\"\"\n",
    "    Complete score estimator function using dummy data to demonstrate\n",
    "    the stable computation method.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    torch.manual_seed(42)\n",
    "    n_samples = 64\n",
    "    d = 3\n",
    "    z = torch.randn(d, 2, 2) * 0.1\n",
    "    theta = torch.randn(d, d) * 0.1\n",
    "    data = {'x': torch.randn(100, d)}\n",
    "    hparams = {\n",
    "        'sigma_z': 1.0,\n",
    "        'beta': 1.0,\n",
    "        'alpha': 1.0,\n",
    "        'n_mc_samples': n_samples\n",
    "    }\n",
    "    \n",
    "    print(f\"z shape: {z.shape}\")\n",
    "    print(f\"theta shape: {theta.shape}\")\n",
    "    print(f\"data['x'] shape: {data['x'].shape}\")\n",
    "    \n",
    "    # Simulate the score estimator computation\n",
    "    log_p_samples = []\n",
    "    grad_z_samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Dummy log joint probability (very small values)\n",
    "        log_p = -1000 + torch.randn(1) * 2.0\n",
    "        log_p_samples.append(log_p)\n",
    "        \n",
    "        # Dummy gradient of z\n",
    "        grad_z = torch.randn_like(z) * 0.1\n",
    "        grad_z_samples.append(grad_z)\n",
    "    \n",
    "    log_p_tensor = torch.stack(log_p_samples).squeeze()\n",
    "    grad_z_tensor = torch.stack(grad_z_samples)\n",
    "    \n",
    "    print(f\"\\nLog probabilities range: [{log_p_tensor.min():.2f}, {log_p_tensor.max():.2f}]\")\n",
    "    print(f\"Gradient tensor shape: {grad_z_tensor.shape}\")\n",
    "    \n",
    "    # STABLE COMPUTATION\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"STABLE SCORE ESTIMATOR COMPUTATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Compute stable weights using logsumexp\n",
    "    log_p_max = torch.max(log_p_tensor)\n",
    "    log_p_shifted = log_p_tensor - log_p_max\n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    weights = p_shifted / p_shifted.sum()\n",
    "    \n",
    "    print(f\"Max log probability: {log_p_max:.2f}\")\n",
    "    print(f\"Weights sum: {weights.sum():.6f}\")\n",
    "    print(f\"Weight range: [{weights.min():.6f}, {weights.max():.6f}]\")\n",
    "    \n",
    "    # 2. Compute weighted gradient\n",
    "    weighted_grad = torch.zeros_like(grad_z_tensor[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad += weights[i] * grad_z_tensor[i]\n",
    "    \n",
    "    print(f\"Final weighted gradient shape: {weighted_grad.shape}\")\n",
    "    print(f\"Gradient norm: {torch.norm(weighted_grad):.6f}\")\n",
    "    \n",
    "    return weighted_grad\n",
    "\n",
    "# Run the demonstrations\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the comparison\n",
    "    results = dummy_score_estimator_stable_vs_unstable()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Run the complete estimator\n",
    "    final_grad = score_estimator_with_dummy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\n",
      "============================================================\n",
      "Original log probabilities: tensor([-1000.5000, -1001.2000,  -999.8000, -1002.1000, -1000.9000])\n",
      "Gradient values shape: torch.Size([5, 3, 2, 2])\n",
      "Sample gradient values:\n",
      "tensor([[[ 0.1927,  0.1487],\n",
      "         [ 0.0901, -0.2106]],\n",
      "\n",
      "        [[ 0.0678, -0.1235],\n",
      "         [-0.0043, -0.1605]],\n",
      "\n",
      "        [[-0.0752,  0.1649],\n",
      "         [-0.0392, -0.1404]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 1: NUMERICALLY UNSTABLE\n",
      "--------------------------------------------------\n",
      "Direct exp(log_p): tensor([0., 0., 0., 0., 0.])\n",
      "Sum of exp(log_p): 0.0\n",
      "Weights (unstable): tensor([nan, nan, nan, nan, nan])\n",
      "Sum of weights: nan\n",
      "Weighted gradient (unstable):\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 2: NUMERICALLY STABLE\n",
      "--------------------------------------------------\n",
      "Max log probability: -999.7999877929688\n",
      "Shifted log probabilities: tensor([-0.7000, -1.4000,  0.0000, -2.3000, -1.1000])\n",
      "Exp of shifted log_p: tensor([0.4966, 0.2466, 1.0000, 0.1003, 0.3329])\n",
      "Weights (stable): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 1.0\n",
      "Weighted gradient (stable):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "METHOD 3: USING PYTORCH'S LOGSUMEXP\n",
      "--------------------------------------------------\n",
      "Logsumexp result: -999.0223388671875\n",
      "Weights (logsumexp): tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Sum of weights: 0.9999722838401794\n",
      "Weighted gradient (logsumexp):\n",
      "tensor([[[ 0.1181,  0.0983],\n",
      "         [ 0.0752,  0.0264]],\n",
      "\n",
      "        [[ 0.0342, -0.0149],\n",
      "         [-0.0292, -0.0067]],\n",
      "\n",
      "        [[-0.0903,  0.0186],\n",
      "         [-0.0029,  0.0704]]])\n",
      "\n",
      "--------------------------------------------------\n",
      "COMPARISON\n",
      "--------------------------------------------------\n",
      "Weight comparison:\n",
      "Stable weights:      tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Logsumexp weights:   tensor([0.2282, 0.1133, 0.4595, 0.0461, 0.1529])\n",
      "Unstable weights:    tensor([nan, nan, nan, nan, nan])\n",
      "\n",
      "Gradient comparison:\n",
      "Stable vs Logsumexp gradient difference: 5.937592504778877e-06\n",
      "Stable vs Unstable gradient difference: nan\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\n",
      "============================================================\n",
      "z shape: torch.Size([3, 2, 2])\n",
      "theta shape: torch.Size([3, 3])\n",
      "data['x'] shape: torch.Size([100, 3])\n",
      "\n",
      "Log probabilities range: [-1004.72, -995.40]\n",
      "Gradient tensor shape: torch.Size([64, 3, 2, 2])\n",
      "\n",
      "--------------------------------------------------\n",
      "STABLE SCORE ESTIMATOR COMPUTATION\n",
      "--------------------------------------------------\n",
      "Max log probability: -995.40\n",
      "Weights sum: 1.000000\n",
      "Weight range: [0.000030, 0.330849]\n",
      "Final weighted gradient shape: torch.Size([3, 2, 2])\n",
      "Gradient norm: 0.123335\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "VERIFYING GRADIENT AVERAGING CORRECTNESS\n",
      "============================================================\n",
      "Log probabilities: tensor([-1.0000, -2.0000, -1.5000, -3.0000, -1.2000, -2.5000, -1.8000, -2.2000,\n",
      "        -1.3000, -2.8000])\n",
      "Gradient shape: torch.Size([10, 2, 2])\n",
      "\n",
      "Simple average gradient:\n",
      "tensor([[ 0.3118,  0.3731],\n",
      "        [-0.0852, -0.2451]])\n",
      "\n",
      "Weights: tensor([0.2080, 0.0765, 0.1261, 0.0281, 0.1703, 0.0464, 0.0934, 0.0626, 0.1541,\n",
      "        0.0344])\n",
      "Sum of weights: 1.000000\n",
      "Weighted average gradient:\n",
      "tensor([[ 0.4893,  0.6262],\n",
      "        [ 0.1042, -0.3597]])\n",
      "\n",
      "Softmax weights: tensor([0.2080, 0.0765, 0.1261, 0.0281, 0.1703, 0.0464, 0.0934, 0.0626, 0.1541,\n",
      "        0.0344])\n",
      "Weighted average (softmax):\n",
      "tensor([[ 0.4893,  0.6262],\n",
      "        [ 0.1042, -0.3597]])\n",
      "\n",
      "Consistency checks:\n",
      "Weights vs Softmax weights difference: 3.54e-08\n",
      "Weighted avg vs Softmax avg difference: 1.88e-07\n",
      "\n",
      "Weight sums:\n",
      "Stable weights sum: 1.000000\n",
      "Softmax weights sum: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dummy_score_estimator_stable_vs_unstable():\n",
    "    \"\"\"\n",
    "    Demonstrates numerically stable vs unstable score estimator computation\n",
    "    with mathematically correct gradient averaging.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SCORE ESTIMATOR: STABLE VS UNSTABLE COMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    n_samples = 5\n",
    "    d = 3  # dimension of z\n",
    "    z_dim = (d, 2, 2)  # shape of z tensor\n",
    "    \n",
    "    # Generate dummy log probabilities (very small values to demonstrate instability)\n",
    "    log_p_values = torch.tensor([-1000.5, -1001.2, -999.8, -1002.1, -1000.9])\n",
    "    print(f\"Original log probabilities: {log_p_values}\")\n",
    "    \n",
    "    # Generate dummy gradient values\n",
    "    grad_values = torch.randn(n_samples, *z_dim) * 0.1\n",
    "    print(f\"Gradient values shape: {grad_values.shape}\")\n",
    "    print(f\"Sample gradient values:\\n{grad_values[0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 1: NUMERICALLY UNSTABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Unstable method: direct exponentiation\n",
    "    try:\n",
    "        p_unstable = torch.exp(log_p_values)\n",
    "        print(f\"Direct exp(log_p): {p_unstable}\")\n",
    "        print(f\"Sum of exp(log_p): {p_unstable.sum()}\")\n",
    "        \n",
    "        # Normalize to get weights\n",
    "        weights_unstable = p_unstable / p_unstable.sum()\n",
    "        print(f\"Weights (unstable): {weights_unstable}\")\n",
    "        print(f\"Sum of weights: {weights_unstable.sum()}\")\n",
    "        \n",
    "        # Compute weighted gradient\n",
    "        weighted_grad_unstable = torch.zeros_like(grad_values[0])\n",
    "        for i in range(n_samples):\n",
    "            weighted_grad_unstable += weights_unstable[i] * grad_values[i]\n",
    "        \n",
    "        print(f\"Weighted gradient (unstable):\\n{weighted_grad_unstable}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unstable method failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 2: NUMERICALLY STABLE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Stable method: logsumexp trick\n",
    "    log_p_max = torch.max(log_p_values)\n",
    "    print(f\"Max log probability: {log_p_max}\")\n",
    "    \n",
    "    log_p_shifted = log_p_values - log_p_max\n",
    "    print(f\"Shifted log probabilities: {log_p_shifted}\")\n",
    "    \n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    print(f\"Exp of shifted log_p: {p_shifted}\")\n",
    "    \n",
    "    # Normalize to get weights\n",
    "    weights_stable = p_shifted / p_shifted.sum()\n",
    "    print(f\"Weights (stable): {weights_stable}\")\n",
    "    print(f\"Sum of weights: {weights_stable.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_stable = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_stable += weights_stable[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (stable):\\n{weighted_grad_stable}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"METHOD 3: USING PYTORCH'S LOGSUMEXP\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Using PyTorch's built-in logsumexp\n",
    "    log_sum_exp = torch.logsumexp(log_p_values, dim=0)\n",
    "    print(f\"Logsumexp result: {log_sum_exp}\")\n",
    "    \n",
    "    # Compute weights using logsumexp\n",
    "    weights_logsumexp = torch.exp(log_p_values - log_sum_exp)\n",
    "    print(f\"Weights (logsumexp): {weights_logsumexp}\")\n",
    "    print(f\"Sum of weights: {weights_logsumexp.sum()}\")\n",
    "    \n",
    "    # Compute weighted gradient\n",
    "    weighted_grad_logsumexp = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad_logsumexp += weights_logsumexp[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted gradient (logsumexp):\\n{weighted_grad_logsumexp}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compare the methods\n",
    "    print(\"Weight comparison:\")\n",
    "    print(f\"Stable weights:      {weights_stable}\")\n",
    "    print(f\"Logsumexp weights:   {weights_logsumexp}\")\n",
    "    \n",
    "    if 'weights_unstable' in locals():\n",
    "        print(f\"Unstable weights:    {weights_unstable}\")\n",
    "    \n",
    "    print(\"\\nGradient comparison:\")\n",
    "    print(f\"Stable vs Logsumexp gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_logsumexp)}\")\n",
    "    \n",
    "    if 'weighted_grad_unstable' in locals():\n",
    "        print(f\"Stable vs Unstable gradient difference: {torch.norm(weighted_grad_stable - weighted_grad_unstable)}\")\n",
    "    \n",
    "    return {\n",
    "        'weights_stable': weights_stable,\n",
    "        'weights_logsumexp': weights_logsumexp,\n",
    "        'grad_stable': weighted_grad_stable,\n",
    "        'grad_logsumexp': weighted_grad_logsumexp\n",
    "    }\n",
    "\n",
    "def score_estimator_with_dummy_data():\n",
    "    \"\"\"\n",
    "    Complete score estimator function using dummy data to demonstrate\n",
    "    the stable computation method with correct gradient averaging.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPLETE SCORE ESTIMATOR WITH DUMMY DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dummy parameters\n",
    "    torch.manual_seed(42)\n",
    "    n_samples = 64\n",
    "    d = 3\n",
    "    z = torch.randn(d, 2, 2) * 0.1\n",
    "    theta = torch.randn(d, d) * 0.1\n",
    "    data = {'x': torch.randn(100, d)}\n",
    "    hparams = {\n",
    "        'sigma_z': 1.0,\n",
    "        'beta': 1.0,\n",
    "        'alpha': 1.0,\n",
    "        'n_mc_samples': n_samples\n",
    "    }\n",
    "    \n",
    "    print(f\"z shape: {z.shape}\")\n",
    "    print(f\"theta shape: {theta.shape}\")\n",
    "    print(f\"data['x'] shape: {data['x'].shape}\")\n",
    "    \n",
    "    # Simulate the score estimator computation\n",
    "    log_p_samples = []\n",
    "    grad_z_samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Dummy log joint probability (very small values)\n",
    "        log_p = -1000 + torch.randn(1) * 2.0\n",
    "        log_p_samples.append(log_p)\n",
    "        \n",
    "        # Dummy gradient of z\n",
    "        grad_z = torch.randn_like(z) * 0.1\n",
    "        grad_z_samples.append(grad_z)\n",
    "    \n",
    "    log_p_tensor = torch.stack(log_p_samples).squeeze()\n",
    "    grad_z_tensor = torch.stack(grad_z_samples)\n",
    "    \n",
    "    print(f\"\\nLog probabilities range: [{log_p_tensor.min():.2f}, {log_p_tensor.max():.2f}]\")\n",
    "    print(f\"Gradient tensor shape: {grad_z_tensor.shape}\")\n",
    "    \n",
    "    # STABLE COMPUTATION\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"STABLE SCORE ESTIMATOR COMPUTATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Compute stable weights using logsumexp\n",
    "    log_p_max = torch.max(log_p_tensor)\n",
    "    log_p_shifted = log_p_tensor - log_p_max\n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    weights = p_shifted / p_shifted.sum()\n",
    "    \n",
    "    print(f\"Max log probability: {log_p_max:.2f}\")\n",
    "    print(f\"Weights sum: {weights.sum():.6f}\")\n",
    "    print(f\"Weight range: [{weights.min():.6f}, {weights.max():.6f}]\")\n",
    "    \n",
    "    # 2. Compute weighted gradient\n",
    "    weighted_grad = torch.zeros_like(grad_z_tensor[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_grad += weights[i] * grad_z_tensor[i]\n",
    "    \n",
    "    print(f\"Final weighted gradient shape: {weighted_grad.shape}\")\n",
    "    print(f\"Gradient norm: {torch.norm(weighted_grad):.6f}\")\n",
    "    \n",
    "    return weighted_grad\n",
    "\n",
    "def verify_gradient_averaging():\n",
    "    \"\"\"\n",
    "    Verify that the gradient averaging is mathematically correct\n",
    "    by comparing with simple average and weighted average.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"VERIFYING GRADIENT AVERAGING CORRECTNESS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    n_samples = 10\n",
    "    d = 2\n",
    "    z_dim = (d, 2)\n",
    "    \n",
    "    # Create simple test case with known probabilities\n",
    "    log_p_values = torch.tensor([-1.0, -2.0, -1.5, -3.0, -1.2, -2.5, -1.8, -2.2, -1.3, -2.8])\n",
    "    grad_values = torch.randn(n_samples, *z_dim)\n",
    "    \n",
    "    print(f\"Log probabilities: {log_p_values}\")\n",
    "    print(f\"Gradient shape: {grad_values.shape}\")\n",
    "    \n",
    "    # Method 1: Simple average (equal weights)\n",
    "    simple_avg = grad_values.mean(dim=0)\n",
    "    print(f\"\\nSimple average gradient:\\n{simple_avg}\")\n",
    "    \n",
    "    # Method 2: Weighted average using stable method\n",
    "    log_p_max = torch.max(log_p_values)\n",
    "    log_p_shifted = log_p_values - log_p_max\n",
    "    p_shifted = torch.exp(log_p_shifted)\n",
    "    weights = p_shifted / p_shifted.sum()\n",
    "    \n",
    "    print(f\"\\nWeights: {weights}\")\n",
    "    print(f\"Sum of weights: {weights.sum():.6f}\")\n",
    "    \n",
    "    weighted_avg = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_avg += weights[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"Weighted average gradient:\\n{weighted_avg}\")\n",
    "    \n",
    "    # Method 3: Using PyTorch's softmax\n",
    "    weights_softmax = F.softmax(log_p_values, dim=0)\n",
    "    weighted_avg_softmax = torch.zeros_like(grad_values[0])\n",
    "    for i in range(n_samples):\n",
    "        weighted_avg_softmax += weights_softmax[i] * grad_values[i]\n",
    "    \n",
    "    print(f\"\\nSoftmax weights: {weights_softmax}\")\n",
    "    print(f\"Weighted average (softmax):\\n{weighted_avg_softmax}\")\n",
    "    \n",
    "    # Verify consistency\n",
    "    print(f\"\\nConsistency checks:\")\n",
    "    print(f\"Weights vs Softmax weights difference: {torch.norm(weights - weights_softmax):.2e}\")\n",
    "    print(f\"Weighted avg vs Softmax avg difference: {torch.norm(weighted_avg - weighted_avg_softmax):.2e}\")\n",
    "    \n",
    "    # Show that weights sum to 1\n",
    "    print(f\"\\nWeight sums:\")\n",
    "    print(f\"Stable weights sum: {weights.sum():.6f}\")\n",
    "    print(f\"Softmax weights sum: {weights_softmax.sum():.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'simple_avg': simple_avg,\n",
    "        'weighted_avg': weighted_avg,\n",
    "        'weighted_avg_softmax': weighted_avg_softmax,\n",
    "        'weights': weights,\n",
    "        'weights_softmax': weights_softmax\n",
    "    }\n",
    "\n",
    "# Run the demonstrations\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the comparison\n",
    "    results = dummy_score_estimator_stable_vs_unstable()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Run the complete estimator\n",
    "    final_grad = score_estimator_with_dummy_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Verify gradient averaging\n",
    "    verification = verify_gradient_averaging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's assume the calculation for the score estimator is done as above\n",
    "THe numerator and the denominator is needed to calculate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z_score_with_ratio_stable(z: torch.Tensor, data: Dict[str, Any], theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes _z log p(Z,|D) using equation (10) with stable ratio computation.\n",
    "    \n",
    "    Equation (10): _Z log p(Z,|D) = _Z log p(Z) + [_Z E[p(,D|G)]] / [E[p(,D|G)]]\n",
    "    \"\"\"\n",
    "    d = z.shape[0]\n",
    "    n_samples = hparams.get('n_mc_samples', 64)\n",
    "\n",
    "    # --- Gradient of the Prior ---\n",
    "    grad_z_prior = -(z / hparams['sigma_z']**2)\n",
    "\n",
    "    # --- Collect samples for both numerator and denominator ---\n",
    "    log_joint_rewards = []  # log p(,D|G) for each sample\n",
    "    scores_list = []        # _z log q(G|z) for each sample\n",
    "    \n",
    "    # Pre-compute probabilities to sample from\n",
    "    with torch.no_grad():\n",
    "        edge_probs = torch.sigmoid(scores(z, hparams))\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # 1. Sample a hard graph G\n",
    "        g_hard = torch.bernoulli(edge_probs)\n",
    "\n",
    "        # 2. Calculate the score _z log q(G|z)\n",
    "        score = score_autograd_g_given_z(z, g_hard, hparams)\n",
    "\n",
    "        # 3. Calculate log joint reward: log p(,D|G)\n",
    "        with torch.no_grad():\n",
    "            log_joint_reward = (log_full_likelihood(data, g_hard, theta, hparams) + \n",
    "                               log_theta_prior(theta * g_hard, hparams.get('theta_prior_sigma')))\n",
    "            \n",
    "        log_joint_rewards.append(log_joint_reward)\n",
    "        scores_list.append(score)\n",
    "\n",
    "    # Convert to tensors\n",
    "    log_joint_tensor = torch.stack(log_joint_rewards)  # (n_samples,)\n",
    "    scores_tensor = torch.stack(scores_list)           # (n_samples, d, k, 2)\n",
    "    \n",
    "    # --- STABLE COMPUTATION OF THE RATIO ---\n",
    "    \n",
    "    # Step 1: Compute NUMERATOR = _Z E_p(G|Z) [p(,D|G)]\n",
    "    # This is the score function estimator result\n",
    "    # We need: _i p(G_i|Z) * _z log p(G_i|Z) * p(,D|G_i)\n",
    "    # Since we sampled from p(G|Z), this becomes: (1/M) * _i _z log p(G_i|Z) * p(,D|G_i)\n",
    "    \n",
    "    # Convert log probabilities to actual probabilities using stable method\n",
    "    log_max = torch.max(log_joint_tensor)\n",
    "    log_shifted = log_joint_tensor - log_max\n",
    "    p_shifted = torch.exp(log_shifted)  # These are p(,D|G_i) / exp(log_max)\n",
    "    \n",
    "    # Numerator: weighted sum of scores by probabilities\n",
    "    numerator = torch.zeros_like(scores_tensor[0])\n",
    "    for i in range(n_samples):\n",
    "        numerator += p_shifted[i] * scores_tensor[i]\n",
    "    numerator = numerator / n_samples  # Average over samples\n",
    "    \n",
    "    # Step 2: Compute DENOMINATOR = E_p(G|Z) [p(,D|G)]\n",
    "    # This is: (1/M) * _i p(,D|G_i)\n",
    "    denominator = p_shifted.sum() / n_samples\n",
    "    \n",
    "    # Step 3: Compute the ratio stably\n",
    "    # We have: numerator/denominator, but both are scaled by exp(log_max)\n",
    "    # So the ratio is: (numerator_scaled / exp(log_max)) / (denominator_scaled / exp(log_max))\n",
    "    #                = numerator_scaled / denominator_scaled\n",
    "    grad_likelihood_ratio = numerator / denominator\n",
    "    \n",
    "    # --- Final result ---\n",
    "    total_grad = grad_z_prior + grad_likelihood_ratio\n",
    "    \n",
    "    return total_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable mean \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def stable_mean(fxs: torch.Tensor, dim: int = 0, keepdim: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically stable mean for arrays with mixed positive/negative values.\n",
    "    \n",
    "    Args:\n",
    "        fxs: Input tensor\n",
    "        dim: Dimension along which to compute the mean\n",
    "        keepdim: Whether to keep the reduced dimension\n",
    "    \n",
    "    Returns:\n",
    "        Stable mean of the input tensor\n",
    "    \"\"\"\n",
    "    jitter = 1e-30\n",
    "    \n",
    "    def stable_mean_positive_only(fs: torch.Tensor, n: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Helper function for positive-only values\"\"\"\n",
    "        # Add jitter to avoid log(0)\n",
    "        fs_safe = fs + jitter\n",
    "        log_sum = torch.logsumexp(torch.log(fs_safe), dim=dim, keepdim=True)\n",
    "        log_n = torch.log(n + jitter)\n",
    "        return torch.exp(log_sum - log_n)\n",
    "    \n",
    "    # Separate positive and negative values\n",
    "    positive_mask = fxs > 0.0\n",
    "    negative_mask = fxs < 0.0\n",
    "    \n",
    "    f_xs_positive = fxs * positive_mask.float()\n",
    "    f_xs_negative = -fxs * negative_mask.float()  # Make negative values positive\n",
    "    \n",
    "    # Count positive and negative elements\n",
    "    n_positive = positive_mask.sum(dim=dim, keepdim=True).float()\n",
    "    n_negative = negative_mask.sum(dim=dim, keepdim=True).float()\n",
    "    \n",
    "    # Total number of elements\n",
    "    if dim is None:\n",
    "        n_total = torch.tensor(fxs.numel(), dtype=fxs.dtype, device=fxs.device)\n",
    "    else:\n",
    "        n_total = torch.tensor(fxs.shape[dim], dtype=fxs.dtype, device=fxs.device)\n",
    "    \n",
    "    # Compute stable means for positive and negative parts\n",
    "    avg_positive = stable_mean_positive_only(f_xs_positive, n_positive)\n",
    "    avg_negative = stable_mean_positive_only(f_xs_negative, n_negative)\n",
    "    \n",
    "    # Combine with proper weighting\n",
    "    result = (n_positive / n_total) * avg_positive - (n_negative / n_total) * avg_negative\n",
    "    \n",
    "    return result if keepdim else result.squeeze(dim)\n",
    "\n",
    "\n",
    "def log_stable_mean_from_logs(fsx: torch.Tensor, dim: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log(mean(exp(fsx))) stably when fsx contains log values.\n",
    "    \n",
    "    This is equivalent to computing the log of the mean of probabilities\n",
    "    when fsx contains log probabilities.\n",
    "    \n",
    "    Args:\n",
    "        fsx: Input tensor containing log values\n",
    "        dim: Dimension along which to compute the mean\n",
    "    \n",
    "    Returns:\n",
    "        Log of the mean of exp(fsx)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = fsx.shape[dim]\n",
    "    \n",
    "    # Compute logsumexp and subtract log(n) for the mean\n",
    "    lse = torch.logsumexp(fsx, dim=dim)\n",
    "    log_n = torch.log(torch.tensor(n, dtype=fsx.dtype, device=fsx.device))\n",
    "    \n",
    "    return lse - log_n\n",
    "\n",
    "\n",
    "def stable_mean_simple(x: torch.Tensor, dim: int = 0, keepdim: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Simplified stable mean that works well for most cases.\n",
    "    Similar to the one in your models/utils.py but cleaner.\n",
    "    \"\"\"\n",
    "    jitter = 1e-30\n",
    "    \n",
    "    if not x.is_floating_point():\n",
    "        x = x.float()\n",
    "    \n",
    "    # Separate positive and negative parts\n",
    "    pos = x.clamp(min=0) + jitter\n",
    "    neg = (-x).clamp(min=0) + jitter\n",
    "    \n",
    "    # Compute stable sums using logsumexp\n",
    "    sum_pos = torch.exp(torch.logsumexp(torch.log(pos), dim=dim, keepdim=True))\n",
    "    sum_neg = torch.exp(torch.logsumexp(torch.log(neg), dim=dim, keepdim=True))\n",
    "    \n",
    "    # Get the count\n",
    "    n = torch.tensor(x.shape[dim] if dim < x.dim() else x.numel(), \n",
    "                     dtype=x.dtype, device=x.device)\n",
    "    \n",
    "    # Compute mean\n",
    "    mean = (sum_pos - sum_neg) / (n + jitter)\n",
    "    \n",
    "    return mean if keepdim else mean.squeeze(dim)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_tensor: tensor([-1000, -1000, -1000, -1000, -1000])\n",
      "stable_mean: -999.9996337890625\n",
      "stable_mean exp: 0.0\n"
     ]
    }
   ],
   "source": [
    "## try the stable mean function \n",
    "\n",
    "dummy_tensor = torch.tensor([-1000, -1000, -1000, -1000, -1000])\n",
    "\n",
    "print(f'dummy_tensor: {dummy_tensor}')\n",
    "\n",
    "print(f'stable_mean: {stable_mean(dummy_tensor)}')\n",
    "\n",
    "print(f'stable_mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def logsumexp_with_sign(a, b, dim=-1, keepdim=False, eps=1e-40):\n",
    "    \"\"\"\n",
    "    Stable log| bexp(a)|   and   sign( bexp(a))\n",
    "    ------------------------------------------------\n",
    "    a   : tensor  the log-terms (_j)\n",
    "    b   : tensor  the weights   (b_j)  (can be )\n",
    "          `a` and `b` must be broadcastcompatible.\n",
    "    dim : dimension over which to sum\n",
    "    \"\"\"\n",
    "    a_max = torch.max(a, dim=dim, keepdim=True)[0]           # shift\n",
    "    scaled = b * torch.exp(a - a_max)                        # weights in safe range\n",
    "    s      = scaled.sum(dim=dim, keepdim=keepdim)            # may be \n",
    "    sign   = torch.sign(s).detach()                          # 1 (detach  no grad through sign)\n",
    "    logabs = torch.log(torch.abs(s) + eps) + a_max.squeeze(dim) \\\n",
    "             if not keepdim else torch.log(torch.abs(s) + eps) + a_max\n",
    "    return logabs, sign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_num: tensor([-2.0601, -0.6257, -2.0684, -0.0838,  1.4336, -0.1034,  0.1488,  0.1566,\n",
      "         1.1128, -1.3828,  1.5209,  0.6923])\n",
      "sign_num: tensor([ 1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.])\n",
      "grad_z: tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487],\n",
      "        [ 0.6920, -0.3160, -2.1152,  0.3223, -1.2633],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168],\n",
      "        [-0.2473, -1.3527, -1.6959,  0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502],\n",
      "        [-0.5855, -0.1734,  0.1835,  1.3894,  1.5863],\n",
      "        [ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927],\n",
      "        [ 0.2484,  0.4397,  0.1124,  0.6408,  0.4412],\n",
      "        [-0.1023,  0.7924, -0.2897,  0.0525,  0.5943],\n",
      "        [ 1.5419,  0.5073, -0.5910, -0.5692,  0.9200],\n",
      "        [ 1.1108,  1.2899, -1.4959, -0.1938,  0.4455],\n",
      "        [ 1.3253, -1.6293, -0.5497, -0.4798, -0.4997]])\n",
      "torch.Size([3, 2, 2])\n",
      "tensor([[[ 0.0134, -0.0562],\n",
      "         [-0.0133, -0.0967]],\n",
      "\n",
      "        [[-0.4408, -0.0948],\n",
      "         [-0.1220,  0.1229]],\n",
      "\n",
      "        [[ 0.3199,  0.0264],\n",
      "         [ 0.4811, -0.2101]]])\n"
     ]
    }
   ],
   "source": [
    "# --- dummy data for demonstration ---\n",
    "K          = 5                          # number of graph samples\n",
    "flat_dim   = 12                         # d*k*2  after you reshape\n",
    "\n",
    "torch.manual_seed(0)\n",
    "grad_z     = torch.randn(flat_dim, K)   # [flat_dim, K]  each row = score_row\n",
    "log_lik    = torch.randn(K) * -2.0      # [K]\n",
    "\n",
    "# --- denominator: plain logsumexp because weights are +1 ---\n",
    "log_den    = torch.logsumexp(log_lik, dim=0)          # scalar  log D\n",
    "\n",
    "# --- numerator: row-wise signed logsumexp ---\n",
    "log_num, sign_num = [], []\n",
    "for r in range(flat_dim):\n",
    "    ln, sg = logsumexp_with_sign(grad_z[r], log_lik)\n",
    "    log_num.append(ln)\n",
    "    sign_num.append(sg)\n",
    "\n",
    "log_num = torch.stack(log_num)    # [flat_dim]\n",
    "sign_num= torch.stack(sign_num)   # [flat_dim]  (+1/-1/0)\n",
    "print(f'log_num: {log_num}')\n",
    "print(f'sign_num: {sign_num}')\n",
    "print(f'grad_z: {grad_z}')\n",
    "\n",
    "# --- weighted average  N/D  (still on-graph wrt log_lik and grad_z) ---\n",
    "weighted_score_flat = sign_num * torch.exp(log_num - log_den)   # [flat_dim]\n",
    "\n",
    "# reshape back to [d, k, 2] if you like\n",
    "d, k = 3, 2\n",
    "weighted_score = weighted_score_flat.view(d, k, 2)\n",
    "\n",
    "print(weighted_score.shape)   # torch.Size([3, 2, 2])\n",
    "print(weighted_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stable_mean(fxs):\n",
    "    # assumes fs are only positive\n",
    "    jitter = 1e-30\n",
    "\n",
    "    # Taking n separately we need non-zero\n",
    "    stable_mean_psve_only = lambda fs, n: np.exp(\n",
    "        np.logsumexp(np.log(fs)) - np.log(n + jitter)\n",
    "    )\n",
    "\n",
    "    f_xs_psve = fxs * (fxs > 0.0)\n",
    "    f_xs_ngve = -fxs * (fxs < 0.0)\n",
    "\n",
    "    n_psve = np.sum((fxs > 0.0))\n",
    "    n_ngve = fxs.size - n_psve\n",
    "\n",
    "    avg_psve = stable_mean_psve_only(f_xs_psve, n_psve)\n",
    "    avg_ngve = stable_mean_psve_only(f_xs_ngve, n_ngve)\n",
    "    return (n_psve / fxs.size) * avg_psve - (n_ngve / fxs.size) * avg_ngve\n",
    "\n",
    "\n",
    "def log_stable_mean_from_logs(fsx):\n",
    "    lse = np.logsumexp(fsx)\n",
    "    return lse - fsx.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pos: 37, n_neg: 13\n",
      "log_grads_abs: tensor([ -982.2874, -1000.7169, -1019.6636,  -992.2940, -1008.9379, -1012.2079,\n",
      "         -994.3369,  -990.1645, -1005.9486, -1003.0577, -1005.3584,  -998.1594,\n",
      "        -1010.1560,  -989.4846, -1010.3552,  -997.9644, -1004.5443,  -994.9136,\n",
      "        -1007.3737, -1013.3845,  -994.4514,  -983.4689,  -988.5276,  -990.0796,\n",
      "         -995.8204,  -985.0195, -1010.4863, -1005.4227,  -997.6549, -1004.5693,\n",
      "         -992.1402, -1010.3970,  -995.2999,  -991.7427, -1019.3112,  -994.4885,\n",
      "        -1007.3323, -1002.0017,  -994.2208, -1006.0974,  -997.2839,  -993.9631,\n",
      "         -996.8855, -1005.9220, -1007.6309,  -999.3800,  -994.7897,  -998.8450,\n",
      "         -976.0505,  -996.9531])\n",
      "log_grads_abs_pos: tensor([      -inf,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,  -989.4846, -1010.3552,  -997.9644, -1004.5443,  -994.9136,\n",
      "        -1007.3737, -1013.3845,  -994.4514,  -983.4689,  -988.5276,  -990.0796,\n",
      "         -995.8204,  -985.0195, -1010.4863, -1005.4227,  -997.6549, -1004.5693,\n",
      "         -992.1402, -1010.3970,  -995.2999,  -991.7427, -1019.3112,  -994.4885,\n",
      "        -1007.3323, -1002.0017,  -994.2208, -1006.0974,  -997.2839,  -993.9631,\n",
      "         -996.8855, -1005.9220, -1007.6309,  -999.3800,  -994.7897,  -998.8450,\n",
      "         -976.0505,  -996.9531])\n",
      "log_grads_abs_neg: tensor([ -982.2874, -1000.7169, -1019.6636,  -992.2940, -1008.9379, -1012.2079,\n",
      "         -994.3369,  -990.1645, -1005.9486, -1003.0577, -1005.3584,  -998.1594,\n",
      "        -1010.1560,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,       -inf,       -inf,       -inf,       -inf,       -inf,\n",
      "              -inf,       -inf])\n",
      "log_numerator_positive: -979.66064453125\n",
      "log_numerator_negative: -984.8519287109375\n",
      "log_den: -983.3275756835938\n",
      "log_den_shifted: -983.3275146484375\n",
      "log_den_shifted_exp: 0.0\n",
      "final_grad: 38.91387176513672\n",
      "final_grad_shifted: 38.9114990234375\n",
      "result: None\n",
      "result_signed: 28.956180572509766\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def stable_ratio(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    \n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grads = torch.stack(grad_samples)\n",
    "\n",
    "    log_grads_abs = torch.log(grads.abs() + eps)\n",
    "\n",
    "    log_grads_abs += log_p\n",
    "\n",
    "    pos_mask = grads >= 0\n",
    "    neg_mask = grads < 0\n",
    "\n",
    "    n_pos = pos_mask.sum().clamp(min=1)\n",
    "    n_neg = neg_mask.sum().clamp(min=1)\n",
    "\n",
    "    print(f'n_pos: {n_pos}, n_neg: {n_neg}')\n",
    "\n",
    "    print(f'log_grads_abs: {log_grads_abs}')\n",
    "\n",
    "    log_grads_abs_pos = log_grads_abs.masked_fill(~pos_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_pos: {log_grads_abs_pos}')\n",
    "    log_grads_abs_neg = log_grads_abs.masked_fill(~neg_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_neg: {log_grads_abs_neg}')\n",
    "\n",
    "    log_numerator_positive = torch.logsumexp(log_grads_abs_pos, dim=0) - torch.log(n_pos.float())\n",
    "    print(f'log_numerator_positive: {log_numerator_positive}')\n",
    "    log_numerator_negative = torch.logsumexp(log_grads_abs_neg, dim=0) - torch.log(n_neg.float())\n",
    "    print(f'log_numerator_negative: {log_numerator_negative}')\n",
    "    log_den = torch.logsumexp(log_p, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "    print(f'log_den: {log_den}')\n",
    "\n",
    "\n",
    "    # can we use the log_p - log_p_max for the denominator\n",
    "    log_p_max = log_p.max()\n",
    "    log_p_shifted = log_p - log_p_max\n",
    "    log_den_shifted = torch.logsumexp(log_p_shifted, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "    log_den_shifted = log_den_shifted + log_p_max\n",
    "    print(f'log_den_shifted: {log_den_shifted}')\n",
    "\n",
    "    log_den_shifted_exp = torch.exp(log_den_shifted)\n",
    "    print(f'log_den_shifted_exp: {log_den_shifted_exp}')\n",
    "\n",
    "\n",
    "\n",
    "    final_grad = torch.exp(log_numerator_positive - log_den) - torch.exp(log_numerator_negative - log_den)\n",
    "    print(f'final_grad: {final_grad}')\n",
    "\n",
    "    final_grad_shifted = torch.exp(log_numerator_positive - log_den_shifted) - torch.exp(log_numerator_negative - log_den_shifted)\n",
    "    print(f'final_grad_shifted: {final_grad_shifted}')\n",
    "\n",
    "    #print(f'n_pos: {n_pos}, n_neg: {n_neg}')\n",
    "    return final_grad\n",
    "\n",
    "\n",
    "def stable_ratio_signed(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    S   = len(log_density_samples)                    # == M\n",
    "\n",
    "    log_p   = torch.stack(log_density_samples)        # [S]\n",
    "    grads   = torch.stack(grad_samples)               # [S,*]\n",
    "\n",
    "    while log_p.dim() < grads.dim():\n",
    "        log_p = log_p.unsqueeze(-1)\n",
    "\n",
    "    log_numerator = torch.logsumexp(\n",
    "    log_p + torch.log(torch.abs(grads)) * torch.sign(grads),\n",
    "    dim=0,\n",
    "    )\n",
    "\n",
    "    log_denominator = torch.logsumexp(log_p, dim=0)\n",
    "    result = torch.exp(log_numerator - log_denominator)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    #print(f\"grad_samples: {grad_samples}\")\n",
    "    #print(f'grads: {grads}')\n",
    "    #print(f'pos_mask: {pos_mask}')\n",
    "    #print(f'pos_mask logs: {pos_mask.log()}')\n",
    "\n",
    "\n",
    "    #print(f'neg_mask: {neg_mask}')\n",
    "    #print(f'neg_mask logs: {neg_mask.log()}')\n",
    "\n",
    "\n",
    "\n",
    "    #print(f'log_grads_abs: {log_grads_abs}')\n",
    "\n",
    "\n",
    "    #print(f'end of grads')x\n",
    "    #print(f'--------------------------------')\n",
    "\n",
    "\n",
    "    #print(f'log_p: {log_p}')\n",
    "\n",
    "    #print(f'grads: {grads}')\n",
    "\n",
    "    return final_grad\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_samples = 50\n",
    "\n",
    "# Create grad samples with net positive mean\n",
    "grad_samples = [x.clone().detach() for x in torch.linspace(-10, 30, num_samples)]  # More positive values than negative\n",
    "\n",
    "# Create log density samples from N(-1000, 100)\n",
    "log_density_samples = [torch.normal(mean=-1000.0, std=10.0, size=()) for _ in range(num_samples)]\n",
    "\n",
    "result = stable_ratio(grad_samples, log_density_samples)\n",
    "result_signed = stable_ratio_signed(grad_samples, log_density_samples)\n",
    "print(f'result: {result}')\n",
    "print(f'result_signed: {result_signed}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stable_ratio(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    \n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grads = torch.stack(grad_samples)\n",
    "\n",
    "    log_grads_abs = torch.log(grads.abs() + eps)\n",
    "\n",
    "    pos_mask = grads >= 0\n",
    "    neg_mask = grads < 0\n",
    "\n",
    "    n_pos = pos_mask.sum().clamp(min=1)\n",
    "    n_neg = neg_mask.sum().clamp(min=1)\n",
    "\n",
    "    print(f'n_pos: {n_pos}, n_neg: {n_neg}')\n",
    "\n",
    "    print(f'log_grads_abs: {log_grads_abs}')\n",
    "\n",
    "    log_grads_abs_pos = log_grads_abs.masked_fill(~pos_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_pos: {log_grads_abs_pos}')\n",
    "    log_grads_abs_neg = log_grads_abs.masked_fill(~neg_mask, float('-inf'))\n",
    "    print(f'log_grads_abs_neg: {log_grads_abs_neg}')\n",
    "\n",
    "    log_numerator_positive = torch.logsumexp(log_grads_abs_pos, dim=0) - torch.log(n_pos.float())\n",
    "    print(f'log_numerator_positive: {log_numerator_positive}')\n",
    "    log_numerator_negative = torch.logsumexp(log_grads_abs_neg, dim=0) - torch.log(n_neg.float())\n",
    "    print(f'log_numerator_negative: {log_numerator_negative}')\n",
    "    log_den = torch.logsumexp(log_p, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "    print(f'log_den: {log_den}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    final_grad = torch.exp(log_numerator_positive - log_den) - torch.exp(log_numerator_negative - log_den)\n",
    "    print(f'final_grad: {final_grad}')\n",
    "\n",
    "\n",
    "    return final_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m grad_samples[:\u001b[38;5;241m20\u001b[39m] \u001b[38;5;241m=\u001b[39m grad_samples[:\u001b[38;5;241m20\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     53\u001b[0m log_density_samples \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnormal(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000.0\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m, size\u001b[38;5;241m=\u001b[39m()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[0;32m---> 55\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mstable_ratio_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_density_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrected Final Gradient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[101], line 7\u001b[0m, in \u001b[0;36mstable_ratio_fixed\u001b[0;34m(grad_samples, log_density_samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-30\u001b[39m\n\u001b[1;32m      6\u001b[0m log_p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(log_density_samples)\n\u001b[0;32m----> 7\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m log_grads_abs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(grads\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m     11\u001b[0m pos_mask \u001b[38;5;241m=\u001b[39m grads \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def stable_ratio_fixed(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    \n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grads = torch.stack(grad_samples)\n",
    "\n",
    "    log_grads_abs = torch.log(grads.abs() + eps)\n",
    "\n",
    "    pos_mask = grads >= 0\n",
    "    neg_mask = grads < 0\n",
    "\n",
    "    n_pos = pos_mask.sum().clamp(min=1)\n",
    "    n_neg = neg_mask.sum().clamp(min=1)\n",
    "\n",
    "    log_grads_abs_pos = log_grads_abs.masked_fill(~pos_mask, float('-inf'))\n",
    "    log_grads_abs_neg = log_grads_abs.masked_fill(~neg_mask, float('-inf'))\n",
    "\n",
    "    # Numerator terms in log space\n",
    "    log_num_pos = torch.logsumexp(log_grads_abs_pos, dim=0) - torch.log(n_pos.float())\n",
    "    log_num_neg = torch.logsumexp(log_grads_abs_neg, dim=0) - torch.log(n_neg.float())\n",
    "    \n",
    "    # Denominator in log space\n",
    "    log_den = torch.logsumexp(log_p, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "\n",
    "    # Perform subtraction in log-space to avoid overflow/underflow\n",
    "    # This computes log(num_pos - num_neg)\n",
    "    # We must handle the case where the result is negative or zero\n",
    "    if log_num_pos > log_num_neg:\n",
    "        # Using log(exp(a) - exp(b)) = a + log(1 - exp(b-a))\n",
    "        log_numerator = log_num_pos + torch.log1p(-torch.exp(log_num_neg - log_num_pos))\n",
    "        sign = 1.0\n",
    "    else:\n",
    "        # To handle num_neg > num_pos\n",
    "        log_numerator = log_num_neg + torch.log1p(-torch.exp(log_num_pos - log_num_neg))\n",
    "        sign = -1.0\n",
    "\n",
    "    # Final calculation: log(numerator) - log(denominator)\n",
    "    final_log_grad = log_numerator - log_den\n",
    "    \n",
    "    return sign * torch.exp(final_log_grad)\n",
    "\n",
    "# Using your test case\n",
    "torch.manual_seed(0)\n",
    "num_samples = 50\n",
    "grad_samples = [torch.tensor(x) for x in torch.linspace(-4, 10, num_samples)]\n",
    "## change first 20 to more *2\n",
    "\n",
    "\n",
    "log_density_samples = [torch.normal(mean=-1000.0, std=100.0, size=()) for _ in range(num_samples)]\n",
    "\n",
    "result = stable_ratio_fixed(grad_samples, log_density_samples)\n",
    "print(f'Corrected Final Gradient: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_density_samples: tensor([ -923.8889,  -938.1699, -1029.9385, -1018.7778,  -808.4102]) ...\n",
      "grad_samples: tensor([-4.0000, -3.7143, -3.4286, -3.1429, -2.8571]) ...\n",
      "num: 3.000000238418579, den: 0.019999999552965164\n",
      "den: 0.019999999552965164 max p exp: 0.0\n",
      "\n",
      "Final Result: 150.00001525878906\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def stable_mean(x: torch.Tensor, eps: float = 1e-30) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically-stable mean of `x` (any shape, any sign).\n",
    "    Works by treating positive and negative parts separately and\n",
    "    using log-sum-exp for each part.\n",
    "    Returns a scalar.\n",
    "    \"\"\"\n",
    "    x = x.flatten()\n",
    "\n",
    "    pos_mask = x >= 0\n",
    "    neg_mask = ~pos_mask                # same as x < 0\n",
    "\n",
    "    pos_vals = x[pos_mask]              #  0\n",
    "    neg_vals = -x[neg_mask]             # magnitudes of the negatives\n",
    "\n",
    "    n_pos = pos_vals.numel()\n",
    "    n_neg = neg_vals.numel()\n",
    "    n_tot = n_pos + n_neg\n",
    "    if n_tot == 0:\n",
    "        raise ValueError(\"`x` is empty!\")\n",
    "\n",
    "    # ----  log-mean of the positive part  ---------------------------------\n",
    "    if n_pos:\n",
    "        log_pos_mean = (\n",
    "            torch.logsumexp(torch.log(pos_vals + eps), dim=0) -\n",
    "            math.log(n_pos)\n",
    "        )\n",
    "        pos_mean = log_pos_mean.exp()\n",
    "    else:\n",
    "        pos_mean = torch.tensor(0., dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # ----  log-mean of the |negative| part  -------------------------------\n",
    "    if n_neg:\n",
    "        log_neg_mean = (\n",
    "            torch.logsumexp(torch.log(neg_vals + eps), dim=0) -\n",
    "            math.log(n_neg)\n",
    "        )\n",
    "        neg_mean = log_neg_mean.exp()\n",
    "    else:\n",
    "        neg_mean = torch.tensor(0., dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # unconditional mean =  P(+)E[+ ]    P()E[||]\n",
    "    return (n_pos / n_tot) * pos_mean - (n_neg / n_tot) * neg_mean\n",
    "\n",
    "\n",
    "def log_mean_from_logs(log_x, eps: float = 1e-30):\n",
    "    \"\"\"log E[x] where `log_x = log(x)` and x0.\"\"\"\n",
    "    return torch.logsumexp(log_x, dim=0) - math.log(log_x.shape[0])\n",
    "\n",
    "def stable_ratio_with_stable_mean(grad_samples, log_density_samples):\n",
    "    grads = torch.stack(grad_samples).flatten()\n",
    "\n",
    "    # numerator   ordinary stable mean (takes  values)\n",
    "    num = stable_mean(grads)\n",
    "\n",
    "    # denominator  stay in log-space until the very end\n",
    "    log_p = torch.stack(log_density_samples).flatten()\n",
    "    log_p_max = log_p.max()\n",
    "    log_p = log_p - log_p_max\n",
    "    log_den = log_mean_from_logs(log_p)     # log E[p]\n",
    "    den = torch.exp(log_den)               # E[p] in \n",
    "\n",
    "    print(f'num: {num}, den: {den}')\n",
    "    print(f'den: {den} max p exp: {torch.exp(log_p_max)}')\n",
    "\n",
    "    return num / den\n",
    "\n",
    "# 50 log-density samples ~ N(-1000, 100)\n",
    "log_density_samples = [torch.normal(mean=-1000.0, std=100.0, size=(50,))]\n",
    "\n",
    "# 50 gradient samples (some positive, some negative)\n",
    "grad_samples = [torch.linspace(-4, 10, steps=50)]  # evenly spaced, centered at 0\n",
    "\n",
    "# Print and test\n",
    "\n",
    "# Print and test\n",
    "print(f'log_density_samples: {log_density_samples[0][:5]} ...')\n",
    "print(f'grad_samples: {grad_samples[0][:5]} ...')\n",
    "\n",
    "result = stable_ratio_with_stable_mean(grad_samples, log_density_samples)\n",
    "print(f'\\nFinal Result: {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING STABLE RATIO FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "Test Case 1: Mixed gradients with small log densities\n",
      "--------------------------------------------------\n",
      "Gradient shapes: [torch.Size([5]), torch.Size([5]), torch.Size([5])]\n",
      "Log density ranges: ['[-1002.1, -999.8]', '[-1001.8, -998.7]', '[-1001.2, -998.3]']\n",
      "stable_ratio result: tensor([nan, nan, nan, nan, nan])\n",
      "stable_ratio result shape: torch.Size([5])\n",
      "stable_ratio_with_stable_mean result: inf\n",
      "stable_ratio_with_stable_mean result shape: torch.Size([])\n",
      "Difference between methods: nan\n",
      "Results are close: False\n",
      "\n",
      "Test Case 2: Comparison with naive method\n",
      "--------------------------------------------------\n",
      "Naive numerator: 0.7733333706855774\n",
      "Naive denominator: 0.0\n",
      "Naive result: inf\n",
      "Stable vs Naive difference: nan\n",
      "\n",
      "Test Case 3: Simple validation case\n",
      "--------------------------------------------------\n",
      "Simple case - stable_ratio: tensor([ 1.3591, -7.3891,  4.4817])\n",
      "Simple case - stable_mean version: 2.7535126209259033\n",
      "Simple case difference: 10.382801055908203\n",
      "Manual calculation: 2.7535128593444824\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def test_stable_ratio_functions():\n",
    "    \"\"\"\n",
    "    Quick test to verify both stable ratio functions work correctly\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING STABLE RATIO FUNCTIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test case 1: Mixed positive/negative gradients with very small log densities\n",
    "    print(\"\\nTest Case 1: Mixed gradients with small log densities\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    grad_samples = [\n",
    "        torch.tensor([15.5, -8.2, 3.1, -12.7, 0.5]),\n",
    "        torch.tensor([-2.3, 9.8, -15.1, 4.6, -0.8]),\n",
    "        torch.tensor([7.2, -5.4, 11.3, -1.9, 6.0])\n",
    "    ]\n",
    "    \n",
    "    log_density_samples = [\n",
    "        torch.tensor([-1001.5, -999.8, -1000.2, -1002.1, -1000.9]),\n",
    "        torch.tensor([-1000.3, -998.7, -1001.8, -999.5, -1000.6]),\n",
    "        torch.tensor([-999.9, -1001.2, -998.3, -1000.7, -999.4])\n",
    "    ]\n",
    "    \n",
    "    print(f\"Gradient shapes: {[g.shape for g in grad_samples]}\")\n",
    "    print(f\"Log density ranges: {[f'[{lg.min():.1f}, {lg.max():.1f}]' for lg in log_density_samples]}\")\n",
    "    \n",
    "    # Test both functions\n",
    "    try:\n",
    "        result1 = stable_ratio(grad_samples, log_density_samples)\n",
    "        print(f\"stable_ratio result: {result1}\")\n",
    "        print(f\"stable_ratio result shape: {result1.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"stable_ratio failed: {e}\")\n",
    "        result1 = None\n",
    "    \n",
    "    try:\n",
    "        result2 = stable_ratio_with_stable_mean(grad_samples, log_density_samples)\n",
    "        print(f\"stable_ratio_with_stable_mean result: {result2}\")\n",
    "        print(f\"stable_ratio_with_stable_mean result shape: {result2.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"stable_ratio_with_stable_mean failed: {e}\")\n",
    "        result2 = None\n",
    "    \n",
    "    if result1 is not None and result2 is not None:\n",
    "        diff = torch.norm(result1 - result2)\n",
    "        print(f\"Difference between methods: {diff}\")\n",
    "        print(f\"Results are close: {diff < 1e-5}\")\n",
    "    \n",
    "    # Test case 2: Naive comparison (will likely fail due to numerical issues)\n",
    "    print(\"\\nTest Case 2: Comparison with naive method\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Naive unstable computation\n",
    "        grads_flat = torch.cat([g.flatten() for g in grad_samples])\n",
    "        log_densities_flat = torch.cat([ld.flatten() for ld in log_density_samples])\n",
    "        densities_flat = torch.exp(log_densities_flat)\n",
    "        \n",
    "        naive_num = grads_flat.mean()\n",
    "        naive_den = densities_flat.mean()\n",
    "        naive_result = naive_num / naive_den if naive_den != 0 else torch.tensor(float('inf'))\n",
    "        \n",
    "        print(f\"Naive numerator: {naive_num}\")\n",
    "        print(f\"Naive denominator: {naive_den}\")\n",
    "        print(f\"Naive result: {naive_result}\")\n",
    "        \n",
    "        if result1 is not None:\n",
    "            print(f\"Stable vs Naive difference: {torch.norm(result1.flatten().mean() - naive_result)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Naive method failed (expected): {e}\")\n",
    "    \n",
    "    # Test case 3: Simple case for validation\n",
    "    print(\"\\nTest Case 3: Simple validation case\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    simple_grads = [torch.tensor([1.0, -1.0, 2.0])]\n",
    "    simple_log_densities = [torch.tensor([-1.0, -2.0, -1.5])]  # Less extreme values\n",
    "    \n",
    "    try:\n",
    "        simple_result1 = stable_ratio(simple_grads, simple_log_densities)\n",
    "        simple_result2 = stable_ratio_with_stable_mean(simple_grads, simple_log_densities)\n",
    "        \n",
    "        print(f\"Simple case - stable_ratio: {simple_result1}\")\n",
    "        print(f\"Simple case - stable_mean version: {simple_result2}\")\n",
    "        print(f\"Simple case difference: {torch.norm(simple_result1 - simple_result2)}\")\n",
    "        \n",
    "        # Manual calculation for verification\n",
    "        grads = torch.tensor([1.0, -1.0, 2.0])\n",
    "        log_p = torch.tensor([-1.0, -2.0, -1.5])\n",
    "        p = torch.exp(log_p)\n",
    "        \n",
    "        manual_num = grads.mean()\n",
    "        manual_den = p.mean()\n",
    "        manual_result = manual_num / manual_den\n",
    "        \n",
    "        print(f\"Manual calculation: {manual_result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Simple case failed: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_stable_ratio_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: tensor([[[ True,  True,  True],\n",
      "         [ True, False, False],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [ True, False,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [ True,  True,  True],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [ True, False,  True],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [False,  True, False],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True, False],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False, False,  True],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [ True, False,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False, False,  True],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [ True, False,  True],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [False,  True,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [ True, False, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [ True, False,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [ True, False,  True],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [ True, False, False],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [ True, False,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [ True,  True,  True],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [ True, False, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [ True,  True, False],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [False,  True,  True],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [False, False, False],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [False,  True,  True],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False,  True,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [ True,  True, False],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [False, False,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True, False, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [False,  True,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [ True, False,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False, False,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True, False, False],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True, False, False],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [ True, False, False],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [ True, False,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [False,  True, False],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [ True,  True,  True],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [False, False,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [False,  True,  True],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True,  True],\n",
      "         [False, False, False]]])\n",
      "neg: tensor([[[False, False, False],\n",
      "         [False,  True,  True],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [False,  True, False],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [False, False, False],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False, False,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False,  True],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [False,  True, False],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [ True, False,  True],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False, False,  True],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True, False],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [False,  True, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [ True,  True, False],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [False,  True, False],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [ True, False, False],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [False,  True,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [False,  True, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False, False],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False,  True, False],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [False,  True,  True],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False,  True, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [False, False, False],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False, False, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [False,  True,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [False, False,  True],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [ True, False, False],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [False, False,  True],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [ True,  True,  True],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False, False,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [ True, False, False],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False,  True],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [ True, False, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [False, False,  True],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [ True,  True, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False,  True,  True],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [ True, False, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [False,  True, False],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[ True,  True, False],\n",
      "         [ True,  True, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False,  True],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False,  True,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False,  True],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False,  True,  True],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [False,  True,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False,  True, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [ True, False,  True],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [False, False,  True],\n",
      "         [False, False,  True]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False, False, False],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [ True,  True, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [ True, False, False],\n",
      "         [ True,  True, False]],\n",
      "\n",
      "        [[ True, False, False],\n",
      "         [False, False, False],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False, False, False],\n",
      "         [ True,  True,  True]]])\n",
      "pos where torch.where is true: tensor([[[-1185.2966, -1185.2023, -1184.9166],\n",
      "         [-1185.8141,       -inf,       -inf],\n",
      "         [-1186.4197,       -inf, -1186.8418]],\n",
      "\n",
      "        [[-1076.8070,       -inf, -1076.3488],\n",
      "         [-1076.4441,       -inf, -1077.0382],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf, -1009.9510],\n",
      "         [-1008.7932, -1009.3708, -1008.1461],\n",
      "         [-1009.4481,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf, -1105.2571,       -inf],\n",
      "         [-1103.3619, -1103.0426,       -inf],\n",
      "         [-1103.1282, -1104.2280, -1103.4020]],\n",
      "\n",
      "        [[ -833.5591,  -837.6057,  -833.0685],\n",
      "         [ -833.7407,  -833.6544,       -inf],\n",
      "         [      -inf,  -832.6059,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [-1079.5480,       -inf, -1079.1251],\n",
      "         [      -inf, -1076.7810, -1076.6278]],\n",
      "\n",
      "        [[ -979.7043,  -978.0209,  -977.8492],\n",
      "         [      -inf,  -979.5771,       -inf],\n",
      "         [ -979.4586,       -inf,  -978.7786]],\n",
      "\n",
      "        [[      -inf, -1062.4066,       -inf],\n",
      "         [-1061.4407, -1068.2955,       -inf],\n",
      "         [-1062.5090,       -inf, -1062.3374]],\n",
      "\n",
      "        [[-1022.6011,       -inf, -1022.5480],\n",
      "         [      -inf,       -inf, -1022.3432],\n",
      "         [-1023.1597,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf, -1023.9624, -1024.8669],\n",
      "         [-1025.6921,       -inf, -1024.5244],\n",
      "         [-1024.1598, -1026.0038, -1026.0050]],\n",
      "\n",
      "        [[      -inf,       -inf, -1074.8949],\n",
      "         [      -inf,       -inf, -1075.7065],\n",
      "         [-1074.5382,       -inf, -1076.3273]],\n",
      "\n",
      "        [[ -871.2745,       -inf,       -inf],\n",
      "         [ -871.7554,       -inf,  -870.0838],\n",
      "         [      -inf,  -874.2623,       -inf]],\n",
      "\n",
      "        [[      -inf, -1030.8391,       -inf],\n",
      "         [      -inf, -1030.8553, -1030.2083],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[ -915.2108,       -inf,       -inf],\n",
      "         [ -912.6299,       -inf,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[-1025.2351,       -inf, -1026.6398],\n",
      "         [-1025.7823,       -inf, -1025.5648],\n",
      "         [-1024.8906, -1026.7279,       -inf]],\n",
      "\n",
      "        [[ -952.1086,  -951.5513,  -952.1379],\n",
      "         [ -950.7278,  -954.2086,  -952.5394],\n",
      "         [ -952.2241,       -inf,  -952.5523]],\n",
      "\n",
      "        [[-1076.7390, -1079.1880,       -inf],\n",
      "         [-1076.4469,       -inf, -1077.4941],\n",
      "         [      -inf, -1075.7881, -1075.3628]],\n",
      "\n",
      "        [[ -952.3716,       -inf,  -949.9901],\n",
      "         [ -950.4211,       -inf,       -inf],\n",
      "         [      -inf,  -949.6826,  -949.6121]],\n",
      "\n",
      "        [[ -981.8235,  -980.3879,       -inf],\n",
      "         [ -981.3410,       -inf,  -980.8885],\n",
      "         [ -982.4068,  -981.9356,       -inf]],\n",
      "\n",
      "        [[-1203.1581,       -inf,       -inf],\n",
      "         [-1202.3038, -1201.2097, -1202.1915],\n",
      "         [-1200.4091,       -inf,       -inf]],\n",
      "\n",
      "        [[ -891.4872,  -891.2365,  -894.2554],\n",
      "         [ -889.9924,  -891.2390,  -891.1830],\n",
      "         [ -893.0568,  -890.7609,       -inf]],\n",
      "\n",
      "        [[      -inf, -1165.0577,       -inf],\n",
      "         [-1163.6249, -1161.9684, -1164.2943],\n",
      "         [-1162.6284, -1163.9216, -1163.0094]],\n",
      "\n",
      "        [[      -inf,       -inf, -1070.4012],\n",
      "         [-1068.5178,       -inf,       -inf],\n",
      "         [      -inf,       -inf, -1068.6473]],\n",
      "\n",
      "        [[ -919.3325,       -inf,  -919.9323],\n",
      "         [ -919.9119,  -920.1136,       -inf],\n",
      "         [ -919.4334,       -inf,  -920.0613]],\n",
      "\n",
      "        [[-1091.8485, -1091.1708, -1088.9929],\n",
      "         [      -inf, -1090.9487, -1089.8433],\n",
      "         [      -inf,       -inf, -1089.5808]],\n",
      "\n",
      "        [[      -inf, -1069.9033, -1067.2838],\n",
      "         [-1071.0033, -1067.5824,       -inf],\n",
      "         [-1067.8066, -1067.7681,       -inf]],\n",
      "\n",
      "        [[ -977.9810,  -978.1334,  -977.4982],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [      -inf,  -977.9579,       -inf]],\n",
      "\n",
      "        [[      -inf, -1058.9353,       -inf],\n",
      "         [-1059.1647, -1058.1326,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,  -996.8534,       -inf],\n",
      "         [      -inf,  -994.8472,  -995.3114],\n",
      "         [ -996.5530,       -inf,  -996.3126]],\n",
      "\n",
      "        [[-1036.1650, -1036.8875, -1036.2196],\n",
      "         [-1036.2405, -1037.3705,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,  -922.5667],\n",
      "         [      -inf,  -923.5620,  -921.7454],\n",
      "         [ -925.0477,  -922.0359,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [ -974.6238,  -975.8039,       -inf],\n",
      "         [      -inf,  -975.0972,       -inf]],\n",
      "\n",
      "        [[      -inf,  -967.4141,  -964.5445],\n",
      "         [      -inf,       -inf,  -965.5841],\n",
      "         [ -964.9600,  -964.8157,       -inf]],\n",
      "\n",
      "        [[-1111.1334, -1109.4900, -1110.3745],\n",
      "         [-1109.9019,       -inf,       -inf],\n",
      "         [      -inf,       -inf, -1109.0514]],\n",
      "\n",
      "        [[ -853.0355,       -inf,       -inf],\n",
      "         [      -inf,  -852.9586,  -853.3596],\n",
      "         [ -851.8923,  -855.0718,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,  -991.0553],\n",
      "         [ -991.3083,       -inf,  -990.0431],\n",
      "         [ -992.5963,  -990.3784,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf, -1101.0138],\n",
      "         [      -inf,       -inf, -1100.8813],\n",
      "         [-1102.5032, -1102.9480, -1103.1793]],\n",
      "\n",
      "        [[-1091.9851, -1094.7574, -1092.5948],\n",
      "         [-1091.7511, -1093.1315,       -inf],\n",
      "         [-1092.5756, -1095.2163,       -inf]],\n",
      "\n",
      "        [[ -973.7086,  -973.2552,  -974.3315],\n",
      "         [ -972.8974,       -inf,       -inf],\n",
      "         [ -975.0477,  -973.1042,  -972.7050]],\n",
      "\n",
      "        [[ -983.4761,  -985.2519,  -982.8525],\n",
      "         [ -983.0397,  -983.1498,       -inf],\n",
      "         [      -inf,  -982.6909,       -inf]],\n",
      "\n",
      "        [[      -inf,  -940.8572,       -inf],\n",
      "         [ -939.4710,       -inf,       -inf],\n",
      "         [      -inf,  -941.5880,  -939.9291]],\n",
      "\n",
      "        [[      -inf, -1188.4136, -1187.9894],\n",
      "         [-1188.5098,       -inf,       -inf],\n",
      "         [-1189.3899, -1190.0604, -1188.1992]],\n",
      "\n",
      "        [[ -844.5200,  -846.3889,       -inf],\n",
      "         [ -847.1312,       -inf,  -845.0254],\n",
      "         [ -847.3785,  -844.7391,  -847.5383]],\n",
      "\n",
      "        [[-1115.9683, -1116.9680, -1115.9299],\n",
      "         [      -inf, -1116.2150,       -inf],\n",
      "         [-1116.9161, -1116.3533,       -inf]],\n",
      "\n",
      "        [[      -inf, -1076.6282, -1076.4468],\n",
      "         [-1074.6862, -1074.2378,       -inf],\n",
      "         [-1074.7025, -1074.4207,       -inf]],\n",
      "\n",
      "        [[-1118.6083, -1117.7147,       -inf],\n",
      "         [-1117.2133, -1117.6514, -1116.7714],\n",
      "         [      -inf, -1119.5070, -1117.6357]],\n",
      "\n",
      "        [[ -988.2258,  -990.4866,  -987.0149],\n",
      "         [      -inf,       -inf,  -988.1788],\n",
      "         [ -987.4718,  -988.2361,  -989.4716]],\n",
      "\n",
      "        [[ -999.6053, -1001.4562,       -inf],\n",
      "         [      -inf,  -999.6024,  -999.3212],\n",
      "         [      -inf,       -inf,  -999.9244]],\n",
      "\n",
      "        [[      -inf,  -997.7538,  -997.5903],\n",
      "         [ -999.1405,  -997.8577,  -998.3008],\n",
      "         [      -inf,  -998.5714,  -997.0159]],\n",
      "\n",
      "        [[      -inf,  -870.2573,       -inf],\n",
      "         [ -870.9208,  -871.6550,  -871.9267],\n",
      "         [      -inf,       -inf,       -inf]]])\n",
      "neg where torch.where is true: tensor([[[      -inf,       -inf,       -inf],\n",
      "         [      -inf, -1187.6550, -1185.6967],\n",
      "         [      -inf, -1186.1239,       -inf]],\n",
      "\n",
      "        [[      -inf, -1077.8839,       -inf],\n",
      "         [      -inf, -1076.8690,       -inf],\n",
      "         [-1079.0126, -1076.4305, -1077.2505]],\n",
      "\n",
      "        [[-1008.1539, -1009.0909,       -inf],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [      -inf, -1009.2092, -1009.6202]],\n",
      "\n",
      "        [[-1103.2815,       -inf, -1103.8506],\n",
      "         [      -inf,       -inf, -1103.2777],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf,  -833.9918],\n",
      "         [ -835.4002,       -inf,  -836.5829]],\n",
      "\n",
      "        [[-1077.7074, -1078.2903, -1075.9108],\n",
      "         [      -inf, -1077.0341,       -inf],\n",
      "         [-1076.6730,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [ -977.3358,       -inf,  -977.9876],\n",
      "         [      -inf,  -978.6064,       -inf]],\n",
      "\n",
      "        [[-1061.1133,       -inf, -1061.4037],\n",
      "         [      -inf,       -inf, -1062.0353],\n",
      "         [      -inf, -1062.7979,       -inf]],\n",
      "\n",
      "        [[      -inf, -1023.4084,       -inf],\n",
      "         [-1023.1273, -1023.9607,       -inf],\n",
      "         [      -inf, -1024.4590, -1023.0230]],\n",
      "\n",
      "        [[-1027.3158,       -inf,       -inf],\n",
      "         [      -inf, -1024.2191,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[-1075.9153, -1075.9683,       -inf],\n",
      "         [-1075.2939, -1075.1566,       -inf],\n",
      "         [      -inf, -1074.7352,       -inf]],\n",
      "\n",
      "        [[      -inf,  -870.8940,  -870.6914],\n",
      "         [      -inf,  -872.0572,       -inf],\n",
      "         [ -871.2362,       -inf,  -871.8147]],\n",
      "\n",
      "        [[-1032.8575,       -inf, -1031.9349],\n",
      "         [-1030.1490,       -inf,       -inf],\n",
      "         [-1030.7450, -1030.3988, -1030.2892]],\n",
      "\n",
      "        [[      -inf,  -913.0952,  -911.9169],\n",
      "         [      -inf,  -914.7156,  -912.0034],\n",
      "         [ -913.1281,  -911.1197,  -912.2891]],\n",
      "\n",
      "        [[      -inf, -1024.9891,       -inf],\n",
      "         [      -inf, -1025.4744,       -inf],\n",
      "         [      -inf,       -inf, -1025.2931]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [      -inf,  -950.7859,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf, -1077.1835],\n",
      "         [      -inf, -1075.9622,       -inf],\n",
      "         [-1076.3302,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,  -949.8883,       -inf],\n",
      "         [      -inf,  -950.0169,  -950.0488],\n",
      "         [ -949.9575,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,  -983.1345],\n",
      "         [      -inf,  -981.4429,       -inf],\n",
      "         [      -inf,       -inf,  -982.1272]],\n",
      "\n",
      "        [[      -inf, -1203.5737, -1204.0619],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [      -inf, -1201.0199, -1203.0807]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf,  -894.1114]],\n",
      "\n",
      "        [[-1164.1033,       -inf, -1162.8580],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[-1071.9709, -1071.8143,       -inf],\n",
      "         [      -inf, -1070.0503, -1068.2678],\n",
      "         [-1069.0072, -1070.0073,       -inf]],\n",
      "\n",
      "        [[      -inf,  -920.8197,       -inf],\n",
      "         [      -inf,       -inf,  -919.7804],\n",
      "         [      -inf,  -920.2974,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [-1094.0420,       -inf,       -inf],\n",
      "         [-1090.5757, -1090.9611,       -inf]],\n",
      "\n",
      "        [[-1071.6990,       -inf,       -inf],\n",
      "         [      -inf,       -inf, -1068.5610],\n",
      "         [      -inf,       -inf, -1066.8654]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [ -977.9084,  -977.3152,  -978.6304],\n",
      "         [ -978.9765,       -inf,  -981.2565]],\n",
      "\n",
      "        [[-1059.3652,       -inf, -1058.2477],\n",
      "         [      -inf,       -inf, -1059.1042],\n",
      "         [-1058.4542, -1059.7528, -1058.7119]],\n",
      "\n",
      "        [[ -996.3906,       -inf,  -995.9312],\n",
      "         [ -998.3952,       -inf,       -inf],\n",
      "         [      -inf,  -995.8625,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf, -1038.0769],\n",
      "         [-1035.9432, -1036.0232, -1036.3900]],\n",
      "\n",
      "        [[ -921.5551,  -922.2358,       -inf],\n",
      "         [ -921.8985,       -inf,       -inf],\n",
      "         [      -inf,       -inf,  -921.9391]],\n",
      "\n",
      "        [[ -975.0187,  -976.1328,  -976.9433],\n",
      "         [      -inf,       -inf,  -975.7316],\n",
      "         [ -976.8204,       -inf,  -976.3857]],\n",
      "\n",
      "        [[ -965.5578,       -inf,       -inf],\n",
      "         [ -965.5666,  -964.6193,       -inf],\n",
      "         [      -inf,       -inf,  -965.5864]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf, -1109.3236, -1113.5720],\n",
      "         [-1109.4635, -1110.9049,       -inf]],\n",
      "\n",
      "        [[      -inf,  -852.9967,  -855.0248],\n",
      "         [ -855.0231,       -inf,       -inf],\n",
      "         [      -inf,       -inf,  -854.4350]],\n",
      "\n",
      "        [[ -992.3719,  -991.2707,       -inf],\n",
      "         [      -inf,  -990.4764,       -inf],\n",
      "         [      -inf,       -inf,  -993.8340]],\n",
      "\n",
      "        [[-1102.0319, -1101.7164,       -inf],\n",
      "         [-1102.0640, -1102.6404,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf, -1091.6536],\n",
      "         [      -inf,       -inf, -1092.7029]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf,  -973.7211,  -972.8221],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf,  -983.4016],\n",
      "         [ -983.7305,       -inf,  -983.5964]],\n",
      "\n",
      "        [[ -941.2912,       -inf,  -940.9715],\n",
      "         [      -inf,  -940.6540,  -940.0483],\n",
      "         [ -942.0333,       -inf,       -inf]],\n",
      "\n",
      "        [[-1189.6118,       -inf,       -inf],\n",
      "         [      -inf, -1189.5321, -1189.5543],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,  -845.3155],\n",
      "         [      -inf,  -845.9826,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [-1117.0156,       -inf, -1116.9899],\n",
      "         [      -inf,       -inf, -1117.0233]],\n",
      "\n",
      "        [[-1076.0891,       -inf,       -inf],\n",
      "         [      -inf,       -inf, -1076.1168],\n",
      "         [      -inf,       -inf, -1076.2395]],\n",
      "\n",
      "        [[      -inf,       -inf, -1116.8638],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [-1119.2660,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf,       -inf],\n",
      "         [ -989.1249,  -990.9024,       -inf],\n",
      "         [      -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[      -inf,       -inf, -1007.6200],\n",
      "         [-1001.5282,       -inf,       -inf],\n",
      "         [-1002.6412, -1000.3369,       -inf]],\n",
      "\n",
      "        [[ -999.0016,       -inf,       -inf],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [ -999.0663,       -inf,       -inf]],\n",
      "\n",
      "        [[ -875.3571,       -inf,  -870.2122],\n",
      "         [      -inf,       -inf,       -inf],\n",
      "         [ -872.7277,  -869.6631,  -870.8057]]])\n",
      "result shape: torch.Size([3, 3])\n",
      "result: tensor([[ 0.8606,  0.0150,  1.4057],\n",
      "        [ 0.7178,  0.7824, -0.5584],\n",
      "        [-0.1365,  2.2326, -0.0418]])\n",
      "pos: tensor([[[False, False,  True],\n",
      "         [ True, False,  True],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[False,  True, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True, False],\n",
      "         [ True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [False,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [False, False,  True]]])\n",
      "neg: tensor([[[ True,  True, False],\n",
      "         [False,  True, False],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[ True, False,  True],\n",
      "         [False, False,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False,  True],\n",
      "         [False,  True, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False, False],\n",
      "         [ True, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False, False],\n",
      "         [ True,  True, False]]])\n",
      "pos where torch.where is true: tensor([[[    -inf,     -inf,  -6.6648],\n",
      "         [ -8.3414,     -inf,  -9.5030],\n",
      "         [    -inf,  -6.5935,  -7.5150]],\n",
      "\n",
      "        [[    -inf,  -8.2883,     -inf],\n",
      "         [ -8.2753,  -8.4081,     -inf],\n",
      "         [ -8.0160,  -9.4518,  -8.7307]],\n",
      "\n",
      "        [[ -9.4372,  -9.6448,  -9.2933],\n",
      "         [ -8.8572,  -8.9616,     -inf],\n",
      "         [ -8.7879,     -inf,  -9.4348]],\n",
      "\n",
      "        [[-12.2956, -12.0703, -11.3293],\n",
      "         [-11.0667, -11.8172, -11.8106],\n",
      "         [    -inf, -11.6804, -11.6900]],\n",
      "\n",
      "        [[-13.3427, -13.9821, -12.5561],\n",
      "         [-12.5960, -11.9046, -12.9193],\n",
      "         [    -inf,     -inf, -13.2018]]])\n",
      "neg where torch.where is true: tensor([[[ -8.9933,  -7.3604,     -inf],\n",
      "         [    -inf,  -7.0454,     -inf],\n",
      "         [ -8.0044,     -inf,     -inf]],\n",
      "\n",
      "        [[ -9.6878,     -inf,  -7.8863],\n",
      "         [    -inf,     -inf,  -8.6250],\n",
      "         [    -inf,     -inf,     -inf]],\n",
      "\n",
      "        [[    -inf,     -inf,     -inf],\n",
      "         [    -inf,     -inf, -10.0956],\n",
      "         [    -inf,  -8.8178,     -inf]],\n",
      "\n",
      "        [[    -inf,     -inf,     -inf],\n",
      "         [    -inf,     -inf,     -inf],\n",
      "         [-11.1170,     -inf,     -inf]],\n",
      "\n",
      "        [[    -inf,     -inf,     -inf],\n",
      "         [    -inf,     -inf,     -inf],\n",
      "         [-16.4085, -12.2041,     -inf]]])\n",
      "Stable result:\n",
      "tensor([[-0.0871, -0.2717,  0.8732],\n",
      "        [ 0.5677, -0.4389, -0.1182],\n",
      "        [ 0.1160,  1.1304,  0.6909]])\n",
      "Naive result:\n",
      "tensor([[-0.0871, -0.2717,  0.8732],\n",
      "        [ 0.5677, -0.4389, -0.1182],\n",
      "        [ 0.1160,  1.1304,  0.6909]])\n",
      "Difference:\n",
      "tensor([[-4.4703e-08,  0.0000e+00,  1.2517e-06],\n",
      "        [ 1.1921e-07, -4.4703e-07,  0.0000e+00],\n",
      "        [-3.1292e-07,  5.9605e-07,  1.7881e-07]])\n",
      "Max absolute difference: 1.2516975402832031e-06\n"
     ]
    }
   ],
   "source": [
    "def stable_ratio(grad_samples, log_density_samples):\n",
    "    eps = 1e-30\n",
    "    S   = len(log_density_samples)                    # == M\n",
    "\n",
    "    log_p   = torch.stack(log_density_samples)        # [S]\n",
    "    grads   = torch.stack(grad_samples)               # [S,*]\n",
    "\n",
    "    while log_p.dim() < grads.dim():\n",
    "        log_p = log_p.unsqueeze(-1)\n",
    "\n",
    "    log_den = torch.logsumexp(log_p, dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "\n",
    "\n",
    "    pos = grads >= 0\n",
    "    print(f'pos: {pos}')\n",
    "    neg = ~pos\n",
    "    print(f'neg: {neg}')\n",
    "\n",
    "    # print the pos where torch.where is true\n",
    "    print(f'pos where torch.where is true: {torch.where(pos, torch.log(grads.abs() + eps) + log_p, torch.full_like(log_p, -float(\"inf\")))}')\n",
    "    print(f'neg where torch.where is true: {torch.where(neg, torch.log(grads.abs() + eps) + log_p, torch.full_like(log_p, -float(\"inf\")))}')\n",
    "\n",
    "    log_num_pos = torch.logsumexp(\n",
    "        torch.where(pos,\n",
    "                    torch.log(grads + eps) + log_p,\n",
    "                    torch.full_like(log_p, -float('inf'))),\n",
    "        dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "\n",
    "    log_num_neg = torch.logsumexp(\n",
    "        torch.where(neg,\n",
    "                    torch.log(grads.abs() + eps) + log_p,\n",
    "                    torch.full_like(log_p, -float('inf'))),\n",
    "        dim=0) - torch.log(torch.tensor(len(log_p), dtype=log_p.dtype, device=log_p.device))\n",
    "\n",
    "    return torch.exp(log_num_pos - log_den) - torch.exp(log_num_neg - log_den)\n",
    "\n",
    "\n",
    "# mock data with log dnesity shape (50,)\n",
    "# grad of theta is shape (50, 3, 3)\n",
    "\n",
    "n_samples = 50\n",
    "log_density_samples = [torch.normal(mean=-1000.0, std=100.0, size=()) for _ in range(n_samples)]  # 50 scalars\n",
    "grad_samples = [torch.normal(mean=0.2, std=1, size=(3, 3)) for _ in range(n_samples)]  # 50 (3,3) tensors\n",
    "\n",
    "result = stable_ratio(grad_samples, log_density_samples)\n",
    "print(f'result shape: {result.shape}')  # Should be (3, 3)\n",
    "print(f'result: {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def naive_ratio(grad_samples, log_density_samples):\n",
    "    \"\"\"Naive implementation for comparison (may overflow/underflow)\"\"\"\n",
    "    # Convert to tensors\n",
    "    log_p = torch.stack(log_density_samples)  # [S]\n",
    "    grads = torch.stack(grad_samples)         # [S, *]\n",
    "    \n",
    "    # Convert log densities to probabilities\n",
    "    probs = torch.exp(log_p)  # This may underflow to 0\n",
    "    \n",
    "    # Compute weighted average: sum(p_i * grad_i) / sum(p_i)\n",
    "    while probs.dim() < grads.dim():\n",
    "        probs = probs.unsqueeze(-1)\n",
    "    \n",
    "    numerator = torch.sum(probs * grads, dim=0)\n",
    "    denominator = torch.sum(probs)\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "# Test with smaller log values to avoid underflow   \n",
    "n_samples = 5\n",
    "log_density_samples_small = [torch.normal(mean=-10.0, std=2.0, size=()) for _ in range(n_samples)]\n",
    "grad_samples_small = [torch.normal(mean=0.2, std=1, size=(3, 3)) for _ in range(n_samples)]\n",
    "\n",
    "stable_result = stable_ratio(grad_samples_small, log_density_samples_small)\n",
    "stable_ratio_old_result = naive_ratio(grad_samples_small, log_density_samples_small)\n",
    "\n",
    "print(f\"Stable result:\\n{stable_result}\")\n",
    "print(f\"Naive result:\\n{stable_ratio_old_result}\")\n",
    "print(f\"Difference:\\n{stable_result - stable_ratio_old_result}\")\n",
    "print(f\"Max absolute difference: {torch.max(torch.abs(stable_result - stable_ratio_old_result))}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing stable_ratio ===\n",
      "grad_samples: tensor([ 1., -1.,  2.])\n",
      "log_density_samples: tensor([-1.0000, -2.0000, -1.5000])\n",
      "pos_mask: tensor([ True, False,  True])\n",
      "neg_mask: tensor([False,  True, False])\n",
      "pos_cnt: 2\n",
      "pos_grads: tensor([1., 2.])\n",
      "log_density_samples_pos: tensor([-1.0000, -1.5000])\n",
      "log_numerator_pos: -0.8987704515457153\n",
      "log_denominator_pos: -1.624535322189331\n",
      "expected_pos: 2.0663108825683594\n",
      "final_positive: 1.3775405883789062\n",
      "neg_cnt: 1\n",
      "neg_grads (abs): tensor([1.])\n",
      "log_density_samples_neg: tensor([-2.])\n",
      "log_numerator_neg: -2.0\n",
      "log_denominator_neg: -3.0986123085021973\n",
      "expected_neg: 3.0\n",
      "final_negative: 1.0\n",
      "final result: 0.37754058837890625\n",
      "Final result: 0.37754058837890625\n"
     ]
    }
   ],
   "source": [
    "def stable_ratio(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    eps = 1e-30\n",
    "    print(f\"grad_samples: {grad_samples}\")\n",
    "    print(f\"log_density_samples: {log_density_samples}\")\n",
    "    \n",
    "    total_cnt = torch.tensor(len(grad_samples), dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    pos_mask = grad_samples >= 0\n",
    "    print(f\"pos_mask: {pos_mask}\")\n",
    "    neg_mask = grad_samples < 0\n",
    "    print(f\"neg_mask: {neg_mask}\")\n",
    "    \n",
    "    # Handle positive gradients\n",
    "    if pos_mask.any():\n",
    "        pos_cnt = pos_mask.sum()\n",
    "        print(f\"pos_cnt: {pos_cnt}\")\n",
    "        \n",
    "        pos_grads = grad_samples[pos_mask]\n",
    "        print(f\"pos_grads: {pos_grads}\")\n",
    "        log_density_samples_pos = log_density_samples[pos_mask]\n",
    "        print(f\"log_density_samples_pos: {log_density_samples_pos}\")\n",
    "        \n",
    "        log_numerator_pos = torch.logsumexp(log_density_samples_pos + torch.log(pos_grads + eps), dim=0) - torch.log(pos_cnt.float())\n",
    "        log_denominator_pos = torch.logsumexp(log_density_samples_pos, dim=0) - torch.log(total_cnt)\n",
    "        print(f\"log_numerator_pos: {log_numerator_pos}\")\n",
    "        print(f\"log_denominator_pos: {log_denominator_pos}\")\n",
    "        \n",
    "        expected_pos = torch.exp(log_numerator_pos - log_denominator_pos)\n",
    "        final_positive = expected_pos * pos_cnt.float() / total_cnt\n",
    "        print(f\"expected_pos: {expected_pos}\")\n",
    "        print(f\"final_positive: {final_positive}\")\n",
    "    else:\n",
    "        final_positive = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    # Handle negative gradients\n",
    "    if neg_mask.any():\n",
    "        neg_cnt = neg_mask.sum()\n",
    "        print(f\"neg_cnt: {neg_cnt}\")\n",
    "        \n",
    "        neg_grads = -grad_samples[neg_mask]  # Take absolute value\n",
    "        print(f\"neg_grads (abs): {neg_grads}\")\n",
    "        log_density_samples_neg = log_density_samples[neg_mask]\n",
    "        print(f\"log_density_samples_neg: {log_density_samples_neg}\")\n",
    "        \n",
    "        log_numerator_neg = torch.logsumexp(log_density_samples_neg + torch.log(neg_grads + eps), dim=0) - torch.log(neg_cnt.float())\n",
    "        log_denominator_neg = torch.logsumexp(log_density_samples_neg, dim=0) - torch.log(total_cnt)\n",
    "        print(f\"log_numerator_neg: {log_numerator_neg}\")\n",
    "        print(f\"log_denominator_neg: {log_denominator_neg}\")\n",
    "        \n",
    "        expected_neg = torch.exp(log_numerator_neg - log_denominator_neg)\n",
    "        final_negative = expected_neg * neg_cnt.float() / total_cnt\n",
    "        print(f\"expected_neg: {expected_neg}\")\n",
    "        print(f\"final_negative: {final_negative}\")\n",
    "    else:\n",
    "        final_negative = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    # Use the scaled values in the final result\n",
    "    result = final_positive - final_negative\n",
    "    print(f\"final result: {result}\")\n",
    "    return result\n",
    "# Test the function\n",
    "grad_samples = torch.tensor([1.0, -1.0, 2.0])\n",
    "log_density_samples = torch.tensor([-1.0, -2.0, -1.5])\n",
    "\n",
    "print(\"=== Testing stable_ratio ===\")\n",
    "result = stable_ratio(grad_samples, log_density_samples)\n",
    "print(f\"Final result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING: VERY BASIC vs NAIVE vs STABLE IMPLEMENTATIONS\n",
      "================================================================================\n",
      "\n",
      "--- Test 1: Mixed gradients ---\n",
      "Very Basic result:   0.668602\n",
      "Stable old result:   0.668602\n",
      "Stable result:       0.668602\n",
      "Basic vs Stable old diff: 0.00000006\n",
      "Stable old vs Stable diff:0.00000000\n",
      "\n",
      "--- Test 2: Only positive gradients ---\n",
      "Very Basic result:   1.032726\n",
      "Stable old result:   1.032726\n",
      "Stable result:       1.032726\n",
      "Basic vs Stable old diff: 0.00000012\n",
      "Stable old vs Stable diff:0.00000000\n",
      "\n",
      "--- Test 3: Only negative gradients ---\n",
      "Very Basic result:   -1.032726\n",
      "Stable old result:   -1.032726\n",
      "Stable result:       -1.032726\n",
      "Basic vs Stable old diff: 0.00000012\n",
      "Stable old vs Stable diff:0.00000000\n",
      "\n",
      "--- Test 4: Uniform log densities ---\n",
      "Very Basic result:   0.375000\n",
      "Stable old result:   0.375000\n",
      "Stable result:       0.375000\n",
      "Expected basic:      0.375000\n",
      "Expected stable old:      0.750000\n",
      "Basic vs Expected:   0.00000003\n",
      "Stable old vs Expected:   0.37500000\n",
      "\n",
      "--- Test 5: Very small log densities (stability test) ---\n",
      "Very Basic result:   1.000000\n",
      "Stable old result:        1.000000\n",
      "Stable result:       0.999999\n",
      "Basic vs Stable old diff: 0.00000000\n",
      "Stable old vs Stable diff:0.00000095\n",
      "\n",
      "--- Test 6: Detailed breakdown of approaches ---\n",
      "Input grads: tensor([ 3., -1.,  1.])\n",
      "Input log densities: tensor([-0.5000, -1.0000, -0.3000])\n",
      "Input probabilities: tensor([0.6065, 0.3679, 0.7408])\n",
      "\n",
      "Very Basic (simple weighted avg): 1.278273\n",
      "Naive (pos/neg separation):       1.278273\n",
      "Stable (numerically stable):      1.278274\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def very_basic_baseline(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Very basic baseline - just exp(log_density) and simple arithmetic\n",
    "    No separation of pos/neg, no numerical stability considerations\n",
    "    \"\"\"\n",
    "    # Convert log densities to actual probabilities\n",
    "    probabilities = torch.exp(log_density_samples)\n",
    "    \n",
    "    # Simple weighted average: sum(p_i * grad_i) / sum(p_i)\n",
    "    numerator = torch.sum(probabilities * grad_samples)\n",
    "    denominator = torch.sum(probabilities)\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def softmax_baseline(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Softmax baseline - just exp(log_density) and simple arithmetic\n",
    "    No separation of pos/neg, no numerical stability considerations\n",
    "    \"\"\"\n",
    "    log_density_samples_max = torch.max(log_density_samples)\n",
    "    log_density_samples_shifted = log_density_samples - log_density_samples_max\n",
    "    weights = torch.softmax(log_density_samples_shifted, dim=0)\n",
    "    return torch.sum(weights * grad_samples)\n",
    "\n",
    "def stable_ratio_old(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically stable computation of an expectation of gradients weighted by normalized probabilities.\n",
    "    Computes: E_{p(x)}[grad(x)] = sum(p_i * grad_i) / sum(p_i)\n",
    "    \"\"\"\n",
    "    eps = 1e-30\n",
    "\n",
    "    while log_density_samples.dim() < grad_samples.dim():\n",
    "        log_density_samples = log_density_samples.unsqueeze(-1)\n",
    "\n",
    "    log_den = torch.logsumexp(log_density_samples, dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "\n",
    "    pos_grads = torch.where(grad_samples >= 0, grad_samples, 0.)\n",
    "    neg_grads = torch.where(grad_samples < 0, -grad_samples, 0.)\n",
    "\n",
    "    num = log_density_samples + torch.log(pos_grads + eps)\n",
    "\n",
    "    log_num_pos = torch.logsumexp(log_density_samples + torch.log(pos_grads + eps), dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "    log_num_neg = torch.logsumexp(log_density_samples + torch.log(neg_grads + eps), dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "\n",
    "    return torch.exp(log_num_pos - log_den) - torch.exp(log_num_neg - log_den)\n",
    "\n",
    "def stable_ratio_final(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    eps = 1e-30\n",
    "    \n",
    "    total_cnt = torch.tensor(len(grad_samples), dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    pos_mask = grad_samples >= 0\n",
    "    neg_mask = grad_samples < 0\n",
    "    \n",
    "    # Handle positive gradients\n",
    "    if pos_mask.any():\n",
    "        pos_cnt = pos_mask.sum()\n",
    "        pos_grads = grad_samples[pos_mask]\n",
    "        log_density_samples_pos = log_density_samples[pos_mask]\n",
    "        \n",
    "        log_numerator_pos = torch.logsumexp(log_density_samples_pos + torch.log(pos_grads + eps), dim=0) - torch.log(pos_cnt.float())\n",
    "        log_denominator_pos = torch.logsumexp(log_density_samples, dim=0) - torch.log(total_cnt)\n",
    "        \n",
    "        expected_pos = torch.exp(log_numerator_pos - log_denominator_pos)\n",
    "        final_positive = expected_pos * pos_cnt.float() / total_cnt\n",
    "    else:\n",
    "        final_positive = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    # Handle negative gradients\n",
    "    if neg_mask.any():\n",
    "        neg_cnt = neg_mask.sum()\n",
    "        neg_grads = -grad_samples[neg_mask]  # Take absolute value\n",
    "        log_density_samples_neg = log_density_samples[neg_mask]\n",
    "        \n",
    "        log_numerator_neg = torch.logsumexp(log_density_samples_neg + torch.log(neg_grads + eps), dim=0) - torch.log(neg_cnt.float())\n",
    "        log_denominator_neg = torch.logsumexp(log_density_samples, dim=0) - torch.log(total_cnt)\n",
    "        \n",
    "        expected_neg = torch.exp(log_numerator_neg - log_denominator_neg)\n",
    "        final_negative = expected_neg * neg_cnt.float() / total_cnt\n",
    "    else:\n",
    "        final_negative = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    return final_positive - final_negative\n",
    "\n",
    "def run_tests():\n",
    "    print(\"=\"*80)\n",
    "    print(\"TESTING: VERY BASIC vs NAIVE vs STABLE IMPLEMENTATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test 1: Mixed positive and negative gradients\n",
    "    print(\"\\n--- Test 1: Mixed gradients ---\")\n",
    "    grad_samples = torch.tensor([1.0, -1.0, 2.0, -0.5])\n",
    "    log_density_samples = torch.tensor([-1.0, -2.0, -1.5, -1.8])\n",
    "    \n",
    "    basic_result = very_basic_baseline(grad_samples, log_density_samples)\n",
    "    stable_ratio_old_result = stable_ratio_old(grad_samples, log_density_samples)\n",
    "    stable_result = stable_ratio_final(grad_samples, log_density_samples)\n",
    "    \n",
    "    print(f\"Very Basic result:   {basic_result:.6f}\")\n",
    "    print(f\"Stable old result:   {stable_ratio_old_result:.6f}\")\n",
    "    print(f\"Stable result:       {stable_result:.6f}\")\n",
    "    print(f\"Basic vs Stable old diff: {abs(basic_result - stable_ratio_old_result):.8f}\")\n",
    "    print(f\"Stable old vs Stable diff:{abs(stable_ratio_old_result - stable_result):.8f}\")\n",
    "    \n",
    "    # Test 2: Only positive gradients\n",
    "    print(\"\\n--- Test 2: Only positive gradients ---\")\n",
    "    grad_samples = torch.tensor([1.0, 2.0, 0.5])\n",
    "    log_density_samples = torch.tensor([-1.0, -2.0, -1.5])\n",
    "    \n",
    "    basic_result = very_basic_baseline(grad_samples, log_density_samples)\n",
    "    stable_ratio_old_result = stable_ratio_old(grad_samples, log_density_samples)\n",
    "    stable_result = stable_ratio_final(grad_samples, log_density_samples)\n",
    "\n",
    "    print(f\"Very Basic result:   {basic_result:.6f}\")\n",
    "    print(f\"Stable old result:   {stable_ratio_old_result:.6f}\")\n",
    "    print(f\"Stable result:       {stable_result:.6f}\")\n",
    "    print(f\"Basic vs Stable old diff: {abs(basic_result - stable_ratio_old_result):.8f}\")\n",
    "    print(f\"Stable old vs Stable diff:{abs(stable_ratio_old_result - stable_result):.8f}\")\n",
    "    \n",
    "    # Test 3: Only negative gradients\n",
    "    print(\"\\n--- Test 3: Only negative gradients ---\")\n",
    "    grad_samples = torch.tensor([-1.0, -2.0, -0.5])\n",
    "    log_density_samples = torch.tensor([-1.0, -2.0, -1.5])\n",
    "    \n",
    "    basic_result = very_basic_baseline(grad_samples, log_density_samples)\n",
    "    stable_ratio_old_result = stable_ratio_old(grad_samples, log_density_samples)\n",
    "    stable_result = stable_ratio_final(grad_samples, log_density_samples)\n",
    "    \n",
    "    print(f\"Very Basic result:   {basic_result:.6f}\")\n",
    "    print(f\"Stable old result:   {stable_ratio_old_result:.6f}\")\n",
    "    print(f\"Stable result:       {stable_result:.6f}\")\n",
    "    print(f\"Basic vs Stable old diff: {abs(basic_result - stable_ratio_old_result):.8f}\")\n",
    "    print(f\"Stable old vs Stable diff:{abs(stable_ratio_old_result - stable_result):.8f}\")\n",
    "    \n",
    "    # Test 4: Uniform log densities (easy to verify)\n",
    "    print(\"\\n--- Test 4: Uniform log densities ---\")\n",
    "    grad_samples = torch.tensor([1.0, -1.0, 2.0, -0.5])\n",
    "    log_density_samples = torch.tensor([-1.0, -1.0, -1.0, -1.0])  # Uniform\n",
    "    \n",
    "    basic_result = very_basic_baseline(grad_samples, log_density_samples)\n",
    "    stable_ratio_old_result = stable_ratio_old(grad_samples, log_density_samples)\n",
    "    stable_result = stable_ratio_final(grad_samples, log_density_samples)\n",
    "    \n",
    "    print(f\"Very Basic result:   {basic_result:.6f}\")\n",
    "    print(f\"Stable old result:   {stable_ratio_old_result:.6f}\")\n",
    "    print(f\"Stable result:       {stable_result:.6f}\")\n",
    "    \n",
    "    # Hand calculation for uniform case\n",
    "    # Very basic should be: (1 - 1 + 2 - 0.5) / 4 = 1.5 / 4 = 0.375\n",
    "    expected_basic = (1.0 - 1.0 + 2.0 - 0.5) / 4\n",
    "    print(f\"Expected basic:      {expected_basic:.6f}\")\n",
    "    \n",
    "    # Naive should be: (1 + 2) * 2/4 - (1 + 0.5) * 2/4 = 1.5 - 0.75 = 0.75\n",
    "    expected_stable_old = (1.0 + 2.0) * 2/4 - (1.0 + 0.5) * 2/4\n",
    "    print(f\"Expected stable old:      {expected_stable_old:.6f}\")\n",
    "    \n",
    "    print(f\"Basic vs Expected:   {abs(basic_result - expected_basic):.8f}\")\n",
    "    print(f\"Stable old vs Expected:   {abs(stable_ratio_old_result - expected_stable_old):.8f}\")\n",
    "    \n",
    "    # Test 5: Very small log densities (numerical stability test)\n",
    "    print(\"\\n--- Test 5: Very small log densities (stability test) ---\")\n",
    "    grad_samples = torch.tensor([1.0, -1.0, 2.0])\n",
    "    log_density_samples = torch.tensor([-100.0, -200.0, -150.0])  # Very small probabilities\n",
    "    \n",
    "    try:\n",
    "        basic_result = very_basic_baseline(grad_samples, log_density_samples)\n",
    "        print(f\"Very Basic result:   {basic_result:.6f}\")\n",
    "        basic_failed = False\n",
    "    except Exception as e:\n",
    "        print(f\"Very Basic result:   FAILED ({e})\")\n",
    "        basic_result = torch.tensor(float('nan'))\n",
    "        basic_failed = True\n",
    "    \n",
    "    try:\n",
    "        stable_ratio_old_result = stable_ratio_old(grad_samples, log_density_samples)\n",
    "        print(f\"Stable old result:        {stable_ratio_old_result:.6f}\")\n",
    "        stable_old_failed = False\n",
    "    except Exception as e:\n",
    "        print(f\"Stable old result:        FAILED ({e})\")\n",
    "        stable_ratio_old_result = torch.tensor(float('nan'))\n",
    "        stable_old_failed = True\n",
    "    \n",
    "    try:\n",
    "        stable_result = stable_ratio_final(grad_samples, log_density_samples)\n",
    "        print(f\"Stable result:       {stable_result:.6f}\")\n",
    "        stable_failed = False\n",
    "    except Exception as e:\n",
    "        print(f\"Stable result:       FAILED ({e})\")\n",
    "        stable_result = torch.tensor(float('nan'))\n",
    "        stable_failed = True\n",
    "    \n",
    "    if not basic_failed and not stable_old_failed:\n",
    "        print(f\"Basic vs Stable old diff: {abs(basic_result - stable_ratio_old_result):.8f}\")\n",
    "    if not stable_old_failed and not stable_failed:\n",
    "        print(f\"Stable old vs Stable diff:{abs(stable_ratio_old_result - stable_result):.8f}\")\n",
    "    \n",
    "    # Test 6: Show difference in approaches with detailed breakdown\n",
    "    print(\"\\n--- Test 6: Detailed breakdown of approaches ---\")\n",
    "    grad_samples = torch.tensor([3.0, -1.0, 1.0])\n",
    "    log_density_samples = torch.tensor([-0.5, -1.0, -0.3])\n",
    "    \n",
    "    print(f\"Input grads: {grad_samples}\")\n",
    "    print(f\"Input log densities: {log_density_samples}\")\n",
    "    print(f\"Input probabilities: {torch.exp(log_density_samples)}\")\n",
    "    \n",
    "    basic_result = very_basic_baseline(grad_samples, log_density_samples)\n",
    "    stable_ratio_old_result = stable_ratio_old(grad_samples, log_density_samples)\n",
    "    stable_result = stable_ratio_final(grad_samples, log_density_samples)\n",
    "    \n",
    "    print(f\"\\nVery Basic (simple weighted avg): {basic_result:.6f}\")\n",
    "    print(f\"Naive (pos/neg separation):       {stable_ratio_old_result:.6f}\")\n",
    "    print(f\"Stable (numerically stable):      {stable_result:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_weighted_score(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes: E_{p(x)}[grad(x)] = su    Numerically stable computation of an expectation of gradients weighted by normalized probabilities.\n",
    "m(p_i * grad_i) / sum(p_i)\n",
    "    \"\"\"\n",
    "    eps = 1e-30\n",
    "\n",
    "    while log_density_samples.dim() < grad_samples.dim():\n",
    "        log_density_samples = log_density_samples.unsqueeze(-1)\n",
    "\n",
    "    log_den = torch.logsumexp(log_density_samples, dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "\n",
    "    pos_grads = torch.where(grad_samples >= 0, grad_samples, 0.)\n",
    "    neg_grads = torch.where(grad_samples < 0, -grad_samples, 0.)\n",
    "\n",
    "    num = log_density_samples + torch.log(pos_grads + eps)\n",
    "    print(f\"num: {num}\")\n",
    "\n",
    "    log_num_pos = torch.logsumexp(log_density_samples + torch.log(pos_grads + eps), dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "    log_num_neg = torch.logsumexp(log_density_samples + torch.log(neg_grads + eps), dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "    log.info(f\"log_num_pos: {log_num_pos}\")\n",
    "    log.info(f\"log_num_neg: {log_num_neg}\")\n",
    "\n",
    "    return torch.exp(log_num_pos - log_den) - torch.exp(log_num_neg - log_den)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_density_samples: tensor([ 1.,  2., -3.,  4.,  5.])\n",
      "grad_samples: tensor([ 1., -2.,  3., -4.,  5.])\n",
      "total_cnt: 5.0\n",
      "pos_indicator: tensor([1., 0., 1., 0., 1.])\n",
      "neg_indicator: tensor([0., 1., 0., 1., 0.])\n",
      "pos_grads: tensor([1., 0., 3., 0., 5.])\n",
      "neg_grads: tensor([0., 2., 0., 4., 0.])\n",
      "pos_cnt: 3.0\n",
      "neg_cnt: 2.0\n",
      "log_density_samples: tensor([ 1.,  2., -3.,  4.,  5.])\n",
      "pos_log_terms: tensor([ 1.0000,    -inf, -1.9014,    -inf,  6.6094])\n",
      "log_numerator_pos: 5.514682769775391\n",
      "log_denominator: 3.7526445388793945\n",
      "expected_pos: 5.824296474456787\n",
      "final_positive: 3.4945778846740723\n",
      "neg_log_terms: tensor([  -inf, 2.6931,   -inf, 5.3863,   -inf])\n",
      "log_numerator_neg: 4.7586236000061035\n",
      "log_denominator: 3.7526445388793945\n",
      "expected_neg: 2.7345833778381348\n",
      "final_negative: 1.093833327293396\n",
      "result: 2.4007444381713867\n",
      "res_naive: 2.4007434844970703\n",
      "res: 2.4007444381713867\n"
     ]
    }
   ],
   "source": [
    "def _calculate_weighted_score(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    eps = 1e-30\n",
    "    \n",
    "    while log_density_samples.dim() < grad_samples.dim():\n",
    "        log_density_samples = log_density_samples.unsqueeze(-1)\n",
    "    print(f\"log_density_samples: {log_density_samples}\")\n",
    "    print(f\"grad_samples: {grad_samples}\")\n",
    "\n",
    "    total_cnt = torch.tensor(len(grad_samples), dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    print(f\"total_cnt: {total_cnt}\")\n",
    "    pos_indicator = (grad_samples >= 0).float()\n",
    "    neg_indicator = (grad_samples < 0).float()\n",
    "    print(f\"pos_indicator: {pos_indicator}\")\n",
    "    print(f\"neg_indicator: {neg_indicator}\")\n",
    "    # Use torch.where to create separated gradients (preserves shape)\n",
    "    pos_grads = torch.where(grad_samples >= 0, grad_samples, torch.tensor(0.0))\n",
    "    neg_grads = torch.where(grad_samples < 0, -grad_samples, torch.tensor(0.0))  # Take absolute value\n",
    "    print(f\"pos_grads: {pos_grads}\")\n",
    "    print(f\"neg_grads: {neg_grads}\")\n",
    "    # Count positive and negative elements\n",
    "    pos_cnt = pos_indicator.sum()\n",
    "    neg_cnt = neg_indicator.sum()\n",
    "    print(f\"pos_cnt: {pos_cnt}\")\n",
    "    print(f\"neg_cnt: {neg_cnt}\")\n",
    "    # Handle positive gradients\n",
    "    if pos_cnt > 0:\n",
    "        # Only include non-zero positive gradients in logsumexp\n",
    "        print(f\"log_density_samples: {log_density_samples}\")\n",
    "        pos_log_terms = torch.where(\n",
    "            grad_samples >= 0,\n",
    "            log_density_samples + torch.log(pos_grads + eps),\n",
    "            torch.tensor(-float('inf'), device=grad_samples.device)  # Exclude from logsumexp\n",
    "        )\n",
    "        print(f\"pos_log_terms: {pos_log_terms}\")\n",
    "        log_numerator_pos = torch.logsumexp(pos_log_terms, dim=0) - torch.log(pos_cnt.float())\n",
    "        print(f\"log_numerator_pos: {log_numerator_pos}\")\n",
    "        log_denominator = torch.logsumexp(log_density_samples, dim=0) - torch.log(total_cnt)\n",
    "        print(f\"log_denominator: {log_denominator}\")\n",
    "        expected_pos = torch.exp(log_numerator_pos - log_denominator)\n",
    "        print(f\"expected_pos: {expected_pos}\")\n",
    "        final_positive = expected_pos * pos_cnt.float() / total_cnt\n",
    "        print(f\"final_positive: {final_positive}\")\n",
    "    else:\n",
    "        final_positive = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    # Handle negative gradients\n",
    "    if neg_cnt > 0:\n",
    "        # Only include non-zero negative gradients in logsumexp\n",
    "        neg_log_terms = torch.where(\n",
    "            grad_samples < 0,\n",
    "            log_density_samples + torch.log(neg_grads + eps),\n",
    "            torch.tensor(-float('inf'), device=grad_samples.device)  # Exclude from logsumexp\n",
    "        )\n",
    "        print(f\"neg_log_terms: {neg_log_terms}\")\n",
    "        log_numerator_neg = torch.logsumexp(neg_log_terms, dim=0) - torch.log(neg_cnt.float())\n",
    "        print(f\"log_numerator_neg: {log_numerator_neg}\")\n",
    "        log_denominator = torch.logsumexp(log_density_samples, dim=0) - torch.log(total_cnt)\n",
    "        print(f\"log_denominator: {log_denominator}\")\n",
    "        expected_neg = torch.exp(log_numerator_neg - log_denominator)\n",
    "        print(f\"expected_neg: {expected_neg}\")\n",
    "        final_negative = expected_neg * neg_cnt.float() / total_cnt\n",
    "        print(f\"final_negative: {final_negative}\")\n",
    "    else:\n",
    "        final_negative = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    print(f\"result: {final_positive - final_negative}\")\n",
    "    return final_positive - final_negative\n",
    "\n",
    "grads = torch.tensor([1.0, -2.0, 3.0, -4.0, 5.0])\n",
    "log_density_samples = torch.tensor([1.0, 2.0, -3.0, 4.0, 5.0])\n",
    "density_samples = torch.exp(log_density_samples)\n",
    "sum_density_samples = torch.sum(density_samples)\n",
    "sum_grads = 0\n",
    "for i in range(len(grads)):\n",
    "    sum_grads += grads[i] * density_samples[i]\n",
    "res_naive = sum_grads / sum_density_samples\n",
    "\n",
    "res = _calculate_weighted_score(grads, log_density_samples)\n",
    "print(f\"res_naive: {res_naive}\")\n",
    "print(f\"res: {res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads: tensor([ 1., -2.,  3., -4.,  5.])\n",
      "log_density_samples: tensor([ 1.,  2., -3.,  4.,  5.])\n",
      "pos_indices: (tensor([0, 2, 4]),)\n",
      "neg_indices: (tensor([1, 3]),)\n",
      "log_denominator: 3.7526445388793945\n",
      "pos_cnt: 3.0\n",
      "pos_grads: tensor([1., 3., 5.])\n",
      "log_density_pos: tensor([ 1., -3.,  5.])\n",
      "log_numerator_pos: 5.514682769775391\n",
      "expected_pos: 3.4945781230926514\n",
      "neg_cnt: 2.0\n",
      "neg_grads: tensor([-2., -4.])\n",
      "log_density_neg: tensor([2., 4.])\n",
      "log_numerator_neg: 4.7586236000061035\n",
      "expected_neg: 1.093833327293396\n",
      "OLD VERSION -----------------------------\n",
      "log_den old: 3.7526445388793945\n",
      "pos_grads: tensor([1., 0., 3., 0., 5.])\n",
      "neg_grads: tensor([0., 2., 0., 4., 0.])\n",
      "log_num_pos: 5.003857135772705\n",
      "log_num_neg: 3.8423328399658203\n",
      "total_pos: 3.4945778846740723\n",
      "total_neg: 1.093833327293396\n",
      "res_new: 2.400744915008545\n",
      "res_old: 2.4007444381713867\n"
     ]
    }
   ],
   "source": [
    "def _calculate_weighted_score_v2(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    eps = 1e-30\n",
    "\n",
    "    while log_density_samples.dim() < grad_samples.dim():\n",
    "        log_density_samples = log_density_samples.unsqueeze(-1)\n",
    "\n",
    "    pos_indices = torch.nonzero(grad_samples >= 0, as_tuple=True)\n",
    "    neg_indices = torch.nonzero(grad_samples < 0, as_tuple=True)\n",
    "    print(f\"pos_indices: {pos_indices}\")\n",
    "    print(f\"neg_indices: {neg_indices}\")\n",
    "    \n",
    "    total_cnt = torch.tensor(grad_samples.numel(), dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    # Calculate common denominator: LSE(all densities) - log(total_count)\n",
    "    log_denominator = torch.logsumexp(log_density_samples, dim=0) - torch.log(total_cnt)\n",
    "    print(f\"log_denominator: {log_denominator}\")\n",
    "    # Handle positive gradients\n",
    "    if len(pos_indices[0]) > 0:\n",
    "        pos_cnt = torch.tensor(len(pos_indices[0]), dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "        print(f\"pos_cnt: {pos_cnt}\")\n",
    "\n",
    "        # Extract positive gradients and corresponding log densities\n",
    "        pos_grads = grad_samples[pos_indices]\n",
    "        print(f\"pos_grads: {pos_grads}\")\n",
    "        log_density_pos = log_density_samples[pos_indices]\n",
    "        print(f\"log_density_pos: {log_density_pos}\")\n",
    "\n",
    "\n",
    "        log_numerator_pos = torch.logsumexp(log_density_pos + torch.log(torch.abs(pos_grads) + eps), dim=0) - torch.log(pos_cnt)\n",
    "        print(f\"log_numerator_pos: {log_numerator_pos}\")\n",
    "        # Expected positive: (pos_cnt/total) * exp(numerator - denominator)\n",
    "        expected_pos = (pos_cnt / total_cnt) * torch.exp(log_numerator_pos - log_denominator)\n",
    "        print(f'expected_pos: {expected_pos}')\n",
    "    else:\n",
    "        expected_pos = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    # Handle negative gradients\n",
    "    if len(neg_indices[0]) > 0:\n",
    "        neg_cnt = torch.tensor(len(neg_indices[0]), dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "        print(f\"neg_cnt: {neg_cnt}\")\n",
    "        # Extract negative gradients and corresponding log densities\n",
    "        neg_grads = grad_samples[neg_indices]\n",
    "        print(f\"neg_grads: {neg_grads}\")\n",
    "        log_density_neg = log_density_samples[neg_indices]\n",
    "        print(f\"log_density_neg: {log_density_neg}\")\n",
    "\n",
    "        # Calculate numerator: LSE(log_density + log(abs(grads))) - log(neg_cnt)\n",
    "        log_numerator_neg = torch.logsumexp(log_density_neg + torch.log(torch.abs(neg_grads) + eps), dim=0) - torch.log(neg_cnt)\n",
    "        print(f\"log_numerator_neg: {log_numerator_neg}\")\n",
    "\n",
    "        # Expected negative: (neg_cnt/total) * exp(numerator - denominator)\n",
    "        expected_neg = (neg_cnt / total_cnt) * torch.exp(log_numerator_neg - log_denominator)\n",
    "        print(f\"expected_neg: {expected_neg}\")\n",
    "    else:\n",
    "        expected_neg = torch.tensor(0.0, dtype=grad_samples.dtype, device=grad_samples.device)\n",
    "    \n",
    "    # Return pos - neg\n",
    "    return expected_pos - expected_neg\n",
    "\n",
    "\n",
    "def _calculate_weighted_score_old(grad_samples: torch.Tensor, log_density_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically stable computation of an expectation of gradients weighted by normalized probabilities.\n",
    "    Computes: E_{p(x)}[grad(x)] = sum(p_i * grad_i) / sum(p_i)\n",
    "    \"\"\"\n",
    "    eps = 1e-30\n",
    "    print(f'OLD VERSION -----------------------------')\n",
    "    while log_density_samples.dim() < grad_samples.dim():\n",
    "        log_density_samples = log_density_samples.unsqueeze(-1)\n",
    "    log_den = torch.logsumexp(log_density_samples, dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "    print(f\"log_den old: {log_den}\")\n",
    "\n",
    "    pos_grads = torch.where(grad_samples >= 0, grad_samples, 0.)\n",
    "    print(f\"pos_grads: {pos_grads}\")\n",
    "    neg_grads = torch.where(grad_samples < 0, -grad_samples, 0.)\n",
    "    print(f\"neg_grads: {neg_grads}\")\n",
    "\n",
    "    log_num_pos = torch.logsumexp(log_density_samples + torch.log(pos_grads + eps), dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "    log_num_neg = torch.logsumexp(log_density_samples + torch.log(neg_grads + eps), dim=0) - torch.log(torch.tensor(len(log_density_samples), dtype=log_density_samples.dtype, device=log_density_samples.device))\n",
    "    print(f\"log_num_pos: {log_num_pos}\")\n",
    "    print(f\"log_num_neg: {log_num_neg}\")\n",
    "\n",
    "    total_pos = torch.exp(log_num_pos - log_den)\n",
    "    print(f\"total_pos: {total_pos}\")\n",
    "    total_neg = torch.exp(log_num_neg - log_den)\n",
    "    print(f\"total_neg: {total_neg}\")\n",
    "    return total_pos - total_neg\n",
    "\n",
    "\n",
    "grads = torch.tensor([1.0, -2.0, 3.0, -4.0, 5.0], dtype=torch.float32)\n",
    "log_density_samples = torch.tensor([1.0, 2.0, -3.0, 4.0, 5.0], dtype=torch.float32)\n",
    "print(f\"grads: {grads}\")\n",
    "print(f\"log_density_samples: {log_density_samples}\")\n",
    "\n",
    "\n",
    "\n",
    "res_new = _calculate_weighted_score_v2(grads, log_density_samples)\n",
    "res_old = _calculate_weighted_score_old(grads, log_density_samples)\n",
    "print(f\"res_new: {res_new}\")\n",
    "print(f\"res_old: {res_old}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (708705235.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[56], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(torch.log(torch.tensor(1e-30))\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print(torch.log(torch.tensor(1e-30)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dibs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
