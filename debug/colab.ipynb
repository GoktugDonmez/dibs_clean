{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Any, Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = logging.getLogger()\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "    # The matrix power operation is a differentiable way to check for cycles.\n",
    "    return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "\n",
    "def log_gaussian_likelihood(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    gaussian_dist = Normal(loc=pred_mean, scale=sigma)\n",
    "    log_prob = gaussian_dist.log_prob(x)\n",
    "    return torch.sum(log_prob)\n",
    "\n",
    "def scores(z: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "    z shape (n, d, k 2) \n",
    "    returns raw scores \n",
    "  \"\"\"\n",
    "  u, v = z[..., 0], z[..., 1]\n",
    "  raw_scores = hparams[\"alpha\"] * torch.einsum('...ik,...jk->...ij', u, v)\n",
    "  _, d = z.shape[:-1]\n",
    "  diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "  return raw_scores * diag_mask\n",
    "\n",
    "def soft_gmat(z: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    apply sigmoid to raw scores\n",
    "    get the edge probability matrix\n",
    "    return shape (d, d)\n",
    "    \"\"\"\n",
    "    raw_scores = scores(z, hparams)\n",
    "    edge_probs = torch.sigmoid(raw_scores)\n",
    "    d= z.shape[0]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    return edge_probs * diag_mask\n",
    "\n",
    "def gumbel_softmax_sample(g_soft: torch.Tensor, tau: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    skip for now\n",
    "    \"\"\"\n",
    "    return \"skip for now\"\n",
    "\n",
    "def log_full_likelihood(data: Dict[str, Any], g_soft: torch.Tensor, theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    calculate the log full likelihood\n",
    "    expert belief: update this to use interventions, change the full likelihood \n",
    "    \"\"\"\n",
    "    x_data = data['x']\n",
    "    effective_W = theta * g_soft\n",
    "    pred_mean = torch.matmul(x_data, effective_W)\n",
    "    sigma_obs = hparams.get('sigma_obs_noise', 0.1)\n",
    "    return log_gaussian_likelihood(x_data, pred_mean, sigma=sigma_obs)\n",
    "\n",
    "def log_theta_prior(theta_effective: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    return log_gaussian_likelihood(theta_effective, torch.zeros_like(theta_effective), sigma=sigma)\n",
    "\n",
    "def gumbel_acyclic_constr_mc(z: torch.Tensor, d: int, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    implement the gumbel_acyclic_constr_mc later\n",
    "    for now use REINFORCE estimator\n",
    "    \"\"\"\n",
    "    return \"skip for now\"\n",
    "\n",
    "def analytic_score_g_given_z(z, g, hparams):\n",
    "    # 1. logits and probabilities\n",
    "    probs = soft_gmat(z, hparams)\n",
    "    diff   = g - probs                 # (g_ij − σ(s_ij))\n",
    "    u, v   = z[..., 0], z[..., 1]      # (d,k)\n",
    "\n",
    "    # 2. gradients wrt u and v\n",
    "    grad_u = hparams['alpha'] * torch.einsum('ij,jk->ik', diff, v)   # (d,k)\n",
    "    grad_v = hparams['alpha'] * torch.einsum('ij,ik->jk', diff, u)   # (d,k)\n",
    "\n",
    "    return torch.stack([grad_u, grad_v], dim=-1)          # (d,k,2)\n",
    "\n",
    "\n",
    "\n",
    "def score_acyclic_constr_mc(z: torch.Tensor,  hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    score estimator for the acyclicity constraint\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - h_g (torch.Tensor): The acyclicity values for each sample [n_samples].\n",
    "        - log_prob_g (torch.Tensor): The log probability of each sample [n_samples].\n",
    "    \"\"\"\n",
    "    d = z.shape[0]\n",
    "    n_samples = hparams.get('n_mc_samples', 64)\n",
    "    \n",
    "    # 1. Sample hard graphs\n",
    "    g_samples = [torch.bernoulli(soft_gmat(z, hparams)) for _ in range(n_samples)]\n",
    "\n",
    "    # 2. Calculate the \"reward\" h(G) for each sample\n",
    "    total_acyl_score = 0\n",
    "    for g in g_samples:\n",
    "        reward = acyclic_constr(g, d)\n",
    "        score = analytic_score_g_given_z(z, g, hparams)\n",
    "        total_acyl_score += reward * score\n",
    "    \n",
    "    return total_acyl_score / n_samples\n",
    "\n",
    "\n",
    "\n",
    "def grad_z_score(z: torch.Tensor, data: Dict[str, Any], theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \n",
    "    # acylic + prior + likelihood\n",
    "\n",
    "    # acylic grad\n",
    "    grad_acylic = score_acyclic_constr_mc(z, hparams)\n",
    "    grad_z_prior = - (z / hparams['sigma_z']**2)\n",
    "\n",
    "    grad_prior = grad_z_prior - ( hparams['beta'] * grad_acylic)\n",
    "\n",
    "\n",
    "    # likelihood grad with score estimator\n",
    "    n_samples = hparams.get('n_mc_samples', 64)\n",
    "    total_likelihood_score = 0\n",
    "    for _ in range(n_samples):\n",
    "        g_hard = torch.bernoulli(soft_gmat(z, hparams))\n",
    "        ## what should go here?  the full likelihood as the reward ?\n",
    "        \n",
    "        log_joint_reward  = log_full_likelihood(data, g_hard, theta, hparams)  + log_theta_prior(theta * g_hard, hparams.get('theta_prior_sigma'))\n",
    "\n",
    "        score = analytic_score_g_given_z(z, g_hard, hparams)\n",
    "\n",
    "        total_likelihood_score += log_joint_reward * score\n",
    "    grad_likelihood = total_likelihood_score / n_samples\n",
    "\n",
    "    total_grad = grad_prior + grad_likelihood\n",
    "\n",
    "    return total_grad\n",
    "\n",
    "def grad_theta_score(z: torch.Tensor, data: Dict[str, Any], theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the gradient ∇_Θ log p(Z,Θ|D) using a self-normalized importance sampling\n",
    "    estimator, which correctly implements the ratio formula from the DiBS paper.\n",
    "    \"\"\"\n",
    "    n_samples = hparams.get('n_mc_samples', 64)\n",
    "    \n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        theta_for_grad = theta.clone().requires_grad_(True)\n",
    "        \n",
    "        # 1. Sample a hard graph G\n",
    "        with torch.no_grad():\n",
    "            g_hard = torch.bernoulli(soft_gmat(z, hparams))\n",
    "\n",
    "        # 2. Calculate the log-density `log p(D,Θ|G)` for this sample.\n",
    "        # This will be used to create the softmax weights.\n",
    "        log_density = (\n",
    "            log_full_likelihood(data, g_hard, theta_for_grad, hparams) + \n",
    "            log_theta_prior(theta_for_grad * g_hard, hparams.get('theta_prior_sigma', 1.0))\n",
    "        )\n",
    "        \n",
    "        # 3. Calculate the gradient of this log-density, ∇_Θ log p(D,Θ|G).\n",
    "        grad, = torch.autograd.grad(log_density, theta_for_grad)\n",
    "\n",
    "        # Store the results for this sample\n",
    "        log_density_samples.append(log_density)\n",
    "        grad_samples.append(grad)\n",
    "\n",
    "    # 4. Compute the final gradient as a weighted average.\n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grad_p = torch.stack(grad_samples)\n",
    "    \n",
    "    # Use the log-sum-exp trick for numerically stable softmax weights\n",
    "    weights = torch.softmax(log_p, dim=0)\n",
    "\n",
    "    # Broadcast weights `(n_samples)` to match gradient shape `(n_samples, d, d)`\n",
    "    while weights.dim() < grad_p.dim():\n",
    "        weights = weights.unsqueeze(-1)\n",
    "        \n",
    "    final_grad = (weights * grad_p).sum(dim=0)\n",
    "\n",
    "    return final_grad    \n",
    "\n",
    "\n",
    "def grad_log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    A top-level function that orchestrates the calculation of all gradients.\n",
    "    \"\"\"\n",
    "    grad_z = grad_z_score(params[\"z\"], data, params[\"theta\"].detach(), hparams)\n",
    "    grad_th = grad_theta_score(params[\"z\"].detach(), data, params[\"theta\"], hparams)\n",
    "    return grad_z, grad_th\n",
    "\n",
    "def log_joint(params: Dict[str, Any], data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the value of the unnormalized log-joint probability for debugging.\n",
    "    This function combines the different log-probability terms.\n",
    "    \"\"\"\n",
    "    z = params[\"z\"]\n",
    "    theta = params[\"theta\"]\n",
    "    g_soft = soft_gmat(z, hparams)\n",
    "    # For simplicity, we can approximate the expected likelihood with the soft graph\n",
    "    log_likelihood_val = log_full_likelihood(data, g_soft, theta, hparams)\n",
    "    log_theta_prior_val = log_theta_prior(theta, hparams.get('sigma_theta_prior', 1.0))\n",
    "    # The acyclicity constraint is harder to evaluate without sampling, so we can skip it for a rough estimate\n",
    "    # or use the soft graph version\n",
    "    with torch.no_grad():\n",
    "        n_samples = hparams.get('n_mc_samples', 64)\n",
    "        h_vals = []\n",
    "        for _ in range(n_samples):\n",
    "            g_hard = torch.bernoulli(soft_gmat(z, hparams))\n",
    "            d = z.shape[0]\n",
    "            h_vals.append(acyclic_constr(g_hard, d))\n",
    "    acyclicity_val = torch.mean(torch.stack(h_vals))\n",
    "\n",
    "    # Prior on Z\n",
    "    log_z_prior = -0.5 * torch.sum(z**2) / hparams.get('latent_prior_std', 1.0)**2\n",
    "    log_joint = log_likelihood_val + log_theta_prior_val + log_z_prior - hparams['beta'] * acyclicity_val\n",
    "    logging.info(f\"Log-joint: {log_joint.item():.4f}, \"\n",
    "                 f\"Log-Likelihood: {log_likelihood_val.item():.4f}, \"\n",
    "                 f\"Log-Theta-Prior: {log_theta_prior_val.item():.4f}, \"\n",
    "                 f\"Acyclicity: {acyclicity_val.item():.4f}\")\n",
    "    return log_joint\n",
    "\n",
    "def update_dibs_hparams(hparams: Dict[str, Any], t: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Handles annealing schedules for hyperparameters.\n",
    "    \"\"\"\n",
    "    # Simple linear annealing\n",
    "    hparams['alpha'] = hparams['alpha_base'] * t *0.02\n",
    "    hparams['beta'] = hparams['beta_base'] * t\n",
    "    hparams['current_t'] = t\n",
    "    return hparams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. DATA GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_ground_truth_chain_data(num_samples, chain_length, obs_noise_std):\n",
    "    \"\"\"Generates data for a simple causal chain: X1 -> X2 -> ... -> Xn.\"\"\"\n",
    "    if chain_length < 2:\n",
    "        raise ValueError(\"Chain length must be at least 2\")\n",
    "    \n",
    "    G_true = torch.zeros(chain_length, chain_length, dtype=torch.float32)\n",
    "    for i in range(chain_length - 1):\n",
    "        if i == 0:\n",
    "            G_true[i, i + 1] = 2\n",
    "        elif i == 1:\n",
    "            G_true[i, i + 1] = -1.5\n",
    "        else: \n",
    "            G_true[i, i + 1] = 1.0\n",
    "\n",
    "    Theta_true = torch.zeros(chain_length, chain_length, dtype=torch.float32)\n",
    "    for i in range(chain_length - 1):\n",
    "        Theta_true[i, i + 1] = (torch.rand(1).item() - 0.5) * 4.0 # Random weight in [-2, 2]\n",
    "    \n",
    "    X_data = torch.zeros(num_samples, chain_length)\n",
    "    X_data[:, 0] = torch.randn(num_samples) * obs_noise_std\n",
    "    \n",
    "    for i in range(1, chain_length):\n",
    "        parent_value = X_data[:, i - 1]\n",
    "        weight = Theta_true[i - 1, i]\n",
    "        noise = torch.randn(num_samples) * obs_noise_std\n",
    "        X_data[:, i] = weight * parent_value + noise\n",
    "        \n",
    "    return X_data, G_true, Theta_true\n",
    "\n",
    "def generate_ground_truth_erdos_renyi_data(num_samples, n_nodes, p_edge, obs_noise_std):\n",
    "    \"\"\"Generates data from an Erdős-Rényi DAG structure.\"\"\"\n",
    "    if n_nodes < 2:\n",
    "        raise ValueError(\"Number of nodes must be at least 2\")\n",
    "    \n",
    "    # Generate adjacency matrix for Erdős-Rényi DAG\n",
    "    G_true = torch.zeros(n_nodes, n_nodes, dtype=torch.float32)\n",
    "    Theta_true = torch.zeros(n_nodes, n_nodes, dtype=torch.float32)\n",
    "    \n",
    "    # Only consider upper triangular part to ensure DAG structure\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(i + 1, n_nodes):\n",
    "            if torch.rand(1).item() < p_edge:\n",
    "                G_true[i, j] = 1.0\n",
    "                # Random weight between -2 and 2\n",
    "                Theta_true[i, j] = (torch.rand(1).item() - 0.5) * 4.0\n",
    "    \n",
    "    # Generate data following the DAG structure\n",
    "    X_data = torch.zeros(num_samples, n_nodes)\n",
    "    \n",
    "    # Generate data in topological order (i before j for edges i->j)\n",
    "    for j in range(n_nodes):\n",
    "        # Find parent nodes (nodes with edges pointing to j)\n",
    "        parent_indices = (G_true[:, j] == 1).nonzero(as_tuple=True)[0]\n",
    "        noise = torch.randn(num_samples) * obs_noise_std\n",
    "        \n",
    "        if len(parent_indices) == 0:\n",
    "            # Root node: just noise\n",
    "            X_data[:, j] = noise\n",
    "        else:\n",
    "            # Sum weighted contributions from parents\n",
    "            parent_contribution = torch.zeros(num_samples)\n",
    "            for parent_idx in parent_indices:\n",
    "                weight = Theta_true[parent_idx, j]\n",
    "                parent_contribution += weight * X_data[:, parent_idx]\n",
    "            X_data[:, j] = parent_contribution + noise\n",
    "    \n",
    "    return X_data, G_true, Theta_true\n",
    "\n",
    "def generate_ground_truth_scale_free_data(num_samples, n_nodes, m_edges, obs_noise_std):\n",
    "    \"\"\"Generates data from a Scale-Free DAG structure using preferential attachment.\"\"\"\n",
    "    if n_nodes < 2:\n",
    "        raise ValueError(\"Number of nodes must be at least 2\")\n",
    "    if m_edges >= n_nodes:\n",
    "        raise ValueError(\"m_edges must be less than n_nodes\")\n",
    "    \n",
    "    # Initialize adjacency and weight matrices\n",
    "    G_true = torch.zeros(n_nodes, n_nodes, dtype=torch.float32)\n",
    "    Theta_true = torch.zeros(n_nodes, n_nodes, dtype=torch.float32)\n",
    "    \n",
    "    # Build scale-free DAG using preferential attachment\n",
    "    # Start with the first m_edges+1 nodes forming a small DAG\n",
    "    for i in range(min(m_edges + 1, n_nodes - 1)):\n",
    "        for j in range(i + 1, min(m_edges + 1, n_nodes)):\n",
    "            if torch.rand(1).item() < 0.5:  # 50% chance for initial connections\n",
    "                G_true[i, j] = 1.0\n",
    "                Theta_true[i, j] = (torch.rand(1).item() - 0.5) * 4.0\n",
    "    \n",
    "    # Add remaining nodes with preferential attachment\n",
    "    for new_node in range(m_edges + 1, n_nodes):\n",
    "        # Calculate in-degrees for preferential attachment\n",
    "        in_degrees = G_true.sum(dim=0)  # Sum over rows gives in-degree\n",
    "        in_degrees[:new_node] += 1  # Add 1 to avoid zero probabilities\n",
    "        \n",
    "        # Normalize to get probabilities\n",
    "        probs = in_degrees[:new_node] / in_degrees[:new_node].sum()\n",
    "        \n",
    "        # Select m_edges nodes to connect to (from existing nodes)\n",
    "        num_connections = min(m_edges, new_node)\n",
    "        \n",
    "        # Sample connections based on preferential attachment\n",
    "        for _ in range(num_connections):\n",
    "            # Choose a node to connect from (preferential attachment)\n",
    "            source_node = torch.multinomial(probs, 1, replacement=False).item()\n",
    "            \n",
    "            if G_true[source_node, new_node] == 0:  # Avoid duplicate edges\n",
    "                G_true[source_node, new_node] = 1.0\n",
    "                Theta_true[source_node, new_node] = (torch.rand(1).item() - 0.5) * 4.0\n",
    "                \n",
    "            # Update probabilities to reflect new connection\n",
    "            probs[source_node] = 0  # Remove this node from future selection\n",
    "            if probs.sum() > 0:\n",
    "                probs = probs / probs.sum()  # Renormalize\n",
    "    \n",
    "    # Generate data following the DAG structure\n",
    "    X_data = torch.zeros(num_samples, n_nodes)\n",
    "    \n",
    "    # Generate data in topological order\n",
    "    for j in range(n_nodes):\n",
    "        # Find parent nodes\n",
    "        parent_indices = (G_true[:, j] == 1).nonzero(as_tuple=True)[0]\n",
    "        noise = torch.randn(num_samples) * obs_noise_std\n",
    "        \n",
    "        if len(parent_indices) == 0:\n",
    "            # Root node: just noise\n",
    "            X_data[:, j] = noise\n",
    "        else:\n",
    "            # Sum weighted contributions from parents\n",
    "            parent_contribution = torch.zeros(num_samples)\n",
    "            for parent_idx in parent_indices:\n",
    "                weight = Theta_true[parent_idx, j]\n",
    "                parent_contribution += weight * X_data[:, parent_idx]\n",
    "            X_data[:, j] = parent_contribution + noise\n",
    "    \n",
    "    return X_data, G_true, Theta_true\n",
    "\n",
    "# =============================================================================\n",
    "# 3. CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    seed = 31\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # --- Data Generation ---\n",
    "    # Choose data source: 'simple_chain', 'erdos_renyi', 'scale_free'\n",
    "    data_source = 'scale_free'\n",
    "    \n",
    "    # --- Data Parameters ---\n",
    "    d_nodes = 5\n",
    "    num_samples = 100\n",
    "    obs_noise_std = 0.1\n",
    "    \n",
    "    # Parameters for 'simple_chain'\n",
    "    chain_length = d_nodes\n",
    "    \n",
    "    # Parameters for 'erdos_renyi'\n",
    "    p_edge = 0.7  # Probability of edge existence\n",
    "    \n",
    "    # Parameters for 'scale_free'\n",
    "    m_edges = 2  # Number of edges to attach from each new node\n",
    "\n",
    "    # --- Model ---\n",
    "    k_latent = d_nodes\n",
    "    alpha_base = 0.02  # Base value for annealing\n",
    "    beta_base = 1.0   # Base value for annealing\n",
    "    theta_prior_sigma = 1.0\n",
    "    \n",
    "    # --- MC Sampling ---\n",
    "    n_mc_samples = 64\n",
    "\n",
    "    # --- Training ---\n",
    "    lr = 5e-3\n",
    "    num_iterations = 2000\n",
    "    debug_print_iter = 100\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # --- Setup ---\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    log.info(f\"Running on device: {cfg.device}\")\n",
    "\n",
    "    # --- Data Generation ---\n",
    "    log.info(f\"Using data source: {cfg.data_source}\")\n",
    "    \n",
    "    if cfg.data_source == 'simple_chain':\n",
    "        data_x, G_true, Theta_true = generate_ground_truth_chain_data(\n",
    "            num_samples=cfg.num_samples,\n",
    "            chain_length=cfg.chain_length,\n",
    "            obs_noise_std=cfg.obs_noise_std\n",
    "        )\n",
    "        log.info(f\"Generated simple chain data with {cfg.chain_length} nodes.\")\n",
    "        \n",
    "    elif cfg.data_source == 'erdos_renyi':\n",
    "        data_x, G_true, Theta_true = generate_ground_truth_erdos_renyi_data(\n",
    "            num_samples=cfg.num_samples,\n",
    "            n_nodes=cfg.d_nodes,\n",
    "            p_edge=cfg.p_edge,\n",
    "            obs_noise_std=cfg.obs_noise_std\n",
    "        )\n",
    "        log.info(f\"Generated Erdős-Rényi data with {cfg.d_nodes} nodes and p_edge={cfg.p_edge}.\")\n",
    "        \n",
    "    elif cfg.data_source == 'scale_free':\n",
    "        data_x, G_true, Theta_true = generate_ground_truth_scale_free_data(\n",
    "            num_samples=cfg.num_samples,\n",
    "            n_nodes=cfg.d_nodes,\n",
    "            m_edges=cfg.m_edges,\n",
    "            obs_noise_std=cfg.obs_noise_std\n",
    "        )\n",
    "        log.info(f\"Generated Scale-Free data with {cfg.d_nodes} nodes and m_edges={cfg.m_edges}.\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data_source: {cfg.data_source}. \"\n",
    "                        f\"Choose from: 'simple_chain', 'erdos_renyi', 'scale_free'\")\n",
    "    \n",
    "    data = {'x': data_x.to(cfg.device)}\n",
    "    log.info(f\"Ground truth adjacency matrix:\\n{G_true}\")\n",
    "    log.info(f\"Ground truth weights matrix:\\n{Theta_true}\")\n",
    "    \n",
    "    # Count the number of edges in the true graph\n",
    "    num_edges = (G_true > 0).sum().item()\n",
    "    log.info(f\"Number of edges in ground truth graph: {num_edges}\")\n",
    "\n",
    "    # --- Initialization ---\n",
    "    particle = {\n",
    "        \"z\": nn.Parameter(torch.randn(cfg.d_nodes, cfg.k_latent, 2, device=cfg.device)),\n",
    "        \"theta\": nn.Parameter(torch.randn(cfg.d_nodes, cfg.d_nodes, device=cfg.device)),\n",
    "    }\n",
    "    \n",
    "    sigma_z = 1.0 / torch.sqrt(torch.tensor(cfg.k_latent))\n",
    "    hparams = {\n",
    "        \"alpha\": 0.0, # Will be annealed\n",
    "        \"beta\": 0.0,  # Will be annealed\n",
    "        \"alpha_base\": cfg.alpha_base,\n",
    "        \"beta_base\": cfg.beta_base,\n",
    "        \"sigma_z\": sigma_z,\n",
    "        \"sigma_obs_noise\": cfg.obs_noise_std,\n",
    "        \"theta_prior_sigma\": cfg.theta_prior_sigma,\n",
    "        \"n_mc_samples\": cfg.n_mc_samples,\n",
    "        \"total_steps\": cfg.num_iterations\n",
    "    }\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(particle.values(), lr=cfg.lr)\n",
    "\n",
    "    # --- Gradient Ascent Loop ---\n",
    "    log.info(\"Starting training...\")\n",
    "    for t in range(1, cfg.num_iterations + 1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update annealed hyperparameters\n",
    "        hparams = update_dibs_hparams(hparams, t)\n",
    "        \n",
    "        # Get gradients of the log-joint\n",
    "        grad_z, grad_th = grad_log_joint(particle, data, hparams)\n",
    "        \n",
    "        # Assign gradients for gradient ASCENT (optimizers perform descent)\n",
    "        particle['z'].grad = -grad_z\n",
    "        particle['theta'].grad = -grad_th\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Logging ---\n",
    "        if t % cfg.debug_print_iter == 0 or t == cfg.num_iterations:\n",
    "            with torch.no_grad():\n",
    "                lj_val = log_joint(particle, data, hparams).item()\n",
    "                grad_z_norm = torch.linalg.norm(grad_z).item()\n",
    "                grad_theta_norm = torch.linalg.norm(grad_th).item()\n",
    "                edge_probs = soft_gmat(particle['z'], hparams)\n",
    "\n",
    "                log.info(f\"--- Iter {t}/{cfg.num_iterations} ---\")\n",
    "                log.info(f\"log_joint={lj_val:.2f} | grad_Z_norm={grad_z_norm:.2e} | grad_Theta_norm={grad_theta_norm:.2e}\")\n",
    "                log.info(f\"Annealed: alpha={hparams['alpha']:.3f}, beta={hparams['beta']:.3f}\")\n",
    "                log.info(f\"Current Edge Probs (rounded):\\n{np.round(edge_probs.cpu().numpy(), 2)}\")\n",
    "\n",
    "    log.info(\"Training finished.\")\n",
    "    with torch.no_grad():\n",
    "        final_probs = soft_gmat(particle['z'], hparams)\n",
    "        \n",
    "        # Create hard graph from learned probabilities (threshold at 0.5)\n",
    "        learned_hard_graph = (final_probs > 0.5).float()\n",
    "        \n",
    "        log.info(f\"\\n{'='*60}\")\n",
    "        log.info(f\"FINAL RESULTS COMPARISON\")\n",
    "        log.info(f\"{'='*60}\")\n",
    "        log.info(f\"Ground Truth Adjacency Matrix:\\n{G_true}\")\n",
    "        log.info(f\"Ground Truth Weights Matrix:\\n{Theta_true}\")\n",
    "        log.info(f\"\\nLearned Edge Probabilities:\\n{final_probs.cpu().numpy()}\")\n",
    "        log.info(f\"Learned Hard Graph (threshold=0.5):\\n{learned_hard_graph.cpu().numpy()}\")\n",
    "        log.info(f\"Learned Theta:\\n{particle['theta'].cpu().numpy()}\")\n",
    "        \n",
    "        # Compute structural difference\n",
    "        structural_diff = torch.abs(G_true - learned_hard_graph).sum().item()\n",
    "        log.info(f\"\\nStructural Hamming Distance: {structural_diff}\")\n",
    "        \n",
    "        # Show effective learned weights (G_learned * Theta_learned)\n",
    "        effective_learned_weights = learned_hard_graph * particle['theta'].cpu()\n",
    "        effective_true_weights = G_true * Theta_true\n",
    "        log.info(f\"\\nGround Truth Effective Weights (G_true * Theta_true):\\n{effective_true_weights}\")\n",
    "        log.info(f\"Learned Effective Weights (G_learned * Theta_learned):\\n{effective_learned_weights}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
