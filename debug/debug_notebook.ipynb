{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/dibs.py\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    try:\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except RuntimeError:\n",
    "        trace, p = torch.tensor(0.0, device=g.device, dtype=g.dtype), g.clone()\n",
    "        for k in range(1, min(d + 1, 20)):\n",
    "            trace += (alpha ** k) * torch.trace(p) / k\n",
    "            if k < 19:\n",
    "                p = p @ g\n",
    "        return trace\n",
    "\n",
    "\n",
    "def log_gaussian_likelihood(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    residuals = x - pred_mean\n",
    "    #old incorrect log_prob = -0.5 * (np.log(2 * np.pi) -  (1/2)* torch.log(sigma_tensor**2) -  0.5*(residuals / sigma_tensor) ** 2) old\n",
    "    log_prob = -0.5 * (torch.log(2 * torch.pi * sigma_tensor**2)) - 0.5 * ((residuals / sigma_tensor)**2)\n",
    "    #normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    #log_prob = normal_dist.log_prob(x)\n",
    "\n",
    "    return torch.sum(log_prob)\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    raw_scores = alpha * torch.einsum('...ik,...jk->...ij', u, v)\n",
    "    *batch_dims, d, _ = z.shape[:-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    if batch_dims:\n",
    "        diag_mask = diag_mask.expand(*batch_dims, d, d)\n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    if probs.ndim == 3:\n",
    "        diag_mask = diag_mask.expand(probs.shape[0], d, d)\n",
    "    return probs * diag_mask\n",
    "\n",
    "def gumbel_soft_gmat(z: torch.Tensor,\n",
    "                     hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Soft Gumbel–Softmax adjacency  (Eq. B.6)\n",
    "\n",
    "        g_ij  = σ_τ( L_ij + α⟨u_i , v_j⟩ )\n",
    "\n",
    "    where  L_ij ~ Logistic(0,1)  and  τ = hparams['tau']. appendix b2\n",
    "    \"\"\"\n",
    "    raw = scores(z, hparams[\"alpha\"])\n",
    "\n",
    "    # Logistic(0,1) noise   L = log U - log(1-U)\n",
    "    u = torch.rand_like(raw)\n",
    "    L = torch.log(u) - torch.log1p(-u)\n",
    "\n",
    "    logits = (raw + L) / hparams[\"tau\"]\n",
    "    g_soft = torch.sigmoid(logits)\n",
    "\n",
    "    d = g_soft.size(-1)\n",
    "    mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    return g_soft * mask\n",
    "\n",
    "def log_full_likelihood(data: Dict[str, Any], soft_gmat: torch.Tensor, theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    ## TODO: Expert belief: update this to use interventions, change the full likelihood \n",
    "    # and also add log bernoulli likelihood calculatior\n",
    "    x_data = data['x']\n",
    "    effective_W = theta * soft_gmat\n",
    "    pred_mean = torch.matmul(x_data, effective_W)\n",
    "    sigma_obs = hparams.get('sigma_obs_noise', 0.1)\n",
    "    return log_gaussian_likelihood(x_data, pred_mean, sigma=sigma_obs)\n",
    "\n",
    "def log_theta_prior(theta_effective: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    return log_gaussian_likelihood(theta_effective, torch.zeros_like(theta_effective), sigma=sigma)\n",
    "\n",
    "def gumbel_acyclic_constr_mc(z: torch.Tensor, d: int, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    h_samples = []\n",
    "    for _ in range(hparams['n_nongrad_mc_samples']):\n",
    "        # FOR NOW, JUST GIVE THE SOFT MATRIX, AND BY ANNEALING IT TO HARD MATRIX\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "        h_samples.append(acyclic_constr(g_soft, d))\n",
    "        \n",
    "        # should gumbel soft gmat to hard gmat be done with >0.5 or with a sigmoid?  \n",
    "        #print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #g_hard = torch.bernoulli(g_soft)\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #h_samples.append(acyclic_constr(g_hard, d))\n",
    "        #g_hard = (g_soft > 0.5).float()\n",
    "        #how about this  mentioned in dibs       g_ST   = g_hard + (g_soft - g_soft.detach())   # straight-through\n",
    "        \n",
    "        #TODO fix above\n",
    "        # for now use g_soft\n",
    "        \n",
    "    h_samples = torch.stack(h_samples)\n",
    "\n",
    "\n",
    "    return torch.mean(h_samples, dim=0)\n",
    "\n",
    "def grad_z_log_joint_gumbel(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    d = z.shape[0]\n",
    "    theta_const = theta\n",
    "    \n",
    "    #z.requires_grad_(True)\n",
    "    # --- Part 1: Prior Gradient ---\n",
    "    # MC estimate of gradient of acyclicity constraint using Gumbel soft graphs\n",
    "\n",
    "    h_mean = gumbel_acyclic_constr_mc(z, d, hparams)\n",
    "    grad_h_mc = torch.autograd.grad(h_mean, z)[0]\n",
    "    grad_log_z_prior_total = -hparams['beta'] * grad_h_mc - (z / hparams['sigma_z']**2)\n",
    "\n",
    "    # --- Part 2: Likelihood Gradient ---\n",
    "    \n",
    "    # 1. We need to collect the log-probability AND the gradient for each sample.\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "\n",
    "    for _ in range( hparams['n_grad_mc_samples']):\n",
    "        # 2. Generate a single soft graph sample.\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "        # 3. Calculate the log-joint for this single sample.\n",
    "        log_density_one_sample = log_full_likelihood(data, g_soft, theta_const, hparams) + \\\n",
    "                                 log_theta_prior(theta_const * g_soft, hparams.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "        # 4. Calculate the gradient for this single sample.\n",
    "        # We must use retain_graph=True because we are doing a backward pass\n",
    "        # inside a loop, and PyTorch would otherwise free the graph memory.\n",
    "        grad, = torch.autograd.grad(log_density_one_sample, z, retain_graph=True)\n",
    "        \n",
    "        log_density_samples.append(log_density_one_sample)\n",
    "        grad_samples.append(grad)\n",
    "\n",
    "    # 5. After the loop, we can safely detach z_ from any further graph history.\n",
    "\n",
    "    # 6. Compute the final likelihood gradient using the stable weighted average.\n",
    "    # This correctly computes E[p*∇log(p)] / E[p]\n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grad_p = torch.stack(grad_samples)\n",
    "    grad_lik = weighted_grad(log_p, grad_p)\n",
    "\n",
    "\n",
    "    #if z.grad is not None:\n",
    "    #    z.grad.zero_()\n",
    "    #z.requires_grad_(False)\n",
    "    \n",
    "    # Final combined gradient\n",
    "    \n",
    "\n",
    "\n",
    "    total = grad_log_z_prior_total + grad_lik\n",
    "    # 3) Combine\n",
    "    # ------------------------------------------------\n",
    "    return total.detach()\n",
    "\n",
    "\n",
    "## SCORE BASED ESTIMATOR FOR GRADIENT Z \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Score-function estimator for ∇_Z log p(Z,Θ | D)\n",
    "#  (Section B.2 of the paper, b = 0)\n",
    "# ------------------------------------------------------------\n",
    "def analytic_score_g_given_z(z, g, hparams):\n",
    "    # 1. logits and probabilities\n",
    "    probs = bernoulli_soft_gmat(z, hparams)\n",
    "    diff   = g - probs                 # (g_ij − σ(s_ij))\n",
    "    u, v   = z[..., 0], z[..., 1]      # (d,k)\n",
    "\n",
    "    # 2. gradients wrt u and v\n",
    "    grad_u = hparams['alpha'] * torch.einsum('ij,jk->ik', diff, v)   # (d,k)\n",
    "    grad_v = hparams['alpha'] * torch.einsum('ij,ik->jk', diff, u)   # (d,k)\n",
    "\n",
    "    return torch.stack([grad_u, grad_v], dim=-1)          # (d,k,2)\n",
    "\n",
    "\n",
    "def grad_z_log_joint_score(z: torch.Tensor,\n",
    "                           theta: torch.Tensor,\n",
    "                           data: Dict[str, Any],\n",
    "                           hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ∇_Z log p(Z,Θ | D)  using the score-function (REINFORCE) estimator.\n",
    "\n",
    "    This replaces the Gumbel-soft estimator.\n",
    "    \"\"\"\n",
    "    sigma_z2 = hparams['sigma_z'] ** 2\n",
    "    beta     = hparams['beta']\n",
    "\n",
    "    M = hparams['n_grad_mc_samples'] # M = 50\n",
    "    d = z.shape[0]                   # d = 4\n",
    "    theta_const = theta\n",
    "\n",
    "\n",
    "    # 1. sample hard graphs \n",
    "    with torch.no_grad():\n",
    "        g_hard_samples = [torch.bernoulli(bernoulli_soft_gmat(z, hparams)) for _ in range(M)]\n",
    "\n",
    "    ll = []\n",
    "    scores = []\n",
    "    for g in g_hard_samples:\n",
    "        log_lik = log_full_likelihood(data, g, theta_const, hparams)\n",
    "        theta_eff = theta_const * g\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        \n",
    "        # log likelihood \n",
    "        ll.append(log_lik + log_theta_prior_val)\n",
    "        \n",
    "        # score \n",
    "        scores.append(analytic_score_g_given_z(z, g, hparams))\n",
    "    \n",
    "    log_p = torch.stack(ll)\n",
    "    grad_p = torch.stack(scores)\n",
    "\n",
    "    log_p_max = log_p.max()\n",
    "    log_p_shifted = log_p - log_p_max\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    unnormalized_w = torch.exp(log_p_shifted/10)\n",
    "    #print(f'unnormalized_w shape: {unnormalized_w.shape}, values: \\n {unnormalized_w}')\n",
    "    w = unnormalized_w / unnormalized_w.sum()\n",
    "                     # (M,1,1,...)\n",
    "    #print(f'w shape: {w.shape}, values: \\n {w}')\n",
    "\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                    \n",
    "    \n",
    "    ## compute the weighted avg \n",
    "    grad_lik = (w * grad_p).sum(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Z-prior: Gaussian + acyclicity penalty --------------\n",
    "    # gumbel is possible cuz no differentiable function in expectation \n",
    "    z_ = z.detach().clone().requires_grad_(True)\n",
    "    h_mean = gumbel_acyclic_constr_mc(z_, d, hparams)       # differentiable w.r.t z_\n",
    "    grad_h, = torch.autograd.grad(h_mean, z_, retain_graph=False)\n",
    "    grad_prior = -beta * grad_h - z_ / sigma_z2\n",
    "\n",
    "    return (grad_lik + grad_prior).detach()\n",
    "\n",
    "\n",
    "\n",
    "def weighted_grad(log_p: torch.Tensor,\n",
    "                  grad_p: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return   Σ softmax(log_p)_m * grad_p[m]\n",
    "    Shapes\n",
    "        log_p   : (M,)\n",
    "        grad_p  : (M, …)   (any extra dims)\n",
    "    \"\"\"\n",
    "    # 1. numerically stable soft-max weights\n",
    "    #print(f'log_p shape: {log_p.shape}, values:\\n {log_p}')\n",
    "    #print(f'grad_p shape: {grad_p.shape}, values: \\n{grad_p}')\n",
    "    log_p_shifted = log_p - log_p.max()          # (M,)\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    w = torch.exp(log_p_shifted)\n",
    "    #print(f'w shape: {w.shape}, values:\\n {w}')\n",
    "    w = w / w.sum()\n",
    "    #print(f'w after normalization shape: {w.shape}, values:\\n {w}')\n",
    "\n",
    "    # 2. broadcast weights onto grad tensor\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                      # (M,1,1,...)\n",
    "\n",
    "    return (w * grad_p).sum(dim=0)               # same shape as grad slice\n",
    "\n",
    "\n",
    "\n",
    "def grad_theta_log_joint(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    #theta.requires_grad_(True)\n",
    "    n_samples = hparams.get('n_grad_mc_samples', 1)\n",
    "    theta_ = theta.clone().detach().requires_grad_(True)\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        g_soft = bernoulli_soft_gmat(z, hparams)\n",
    "        #print(f\"g_soft values: {g_soft}\")\n",
    "        g_hard = torch.bernoulli(g_soft)\n",
    "        #print(f\"g_hard values: {g_hard}\")\n",
    "\n",
    "        # tryign with gumbel to be consistent with grad z and gumbel mc acylci impelmentation\n",
    "        #g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        log_lik_val = log_full_likelihood(data, g_hard, theta_, hparams)\n",
    "        theta_eff = theta_ * g_hard\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        #ll_grad, = torch.autograd.grad(log_lik_val, theta_, retain_graph=True)\n",
    "        #log_theta_prior_grad, = torch.autograd.grad(log_theta_prior_val, theta_ , retain_graph=True)\n",
    "        #print(f\"ll_grad shape: {ll_grad.shape}, values: {ll_grad}\")\n",
    "        #print(f\"log_theta_prior_grad shape: {log_theta_prior_grad.shape}, values: {log_theta_prior_grad}\")\n",
    "\n",
    "        current_log_density = log_lik_val + log_theta_prior_val\n",
    "        current_grad ,= torch.autograd.grad(current_log_density, theta_)\n",
    "        log_density_samples.append(current_log_density) \n",
    "\n",
    "        grad_samples.append(current_grad)\n",
    "    #print(f\" END OF Grad_theta mc_samples, iter number: {hparams.get('current_iteration',1)}  \\n\")\n",
    "\n",
    "    log_p_tensor = torch.stack(log_density_samples)\n",
    "    grad_p_tensor = torch.stack(grad_samples)\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    #if theta.grad is not None:\n",
    "    #    theta.grad.zero_()\n",
    "    #theta.requires_grad_(False)\n",
    "\n",
    "    grad =weighted_grad(log_p_tensor, grad_p_tensor)\n",
    "    #grad = stable_gradient_estimator(log_p_tensor, grad_p_tensor)\n",
    "    #print(f\"Grad_theta shape: {grad.shape}, values: \\n {grad}\")\n",
    "\n",
    "    return  grad.detach()\n",
    "\n",
    "\n",
    "def grad_log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "    grad_z = grad_z_log_joint_gumbel(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    #grad_z = grad_z_log_joint_score(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    grad_theta = grad_theta_log_joint(params[\"z\"].detach(), params[\"theta\"], data, hparams)\n",
    "    \n",
    "    return {\"z\": grad_z, \"theta\": grad_theta}\n",
    "\n",
    "def log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    hparams_updated = update_dibs_hparams(hparams, params[\"t\"].item())\n",
    "    z, theta = params['z'], params['theta']\n",
    "    d = z.shape[0]\n",
    "\n",
    "    g_soft = bernoulli_soft_gmat(z, hparams_updated)\n",
    "    log_lik = log_full_likelihood(data, g_soft, theta, hparams_updated)\n",
    "\n",
    "    log_prior_z_gaussian = torch.sum(Normal(0.0, hparams_updated['sigma_z']).log_prob(z))\n",
    "    expected_h_val = gumbel_acyclic_constr_mc(z, d, hparams_updated)\n",
    "    log_prior_z_acyclic = -hparams_updated['beta'] * expected_h_val\n",
    "    log_prior_z = log_prior_z_gaussian + log_prior_z_acyclic\n",
    "    \n",
    "    theta_eff = theta * g_soft\n",
    "    log_prior_theta = log_theta_prior(theta_eff, hparams_updated.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "    if (hparams_updated['current_iteration'] > 850 and hparams_updated['current_iteration'] < 1200):\n",
    "        with torch.no_grad():\n",
    "            log_terms = {\n",
    "                \"log_lik\":      log_lik.item(),\n",
    "                \"z_prior_gauss\":log_prior_z_gaussian.item(),\n",
    "                \"z_prior_acyc\": log_prior_z_acyclic.item(),   # usually ≤ 0\n",
    "                \"theta_prior\":  log_prior_theta.item(),\n",
    "                \"log_joint\": log_lik + log_prior_theta + log_prior_z + log_prior_z_acyclic,\n",
    "                \"penalty\": -hparams_updated['beta'] * expected_h_val.item()\n",
    "            }\n",
    "        print(f\"[dbg] {log_terms}\")\n",
    "\n",
    "    \n",
    "    return log_lik + log_prior_z + log_prior_theta\n",
    "\n",
    "def update_dibs_hparams(hparams: Dict[str, Any], t_step: float) -> Dict[str, Any]:\n",
    "\n",
    "    hparams['beta'] = hparams['beta_base'] * t_step # linear \n",
    "\n",
    "    hparams['alpha'] = hparams['alpha_base'] * t_step  # linear slope 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hparams['current_iteration'] = t_step # Store current iteration\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def hard_gmat_from_z(z: torch.Tensor, alpha: float = 1.0) -> torch.Tensor:\n",
    "    s = scores(z, alpha)\n",
    "    return (s > 0).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing acyclic_constr function\n",
      "============================================================\n",
      "\n",
      "1. Testing small acyclic graph (d=3):\n",
      "   Acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0 (should be <= 0 for acyclic)\n",
      "\n",
      "2. Testing small cyclic graph (d=3):\n",
      "   Cyclic graph constraint: 0.011667\n",
      "   Expected: > 0 (penalizes cycles)\n",
      "\n",
      "3. Testing identity/no edges (d=4):\n",
      "   Identity matrix constraint: 5.765625\n",
      "   Expected: close to 0\n",
      "\n",
      "4. Testing large graph (d=12, eigenvalue path):\n",
      "   Large acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0\n",
      "\n",
      "5. Testing large graph with potential eigenvalue issues (d=15):\n",
      "   Problematic graph constraint: 306.006714\n",
      "   Expected: large positive value (many cycles)\n",
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.042117\n",
      "   Gradient computed successfully: False\n",
      "   ERROR in gradient computation: 'NoneType' object has no attribute 'norm'\n",
      "\n",
      "7. Testing edge cases:\n",
      "   Very small values constraint: 0.0000000000\n",
      "   Large values constraint: 422.222290\n",
      "\n",
      "============================================================\n",
      "acyclic_constr testing complete\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111888/216125626.py:91: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
      "/tmp/ipykernel_111888/216125626.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "def test_acyclic_constr():\n",
    "    \"\"\"\n",
    "    Test the acyclic_constr function with various scenarios to debug potential issues\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing acyclic_constr function\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test case 1: Small acyclic graph (d <= 10)\n",
    "    print(\"\\n1. Testing small acyclic graph (d=3):\")\n",
    "    d = 3\n",
    "    g_acyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.3],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.0, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_acyclic = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"   Acyclic graph constraint: {h_acyclic.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0 (should be <= 0 for acyclic)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 2: Small cyclic graph\n",
    "    print(\"\\n2. Testing small cyclic graph (d=3):\")\n",
    "    g_cyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.0],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.3, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_cyclic = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"   Cyclic graph constraint: {h_cyclic.item():.6f}\")\n",
    "        print(f\"   Expected: > 0 (penalizes cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 3: Identity matrix (no edges)\n",
    "    print(\"\\n3. Testing identity/no edges (d=4):\")\n",
    "    d = 4\n",
    "    g_identity = torch.eye(d, dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_identity = acyclic_constr(g_identity, d)\n",
    "        print(f\"   Identity matrix constraint: {h_identity.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 4: Large graph (d > 10, triggers eigenvalue computation)\n",
    "    print(\"\\n4. Testing large graph (d=12, eigenvalue path):\")\n",
    "    d = 12\n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "    g_large = torch.randn(d, d) * 0.1\n",
    "    g_large = torch.triu(g_large, diagonal=1)  # Upper triangular (acyclic)\n",
    "    \n",
    "    try:\n",
    "        h_large = acyclic_constr(g_large, d)\n",
    "        print(f\"   Large acyclic graph constraint: {h_large.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "        \n",
    "    # Test case 5: Large graph that might cause eigenvalue issues\n",
    "    print(\"\\n5. Testing large graph with potential eigenvalue issues (d=15):\")\n",
    "    d = 15\n",
    "    g_problematic = torch.ones(d, d) * 0.5\n",
    "    g_problematic.fill_diagonal_(0)  # No self-loops\n",
    "    \n",
    "    try:\n",
    "        h_problematic = acyclic_constr(g_problematic, d)\n",
    "        print(f\"   Problematic graph constraint: {h_problematic.item():.6f}\")\n",
    "        print(f\"   Expected: large positive value (many cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR (expected - should fallback to series): {e}\")\n",
    "    \n",
    "    # Test case 6: Check gradients work\n",
    "    print(\"\\n6. Testing gradient computation:\")\n",
    "    d = 4\n",
    "    g_test = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "    g_test.data.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_test = acyclic_constr(g_test, d)\n",
    "        h_test.backward()\n",
    "        print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "        print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient computation: {e}\")\n",
    "    \n",
    "    # Test case 7: Edge cases\n",
    "    print(\"\\n7. Testing edge cases:\")\n",
    "    \n",
    "    # Very small values\n",
    "    d = 3\n",
    "    g_small = torch.ones(d, d) * 1e-10\n",
    "    g_small.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_small = acyclic_constr(g_small, d)\n",
    "        print(f\"   Very small values constraint: {h_small.item():.10f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with small values: {e}\")\n",
    "    \n",
    "    # Very large values (might cause overflow)\n",
    "    g_large_vals = torch.ones(d, d) * 10.0\n",
    "    g_large_vals.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_large_vals = acyclic_constr(g_large_vals, d)\n",
    "        print(f\"   Large values constraint: {h_large_vals.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large values: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"acyclic_constr testing complete\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run the test\n",
    "test_acyclic_constr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.000009\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.013367\n",
      "\n",
      "6b. Testing gradient computation (alternative method):\n",
      "   Constraint value: 0.016230\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.014625\n",
      "\n",
      "6c. Testing gradient computation (simple test):\n",
      "   Constraint value: 0.020845\n",
      "   Gradient computed using autograd.grad: True\n",
      "   Gradient norm: 0.276399\n",
      "   Gradient shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test case 6: Check gradients work\n",
    "print(\"\\n6. Testing gradient computation:\")\n",
    "d = 4\n",
    "\n",
    "# Method 1: Create tensor properly to maintain leaf status\n",
    "g_test = torch.randn(d, d) * 0.1\n",
    "# Zero out diagonal elements without modifying .data\n",
    "mask = ~torch.eye(d, dtype=torch.bool)\n",
    "g_test = g_test * mask.float()\n",
    "g_test.requires_grad_(True)\n",
    "\n",
    "try:\n",
    "    h_test = acyclic_constr(g_test, d)\n",
    "    h_test.backward()\n",
    "    print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "    if g_test.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None - tensor may not be leaf\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 2: Alternative approach using retain_grad()\n",
    "print(\"\\n6b. Testing gradient computation (alternative method):\")\n",
    "g_test2 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "g_test2.data.fill_diagonal_(0)\n",
    "g_test2.retain_grad()  # This ensures gradients are kept even for non-leaf tensors\n",
    "\n",
    "try:\n",
    "    h_test2 = acyclic_constr(g_test2, d)\n",
    "    h_test2.backward()\n",
    "    print(f\"   Constraint value: {h_test2.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test2.grad is not None}\")\n",
    "    if g_test2.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test2.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 3: Test with a simple differentiable operation\n",
    "print(\"\\n6c. Testing gradient computation (simple test):\")\n",
    "g_test3 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "# Create off-diagonal matrix using multiplication\n",
    "off_diag_mask = 1.0 - torch.eye(d)\n",
    "g_test3_masked = g_test3 * off_diag_mask\n",
    "\n",
    "try:\n",
    "    h_test3 = acyclic_constr(g_test3_masked, d)\n",
    "    grad_g3 = torch.autograd.grad(h_test3, g_test3, retain_graph=False)[0]\n",
    "    print(f\"   Constraint value: {h_test3.item():.6f}\")\n",
    "    print(f\"   Gradient computed using autograd.grad: {grad_g3 is not None}\")\n",
    "    if grad_g3 is not None:\n",
    "        print(f\"   Gradient norm: {grad_g3.norm().item():.6f}\")\n",
    "        print(f\"   Gradient shape: {grad_g3.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores z to prob and Bernouilli soft_gmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing scores and bernoulli_soft_gmat functions\n",
      "======================================================================\n",
      "\n",
      "1. Testing simple 2D case with known values:\n",
      "   Z shape: torch.Size([2, 3, 2])\n",
      "   Z:\n",
      "tensor([[[1.0000, 0.5000],\n",
      "         [0.0000, 1.0000],\n",
      "         [1.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 1.0000],\n",
      "         [1.0000, 0.5000],\n",
      "         [1.0000, 0.0000]]])\n",
      "   Scores shape: torch.Size([2, 2])\n",
      "   Scores:\n",
      "tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "   Manual score (0->1): 1.000000\n",
      "   Computed score (0->1): 1.000000\n",
      "   Manual score (1->0): 1.000000\n",
      "   Computed score (1->0): 1.000000\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "\n",
      "2. Testing bernoulli_soft_gmat:\n",
      "   Probabilities shape: torch.Size([2, 2])\n",
      "   Probabilities:\n",
      "tensor([[0.0000, 0.7311],\n",
      "        [0.7311, 0.0000]])\n",
      "   Manual prob (0->1): 0.731059\n",
      "   Computed prob (0->1): 0.731059\n",
      "   Manual prob (1->0): 0.731059\n",
      "   Computed prob (1->0): 0.731059\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "   All probs in [0,1]: True\n",
      "\n",
      "3. Testing with different alpha values:\n",
      "   Alpha = 0.1:\n",
      "     Max score: 0.100000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.524979\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 1.0:\n",
      "     Max score: 1.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.731059\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 5.0:\n",
      "     Max score: 5.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.993307\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 10.0:\n",
      "     Max score: 10.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.999955\n",
      "     Min prob: 0.000000\n",
      "\n",
      "4. Testing gradient flow:\n",
      "   ERROR in gradient flow: 'NoneType' object has no attribute 'shape'\n",
      "\n",
      "5. Testing consistency between scores and probabilities:\n",
      "   Max difference between manual and direct computation: 0.0000000000\n",
      "   Are they approximately equal: True\n",
      "\n",
      "6. Testing edge cases:\n",
      "   Zero embeddings - scores: tensor([0.])\n",
      "   Zero embeddings - probs: tensor([0.0000, 0.5000])\n",
      "   Zero embeddings - all probs should be 0.5: False\n",
      "   Large embeddings - max score: 200.000000\n",
      "   Large embeddings - max prob: 1.000000\n",
      "   Large embeddings - are probs valid: True\n",
      "\n",
      "7. Testing batched operation:\n",
      "   Batch scores shape: torch.Size([2, 3, 3])\n",
      "   Expected shape: (2, 3, 3)\n",
      "   Shapes match: True\n",
      "\n",
      "======================================================================\n",
      "scores and bernoulli_soft_gmat testing complete\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111888/3757974893.py:113: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n"
     ]
    }
   ],
   "source": [
    "def test_scores_and_bernoulli_soft_gmat():\n",
    "    \"\"\"\n",
    "    Test the scores and bernoulli_soft_gmat functions based on the mathematical formulation\n",
    "    from section 4.2 of the paper.\n",
    "    \n",
    "    Mathematical background:\n",
    "    - Z = [U, V] where U, V ∈ R^(k×d)\n",
    "    - scores should compute α * u_i^T v_j for all i,j\n",
    "    - bernoulli_soft_gmat should compute σ_α(u_i^T v_j) = 1/(1 + exp(-α * u_i^T v_j))\n",
    "    - Diagonal elements should be 0 (no self-loops)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Testing scores and bernoulli_soft_gmat functions\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test case 1: Simple 2D case with known values\n",
    "    print(\"\\n1. Testing simple 2D case with known values:\")\n",
    "    d, k = 2, 3\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Create simple Z = [U, V] with known values\n",
    "    U = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]], dtype=torch.float32)  # (k, d)\n",
    "    V = torch.tensor([[0.5, 1.0], [1.0, 0.5], [0.0, 0.0]], dtype=torch.float32)  # (k, d)\n",
    "    Z = torch.stack([U.T, V.T], dim=-1)  # (d, k, 2)\n",
    "    \n",
    "    print(f\"   Z shape: {Z.shape}\")\n",
    "    print(f\"   Z:\\n{Z}\")\n",
    "    \n",
    "    # Test scores function\n",
    "    try:\n",
    "        scores_result = scores(Z, alpha)\n",
    "        print(f\"   Scores shape: {scores_result.shape}\")\n",
    "        print(f\"   Scores:\\n{scores_result}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        u1, v1 = Z[0, :, 0], Z[0, :, 1]  # u1, v1 for node 0\n",
    "        u2, v2 = Z[1, :, 0], Z[1, :, 1]  # u2, v2 for node 1\n",
    "        \n",
    "        manual_score_01 = alpha * torch.dot(u1, v2)\n",
    "        manual_score_10 = alpha * torch.dot(u2, v1)\n",
    "        \n",
    "        print(f\"   Manual score (0->1): {manual_score_01.item():.6f}\")\n",
    "        print(f\"   Computed score (0->1): {scores_result[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual score (1->0): {manual_score_10.item():.6f}\")\n",
    "        print(f\"   Computed score (1->0): {scores_result[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(scores_result)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in scores: {e}\")\n",
    "    \n",
    "    # Test bernoulli_soft_gmat function\n",
    "    print(\"\\n2. Testing bernoulli_soft_gmat:\")\n",
    "    hparams = {\"alpha\": alpha}\n",
    "    \n",
    "    try:\n",
    "        probs = bernoulli_soft_gmat(Z, hparams)\n",
    "        print(f\"   Probabilities shape: {probs.shape}\")\n",
    "        print(f\"   Probabilities:\\n{probs}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        manual_prob_01 = torch.sigmoid(manual_score_01)\n",
    "        manual_prob_10 = torch.sigmoid(manual_score_10)\n",
    "        \n",
    "        print(f\"   Manual prob (0->1): {manual_prob_01.item():.6f}\")\n",
    "        print(f\"   Computed prob (0->1): {probs[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual prob (1->0): {manual_prob_10.item():.6f}\")\n",
    "        print(f\"   Computed prob (1->0): {probs[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(probs)}\")\n",
    "        \n",
    "        # Check probabilities are in [0, 1]\n",
    "        print(f\"   All probs in [0,1]: {torch.all((probs >= 0) & (probs <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in bernoulli_soft_gmat: {e}\")\n",
    "    \n",
    "    # Test case 3: Test with different alpha values\n",
    "    print(\"\\n3. Testing with different alpha values:\")\n",
    "    alphas = [0.1, 1.0, 5.0, 10.0]\n",
    "    \n",
    "    for alpha_test in alphas:\n",
    "        hparams_test = {\"alpha\": alpha_test}\n",
    "        try:\n",
    "            scores_test = scores(Z, alpha_test)\n",
    "            probs_test = bernoulli_soft_gmat(Z, hparams_test)\n",
    "            \n",
    "            print(f\"   Alpha = {alpha_test}:\")\n",
    "            print(f\"     Max score: {scores_test.max().item():.6f}\")\n",
    "            print(f\"     Min score: {scores_test.min().item():.6f}\")\n",
    "            print(f\"     Max prob: {probs_test.max().item():.6f}\")\n",
    "            print(f\"     Min prob: {probs_test.min().item():.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR with alpha {alpha_test}: {e}\")\n",
    "    \n",
    "    # Test case 4: Test gradient flow\n",
    "    print(\"\\n4. Testing gradient flow:\")\n",
    "    d, k = 3, 4\n",
    "    Z_grad = torch.randn(d, k, 2, requires_grad=True) * 0.5\n",
    "    hparams_grad = {\"alpha\": 2.0}\n",
    "    \n",
    "    try:\n",
    "        scores_grad = scores(Z_grad, hparams_grad[\"alpha\"])\n",
    "        probs_grad = bernoulli_soft_gmat(Z_grad, hparams_grad)\n",
    "        \n",
    "        # Compute some loss and backpropagate\n",
    "        loss = torch.sum(probs_grad ** 2)\n",
    "        loss.backward()\n",
    "        \n",
    "        print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n",
    "        print(f\"   Z gradient norm: {Z_grad.grad.norm().item():.6f}\")\n",
    "        print(f\"   Loss value: {loss.item():.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient flow: {e}\")\n",
    "    \n",
    "    # Test case 5: Test consistency between scores and probabilities\n",
    "    print(\"\\n5. Testing consistency between scores and probabilities:\")\n",
    "    d, k = 4, 3\n",
    "    Z_test = torch.randn(d, k, 2) * 0.3\n",
    "    alpha_test = 1.5\n",
    "    hparams_test = {\"alpha\": alpha_test}\n",
    "    \n",
    "    try:\n",
    "        scores_manual = scores(Z_test, alpha_test)\n",
    "        probs_from_scores = torch.sigmoid(scores_manual)\n",
    "        \n",
    "        # Zero out diagonal\n",
    "        diag_mask = 1.0 - torch.eye(d)\n",
    "        probs_from_scores = probs_from_scores * diag_mask\n",
    "        \n",
    "        probs_direct = bernoulli_soft_gmat(Z_test, hparams_test)\n",
    "        \n",
    "        # Check if they match\n",
    "        max_diff = torch.max(torch.abs(probs_from_scores - probs_direct))\n",
    "        print(f\"   Max difference between manual and direct computation: {max_diff.item():.10f}\")\n",
    "        print(f\"   Are they approximately equal: {torch.allclose(probs_from_scores, probs_direct, atol=1e-6)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in consistency test: {e}\")\n",
    "    \n",
    "    # Test case 6: Test edge cases\n",
    "    print(\"\\n6. Testing edge cases:\")\n",
    "    \n",
    "    # Zero embeddings\n",
    "    Z_zero = torch.zeros(3, 2, 2)\n",
    "    hparams_zero = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_zero = scores(Z_zero, 1.0)\n",
    "        probs_zero = bernoulli_soft_gmat(Z_zero, hparams_zero)\n",
    "        \n",
    "        print(f\"   Zero embeddings - scores: {scores_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - probs: {probs_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - all probs should be 0.5: {torch.allclose(probs_zero, torch.ones_like(probs_zero) * 0.5)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with zero embeddings: {e}\")\n",
    "    \n",
    "    # Very large embeddings (test numerical stability)\n",
    "    Z_large = torch.ones(2, 2, 2) * 10.0\n",
    "    hparams_large = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_large = scores(Z_large, 1.0)\n",
    "        probs_large = bernoulli_soft_gmat(Z_large, hparams_large)\n",
    "        \n",
    "        print(f\"   Large embeddings - max score: {scores_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - max prob: {probs_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - are probs valid: {torch.all((probs_large >= 0) & (probs_large <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large embeddings: {e}\")\n",
    "    \n",
    "    # Test case 7: Test batched operation\n",
    "    print(\"\\n7. Testing batched operation:\")\n",
    "    batch_size = 2\n",
    "    d, k = 3, 2\n",
    "    Z_batch = torch.randn(batch_size, d, k, 2) * 0.5\n",
    "    hparams_batch = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_batch = scores(Z_batch, 1.0)\n",
    "        # Note: bernoulli_soft_gmat might not support batching directly\n",
    "        print(f\"   Batch scores shape: {scores_batch.shape}\")\n",
    "        print(f\"   Expected shape: ({batch_size}, {d}, {d})\")\n",
    "        print(f\"   Shapes match: {scores_batch.shape == (batch_size, d, d)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in batch operation: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"scores and bernoulli_soft_gmat testing complete\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run the test\n",
    "test_scores_and_bernoulli_soft_gmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running tests for acyclic_constr ---\n",
      "\n",
      "--- Running tests for Graph Generation ---\n",
      "Large acyclic graph (d=12) H(G): 0.000000\n",
      "Graph with self-loop H(G): 1.370371\n",
      "Acyclic graph H(G): 0.000000\n",
      "Graph with 2-cycle H(G): 0.666667\n",
      "\n",
      "Calculated soft G-matrix:\n",
      "tensor([[0.0000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.0000]])\n",
      "Expected soft G-matrix:\n",
      "tensor([[0.5000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.5000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.5000]])\n",
      "\n",
      "Calculated scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n",
      "Expected scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standalone test cell for debug_notebook.ipynb\n",
    "#\n",
    "# To use this, you would typically have the functions available \n",
    "# in the same notebook or imported from your 'models/dibs.py' script.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import unittest\n",
    "import logging\n",
    "\n",
    "# --- Setup basic logger ---\n",
    "# This is to prevent errors if log.warning is called in acyclic_constr\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# --- Functions to be tested ---\n",
    "# Pasted here for stand-alone execution.\n",
    "# In a real scenario, you would import these from your scripts.\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    # Ensure g is a floating point tensor for matrix operations\n",
    "    g = g.float()\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    # Using matrix_power for d <= 10 as it's generally stable for smaller matrices\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    # For larger d, eigenvalues are more efficient but can be numerically unstable\n",
    "    try:\n",
    "        # Eigenvalue decomposition is faster for large d\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        # The constraint is based on the sum of the d-th power of eigenvalues\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except torch.linalg.LinAlgError:\n",
    "        # Fallback to series expansion if eigenvalue computation fails\n",
    "        # This is a less precise but more stable approximation\n",
    "        log.warning(f\"Eigenvalue computation failed for d={d}. Falling back to series expansion.\")\n",
    "        trace = torch.tensor(0.0, device=g.device, dtype=g.dtype)\n",
    "        p = eye.clone() # Start with identity matrix for power calculation\n",
    "        for k in range(1, min(d + 1, 20)): # Limit to 20 terms for practical purposes\n",
    "            p = p @ m\n",
    "            trace += torch.trace(p) / k\n",
    "        return trace\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    \"\"\"Calculates the raw edge scores from latent embeddings.\"\"\"\n",
    "    # z has shape [d, k, 2]\n",
    "    # u and v have shape [d, k]\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    \n",
    "    # einsum performs batch matrix multiplication of u and v.T\n",
    "    # 'ik,jk->ij' means: sum over k for each i and j\n",
    "    raw_scores = alpha * torch.einsum('ik,jk->ij', u, v)\n",
    "    \n",
    "    # Ensure no self-loops by masking the diagonal\n",
    "    d = z.shape[0]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    \n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: dict) -> torch.Tensor:\n",
    "    \"\"\"Generates a soft adjacency matrix using a Bernoulli parameterization.\"\"\"\n",
    "    # Get probabilities by applying a sigmoid to the raw scores\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    \n",
    "    # The scores function already handles the diagonal masking, but as a safeguard:\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    \n",
    "    return probs * diag_mask\n",
    "\n",
    "\n",
    "# --- Test Cases ---\n",
    "\n",
    "class TestAcyclicConstraint(unittest.TestCase):\n",
    "\n",
    "    def test_strictly_acyclic_graph(self):\n",
    "        \"\"\"Tests a graph with no cycles (a Directed Acyclic Graph).\"\"\"\n",
    "        g_acyclic = torch.tensor([[0., 1., 1.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_acyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"Acyclic graph H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=5, msg=\"Acyclic graph should have H(G) = 0\")\n",
    "\n",
    "    def test_self_loop_cycle(self):\n",
    "        \"\"\"Tests a graph with a self-loop (the simplest cycle).\"\"\"\n",
    "        g_cyclic = torch.tensor([[1., 1., 0.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with self-loop H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "\n",
    "    def test_two_node_cycle(self):\n",
    "        \"\"\"Tests a graph with a 2-cycle (A -> B, B -> A).\"\"\"\n",
    "        g_cyclic = torch.tensor([[0., 1., 0.], [1., 0., 0.], [0., 1., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with 2-cycle H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "        \n",
    "    def test_large_graph_eigenvalue_path(self):\n",
    "        \"\"\"Tests the eigenvalue code path with a larger (d=12) acyclic graph.\"\"\"\n",
    "        d = 12\n",
    "        g_large_acyclic = torch.triu(torch.ones(d, d), diagonal=1)\n",
    "        h_val = acyclic_constr(g_large_acyclic, d)\n",
    "        print(f\"Large acyclic graph (d=12) H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=4, msg=\"Large acyclic graph should have H(G) near 0\")\n",
    "\n",
    "\n",
    "class TestGraphGeneration(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up common variables for the tests.\"\"\"\n",
    "        self.d = 3  # Number of nodes\n",
    "        self.k = 2  # Latent dimension\n",
    "        # Latent variable Z = [U, V]\n",
    "        self.z = torch.arange(self.d * self.k * 2, dtype=torch.float32).view(self.d, self.k, 2)\n",
    "        # z will be:\n",
    "        # [[[ 0,  1], [ 2,  3]],\n",
    "        #  [[ 4,  5], [ 6,  7]],\n",
    "        #  [[ 8,  9], [10, 11]]]\n",
    "        self.alpha = 0.5\n",
    "        self.hparams = {\"alpha\": self.alpha}\n",
    "\n",
    "    def test_scores_calculation(self):\n",
    "        \"\"\"Tests the bilinear score calculation G_ij = alpha * u_i^T v_j.\"\"\"\n",
    "        u = self.z[..., 0] # [[[0, 2], [4, 6], [8, 10]]]\n",
    "        v = self.z[..., 1] # [[[1, 3], [5, 7], [9, 11]]]\n",
    "        \n",
    "        # Manually calculate expected scores\n",
    "        expected_scores = self.alpha * torch.matmul(u, v.T)\n",
    "        # Set diagonal to zero\n",
    "        expected_scores.fill_diagonal_(0)\n",
    "        \n",
    "        # Get scores from function\n",
    "        s = scores(self.z, self.alpha)\n",
    "        print(f\"\\nCalculated scores:\\n{s}\")\n",
    "        print(f\"Expected scores:\\n{expected_scores}\")\n",
    "        \n",
    "        self.assertTrue(torch.allclose(s, expected_scores), \"Scores do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(s) == 0), \"Diagonal of scores matrix should be zero.\")\n",
    "\n",
    "    def test_bernoulli_soft_gmat(self):\n",
    "        \"\"\"Tests the sigmoid transformation of scores to get probabilities.\"\"\"\n",
    "        # Calculate scores first\n",
    "        s = scores(self.z, self.alpha)\n",
    "        \n",
    "        # Manually calculate expected probabilities\n",
    "        expected_probs = torch.sigmoid(s)\n",
    "        \n",
    "        # Get probabilities from function\n",
    "        g_soft = bernoulli_soft_gmat(self.z, self.hparams)\n",
    "        print(f\"\\nCalculated soft G-matrix:\\n{g_soft}\")\n",
    "        print(f\"Expected soft G-matrix:\\n{expected_probs}\")\n",
    "        expected_probs.fill_diagonal_(0)\n",
    "        self.assertTrue(torch.allclose(g_soft, expected_probs), \"Soft G-matrix probabilities do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(g_soft) == 0), \"Diagonal of soft G-matrix should be zero.\")\n",
    "\n",
    "\n",
    "# --- Running the tests ---\n",
    "# This allows running the tests directly from the cell.\n",
    "suite = unittest.TestSuite()\n",
    "print(\"--- Running tests for acyclic_constr ---\")\n",
    "suite.addTest(unittest.makeSuite(TestAcyclicConstraint))\n",
    "print(\"\\n--- Running tests for Graph Generation ---\")\n",
    "suite.addTest(unittest.makeSuite(TestGraphGeneration))\n",
    "\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing Simple Linear Model Log Joint\n",
      "======================================================================\n",
      "Data: 100 samples, 3 features\n",
      "True theta: tensor([ 2.0000, -1.5000,  0.8000])\n",
      "\n",
      "1. Testing log joint evaluation:\n",
      "   Log joint at true theta: 82.612999\n",
      "   Log likelihood: 88.310509\n",
      "   Log prior: -5.697507\n",
      "   Sum: 82.612999\n",
      "\n",
      "2. Testing gradient computation:\n",
      "   Gradient at zero: tensor([ 23538.4922, -11948.1221,   6987.8818])\n",
      "   Gradient norm: 27306.568359\n",
      "   Gradient direction vs true theta direction:\n",
      "   Normalized gradient: tensor([ 0.8620, -0.4376,  0.2559])\n",
      "   Normalized true theta: tensor([ 0.7619, -0.5715,  0.3048])\n",
      "\n",
      "3. Testing gradient ascent optimization:\n",
      "   Iteration 0: log_joint = -35074.003906, theta = tensor([ 0.2354, -0.1195,  0.0699])\n",
      "   Gradient: tensor([ 23538.4922, -11948.1221,   6987.8818])\n",
      "   Iteration 1: log_joint = -28022.892578, theta = tensor([ 0.4431, -0.2297,  0.1329])\n",
      "   Gradient: tensor([ 20766.8730, -11024.6943,   6300.6553])\n",
      "   Iteration 2: log_joint = -22418.244141, theta = tensor([ 0.6263, -0.3314,  0.1898])\n",
      "   Gradient: tensor([ 18322.4785, -10165.9902,   5689.4727])\n",
      "   Iteration 3: log_joint = -17957.472656, theta = tensor([ 0.7879, -0.4251,  0.2412])\n",
      "   Gradient: tensor([16166.5029, -9368.5000,  5145.1680])\n",
      "   Iteration 4: log_joint = -14402.359375, theta = tensor([ 0.9306, -0.5114,  0.2878])\n",
      "   Gradient: tensor([14264.7930, -8628.7383,  4659.7324])\n",
      "   Iteration 5: log_joint = -11565.159180, theta = tensor([ 1.0565, -0.5908,  0.3301])\n",
      "   Gradient: tensor([12587.2383, -7943.2764,  4226.1694])\n",
      "   Iteration 6: log_joint = -9297.748047, theta = tensor([ 1.1675, -0.6639,  0.3685])\n",
      "   Gradient: tensor([11107.3193, -7308.7583,  3838.3584])\n",
      "   Iteration 7: log_joint = -7483.137695, theta = tensor([ 1.2656, -0.7311,  0.4034])\n",
      "   Gradient: tensor([ 9801.6641, -6721.9448,  3490.9490])\n",
      "   Iteration 8: log_joint = -6028.820801, theta = tensor([ 1.3521, -0.7929,  0.4352])\n",
      "   Gradient: tensor([ 8649.6797, -6179.7085,  3179.2551])\n",
      "   Iteration 9: log_joint = -4861.568359, theta = tensor([ 1.4284, -0.8497,  0.4642])\n",
      "   Gradient: tensor([ 7633.2212, -5679.0532,  2899.1741])\n",
      "   Iteration 10: log_joint = -3923.342041, theta = tensor([ 1.4957, -0.9019,  0.4906])\n",
      "   Gradient: tensor([ 6736.2866, -5217.1299,  2647.1099])\n",
      "   Iteration 11: log_joint = -3168.085693, theta = tensor([ 1.5552, -0.9498,  0.5148])\n",
      "   Gradient: tensor([ 5944.7842, -4791.2285,  2419.9055])\n",
      "   Iteration 12: log_joint = -2559.208252, theta = tensor([ 1.6077, -0.9938,  0.5370])\n",
      "   Gradient: tensor([ 5246.2812, -4398.7808,  2214.7932])\n",
      "   Iteration 13: log_joint = -2067.600830, theta = tensor([ 1.6540, -1.0341,  0.5573])\n",
      "   Gradient: tensor([ 4629.8174, -4037.3679,  2029.3402])\n",
      "   Iteration 14: log_joint = -1670.077637, theta = tensor([ 1.6948, -1.0712,  0.5759])\n",
      "   Gradient: tensor([ 4085.7397, -3704.7136,  1861.4087])\n",
      "   Iteration 15: log_joint = -1348.143799, theta = tensor([ 1.7309, -1.1052,  0.5930])\n",
      "   Gradient: tensor([ 3605.5222, -3398.6787,  1709.1141])\n",
      "   Iteration 16: log_joint = -1087.030518, theta = tensor([ 1.7627, -1.1363,  0.6087])\n",
      "   Gradient: tensor([ 3181.6558, -3117.2629,  1570.7975])\n",
      "   Iteration 17: log_joint = -874.925354, theta = tensor([ 1.7908, -1.1649,  0.6231])\n",
      "   Gradient: tensor([ 2807.5151, -2858.5947,  1444.9961])\n",
      "   Iteration 18: log_joint = -702.369080, theta = tensor([ 1.8155, -1.1911,  0.6364])\n",
      "   Gradient: tensor([ 2477.2573, -2620.9285,  1330.4181])\n",
      "   Iteration 19: log_joint = -561.775330, theta = tensor([ 1.8374, -1.2152,  0.6487])\n",
      "   Gradient: tensor([ 2185.7295, -2402.6379,  1225.9182])\n",
      "   Iteration 20: log_joint = -447.050873, theta = tensor([ 1.8567, -1.2372,  0.6600])\n",
      "   Gradient: tensor([ 1928.3839, -2202.2100,  1130.4863])\n",
      "   Iteration 21: log_joint = -353.296265, theta = tensor([ 1.8737, -1.2574,  0.6704])\n",
      "   Gradient: tensor([ 1701.2092, -2018.2397,  1043.2249])\n",
      "   Iteration 22: log_joint = -276.565094, theta = tensor([ 1.8887, -1.2759,  0.6801])\n",
      "   Gradient: tensor([ 1500.6663, -1849.4252,   963.3374])\n",
      "   Iteration 23: log_joint = -213.674286, theta = tensor([ 1.9019, -1.2928,  0.6890])\n",
      "   Gradient: tensor([ 1323.6348, -1694.5560,   890.1157])\n",
      "   Iteration 24: log_joint = -162.052536, theta = tensor([ 1.9136, -1.3083,  0.6972])\n",
      "   Gradient: tensor([ 1167.3578, -1552.5151,   822.9292])\n",
      "   Iteration 25: log_joint = -119.620346, theta = tensor([ 1.9239, -1.3226,  0.7048])\n",
      "   Gradient: tensor([ 1029.4047, -1422.2711,   761.2154])\n",
      "   Iteration 26: log_joint = -84.692390, theta = tensor([ 1.9330, -1.3356,  0.7119])\n",
      "   Gradient: tensor([  907.6296, -1302.8639,   704.4721])\n",
      "   Iteration 27: log_joint = -55.901947, theta = tensor([ 1.9410, -1.3475,  0.7184])\n",
      "   Gradient: tensor([  800.1381, -1193.4159,   652.2498])\n",
      "   Iteration 28: log_joint = -32.138004, theta = tensor([ 1.9480, -1.3584,  0.7244])\n",
      "   Gradient: tensor([  705.2574, -1093.1107,   604.1454])\n",
      "   Iteration 29: log_joint = -12.496960, theta = tensor([ 1.9542, -1.3685,  0.7300])\n",
      "   Gradient: tensor([  621.5128, -1001.1994,   559.7979])\n",
      "   Iteration 30: log_joint = 3.757808, theta = tensor([ 1.9597, -1.3776,  0.7352])\n",
      "   Gradient: tensor([ 547.6007, -916.9910,  518.8814])\n",
      "   Iteration 31: log_joint = 17.227282, theta = tensor([ 1.9645, -1.3860,  0.7400])\n",
      "   Gradient: tensor([ 482.3711, -839.8489,  481.1031])\n",
      "   Iteration 32: log_joint = 28.402710, theta = tensor([ 1.9688, -1.3937,  0.7445])\n",
      "   Gradient: tensor([ 424.8082, -769.1883,  446.1987])\n",
      "   Iteration 33: log_joint = 37.685925, theta = tensor([ 1.9725, -1.4008,  0.7486])\n",
      "   Gradient: tensor([ 374.0159, -704.4716,  413.9295])\n",
      "   Iteration 34: log_joint = 45.406612, theta = tensor([ 1.9758, -1.4072,  0.7525])\n",
      "   Gradient: tensor([ 329.2006, -645.2025,  384.0793])\n",
      "   Iteration 35: log_joint = 51.835159, theta = tensor([ 1.9787, -1.4131,  0.7560])\n",
      "   Gradient: tensor([ 289.6655, -590.9258,  356.4523])\n",
      "   Iteration 36: log_joint = 57.193794, theta = tensor([ 1.9813, -1.4185,  0.7593])\n",
      "   Gradient: tensor([ 254.7909, -541.2266,  330.8698])\n",
      "   Iteration 37: log_joint = 61.665401, theta = tensor([ 1.9835, -1.4235,  0.7624])\n",
      "   Gradient: tensor([ 224.0322, -495.7188,  307.1702])\n",
      "   Iteration 38: log_joint = 65.400841, theta = tensor([ 1.9855, -1.4280,  0.7653])\n",
      "   Gradient: tensor([ 196.9093, -454.0513,  285.2062])\n",
      "   Iteration 39: log_joint = 68.524460, theta = tensor([ 1.9872, -1.4322,  0.7679])\n",
      "   Gradient: tensor([ 172.9937, -415.9003,  264.8430])\n",
      "   Iteration 40: log_joint = 71.138992, theta = tensor([ 1.9887, -1.4360,  0.7704])\n",
      "   Gradient: tensor([ 151.9123, -380.9724,  245.9576])\n",
      "   Iteration 41: log_joint = 73.329529, theta = tensor([ 1.9901, -1.4395,  0.7727])\n",
      "   Gradient: tensor([ 133.3320, -348.9935,  228.4375])\n",
      "   Iteration 42: log_joint = 75.166512, theta = tensor([ 1.9912, -1.4427,  0.7748])\n",
      "   Gradient: tensor([ 116.9594, -319.7152,  212.1795])\n",
      "   Iteration 43: log_joint = 76.708427, theta = tensor([ 1.9923, -1.4456,  0.7768])\n",
      "   Gradient: tensor([ 102.5367, -292.9095,  197.0891])\n",
      "   Iteration 44: log_joint = 78.003700, theta = tensor([ 1.9931, -1.4483,  0.7786])\n",
      "   Gradient: tensor([  89.8355, -268.3678,  183.0794])\n",
      "   Iteration 45: log_joint = 79.092690, theta = tensor([ 1.9939, -1.4508,  0.7803])\n",
      "   Gradient: tensor([  78.6512, -245.8990,  170.0699])\n",
      "   Iteration 46: log_joint = 80.009071, theta = tensor([ 1.9946, -1.4530,  0.7819])\n",
      "   Gradient: tensor([  68.8074, -225.3257,  157.9884])\n",
      "   Iteration 47: log_joint = 80.780655, theta = tensor([ 1.9952, -1.4551,  0.7833])\n",
      "   Gradient: tensor([  60.1463, -206.4888,  146.7665])\n",
      "   Iteration 48: log_joint = 81.430870, theta = tensor([ 1.9958, -1.4570,  0.7847])\n",
      "   Gradient: tensor([  52.5286, -189.2406,  136.3419])\n",
      "   Iteration 49: log_joint = 81.979164, theta = tensor([ 1.9962, -1.4587,  0.7860])\n",
      "   Gradient: tensor([  45.8318, -173.4467,  126.6569])\n",
      "   Iteration 50: log_joint = 82.441841, theta = tensor([ 1.9966, -1.4603,  0.7871])\n",
      "   Gradient: tensor([  39.9447, -158.9842,  117.6582])\n",
      "   Iteration 51: log_joint = 82.832504, theta = tensor([ 1.9970, -1.4618,  0.7882])\n",
      "   Gradient: tensor([  34.7736, -145.7400,  109.2961])\n",
      "   Iteration 52: log_joint = 83.162582, theta = tensor([ 1.9973, -1.4631,  0.7892])\n",
      "   Gradient: tensor([  30.2342, -133.6105,  101.5256])\n",
      "   Iteration 53: log_joint = 83.441650, theta = tensor([ 1.9975, -1.4643,  0.7902])\n",
      "   Gradient: tensor([  26.2510, -122.5020,   94.3049])\n",
      "   Iteration 54: log_joint = 83.677689, theta = tensor([ 1.9977, -1.4654,  0.7911])\n",
      "   Gradient: tensor([  22.7581, -112.3277,   87.5939])\n",
      "   Iteration 55: log_joint = 83.877457, theta = tensor([ 1.9979, -1.4665,  0.7919])\n",
      "   Gradient: tensor([  19.6966, -103.0075,   81.3574])\n",
      "   Iteration 56: log_joint = 84.046616, theta = tensor([ 1.9981, -1.4674,  0.7926])\n",
      "   Gradient: tensor([ 17.0156, -94.4697,  75.5615])\n",
      "   Iteration 57: log_joint = 84.189926, theta = tensor([ 1.9983, -1.4683,  0.7933])\n",
      "   Gradient: tensor([ 14.6697, -86.6479,  70.1752])\n",
      "   Iteration 58: log_joint = 84.311394, theta = tensor([ 1.9984, -1.4691,  0.7940])\n",
      "   Gradient: tensor([ 12.6173, -79.4813,  65.1697])\n",
      "   Iteration 59: log_joint = 84.414391, theta = tensor([ 1.9985, -1.4698,  0.7946])\n",
      "   Gradient: tensor([ 10.8251, -72.9158,  60.5175])\n",
      "   Iteration 60: log_joint = 84.501747, theta = tensor([ 1.9986, -1.4705,  0.7952])\n",
      "   Gradient: tensor([  9.2609, -66.8985,  56.1945])\n",
      "   Iteration 61: log_joint = 84.575920, theta = tensor([ 1.9987, -1.4711,  0.7957])\n",
      "   Gradient: tensor([  7.8966, -61.3846,  52.1776])\n",
      "   Iteration 62: log_joint = 84.638870, theta = tensor([ 1.9987, -1.4717,  0.7962])\n",
      "   Gradient: tensor([  6.7095, -56.3314,  48.4444])\n",
      "   Iteration 63: log_joint = 84.692352, theta = tensor([ 1.9988, -1.4722,  0.7966])\n",
      "   Gradient: tensor([  5.6755, -51.6993,  44.9757])\n",
      "   Iteration 64: log_joint = 84.737747, theta = tensor([ 1.9988, -1.4726,  0.7970])\n",
      "   Gradient: tensor([  4.7783, -47.4531,  41.7524])\n",
      "   Iteration 65: log_joint = 84.776398, theta = tensor([ 1.9989, -1.4731,  0.7974])\n",
      "   Gradient: tensor([  4.0000, -43.5600,  38.7581])\n",
      "   Iteration 66: log_joint = 84.809212, theta = tensor([ 1.9989, -1.4735,  0.7978])\n",
      "   Gradient: tensor([  3.3254, -39.9909,  35.9759])\n",
      "   Iteration 67: log_joint = 84.837128, theta = tensor([ 1.9989, -1.4738,  0.7981])\n",
      "   Gradient: tensor([  2.7430, -36.7181,  33.3914])\n",
      "   Iteration 68: log_joint = 84.860863, theta = tensor([ 1.9990, -1.4742,  0.7984])\n",
      "   Gradient: tensor([  2.2409, -33.7164,  30.9907])\n",
      "   Iteration 69: log_joint = 84.881096, theta = tensor([ 1.9990, -1.4745,  0.7987])\n",
      "   Gradient: tensor([  1.8088, -30.9641,  28.7607])\n",
      "   Iteration 70: log_joint = 84.898277, theta = tensor([ 1.9990, -1.4748,  0.7990])\n",
      "   Gradient: tensor([  1.4376, -28.4398,  26.6893])\n",
      "   Iteration 71: log_joint = 84.912941, theta = tensor([ 1.9990, -1.4750,  0.7992])\n",
      "   Gradient: tensor([  1.1197, -26.1233,  24.7655])\n",
      "   Iteration 72: log_joint = 84.925400, theta = tensor([ 1.9990, -1.4753,  0.7994])\n",
      "   Gradient: tensor([  0.8490, -23.9986,  22.9790])\n",
      "   Iteration 73: log_joint = 84.936028, theta = tensor([ 1.9990, -1.4755,  0.7997])\n",
      "   Gradient: tensor([  0.6188, -22.0490,  21.3199])\n",
      "   Iteration 74: log_joint = 84.945068, theta = tensor([ 1.9990, -1.4757,  0.7999])\n",
      "   Gradient: tensor([  0.4242, -20.2595,  19.7796])\n",
      "   Iteration 75: log_joint = 84.952789, theta = tensor([ 1.9990, -1.4759,  0.8000])\n",
      "   Gradient: tensor([  0.2592, -18.6181,  18.3491])\n",
      "   Iteration 76: log_joint = 84.959335, theta = tensor([ 1.9990, -1.4761,  0.8002])\n",
      "   Gradient: tensor([  0.1217, -17.1107,  17.0213])\n",
      "   Iteration 77: log_joint = 84.964943, theta = tensor([ 1.9990, -1.4762,  0.8004])\n",
      "   Gradient: tensor([ 7.4088e-03, -1.5728e+01,  1.5788e+01])\n",
      "   Iteration 78: log_joint = 84.969749, theta = tensor([ 1.9990, -1.4764,  0.8005])\n",
      "   Gradient: tensor([ -0.0870, -14.4580,  14.6435])\n",
      "   Iteration 79: log_joint = 84.973816, theta = tensor([ 1.9990, -1.4765,  0.8007])\n",
      "   Gradient: tensor([ -0.1646, -13.2920,  13.5811])\n",
      "   Iteration 80: log_joint = 84.977272, theta = tensor([ 1.9990, -1.4766,  0.8008])\n",
      "   Gradient: tensor([ -0.2264, -12.2211,  12.5946])\n",
      "   Iteration 81: log_joint = 84.980263, theta = tensor([ 1.9990, -1.4767,  0.8009])\n",
      "   Gradient: tensor([ -0.2764, -11.2379,  11.6793])\n",
      "   Iteration 82: log_joint = 84.982788, theta = tensor([ 1.9990, -1.4768,  0.8010])\n",
      "   Gradient: tensor([ -0.3153, -10.3343,  10.8305])\n",
      "   Iteration 83: log_joint = 84.984940, theta = tensor([ 1.9990, -1.4769,  0.8011])\n",
      "   Gradient: tensor([-0.3457, -9.5046, 10.0421])\n",
      "   Iteration 84: log_joint = 84.986755, theta = tensor([ 1.9990, -1.4770,  0.8012])\n",
      "   Gradient: tensor([-0.3669, -8.7430,  9.3108])\n",
      "   Iteration 85: log_joint = 84.988327, theta = tensor([ 1.9990, -1.4771,  0.8013])\n",
      "   Gradient: tensor([-0.3812, -8.0433,  8.6320])\n",
      "   Iteration 86: log_joint = 84.989670, theta = tensor([ 1.9990, -1.4772,  0.8014])\n",
      "   Gradient: tensor([-0.3905, -7.3992,  8.0028])\n",
      "   Iteration 87: log_joint = 84.990799, theta = tensor([ 1.9990, -1.4772,  0.8014])\n",
      "   Gradient: tensor([-0.3947, -6.8077,  7.4185])\n",
      "   Iteration 88: log_joint = 84.991791, theta = tensor([ 1.9990, -1.4773,  0.8015])\n",
      "   Gradient: tensor([-0.3950, -6.2644,  6.8767])\n",
      "   Iteration 89: log_joint = 84.992630, theta = tensor([ 1.9990, -1.4774,  0.8016])\n",
      "   Gradient: tensor([-0.3927, -5.7658,  6.3738])\n",
      "   Iteration 90: log_joint = 84.993324, theta = tensor([ 1.9990, -1.4774,  0.8016])\n",
      "   Gradient: tensor([-0.3876, -5.3063,  5.9078])\n",
      "   Iteration 91: log_joint = 84.993927, theta = tensor([ 1.9990, -1.4775,  0.8017])\n",
      "   Gradient: tensor([-0.3791, -4.8841,  5.4756])\n",
      "   Iteration 92: log_joint = 84.994423, theta = tensor([ 1.9990, -1.4775,  0.8017])\n",
      "   Gradient: tensor([-0.3698, -4.4957,  5.0744])\n",
      "   Iteration 93: log_joint = 84.994911, theta = tensor([ 1.9990, -1.4775,  0.8018])\n",
      "   Gradient: tensor([-0.3591, -4.1390,  4.7028])\n",
      "   Iteration 94: log_joint = 84.995300, theta = tensor([ 1.9990, -1.4776,  0.8018])\n",
      "   Gradient: tensor([-0.3475, -3.8111,  4.3578])\n",
      "   Iteration 95: log_joint = 84.995613, theta = tensor([ 1.9990, -1.4776,  0.8019])\n",
      "   Gradient: tensor([-0.3351, -3.5090,  4.0383])\n",
      "   Iteration 96: log_joint = 84.995903, theta = tensor([ 1.9990, -1.4776,  0.8019])\n",
      "   Gradient: tensor([-0.3226, -3.2319,  3.7415])\n",
      "   Iteration 97: log_joint = 84.996132, theta = tensor([ 1.9990, -1.4777,  0.8019])\n",
      "   Gradient: tensor([-0.3097, -2.9765,  3.4666])\n",
      "   Iteration 98: log_joint = 84.996323, theta = tensor([ 1.9990, -1.4777,  0.8020])\n",
      "   Gradient: tensor([-0.2963, -2.7408,  3.2116])\n",
      "   Iteration 99: log_joint = 84.996483, theta = tensor([ 1.9990, -1.4777,  0.8020])\n",
      "   Gradient: tensor([-0.2829, -2.5249,  2.9753])\n",
      "   Iteration 100: log_joint = 84.996643, theta = tensor([ 1.9990, -1.4778,  0.8020])\n",
      "   Gradient: tensor([-0.2689, -2.3256,  2.7565])\n",
      "   Iteration 101: log_joint = 84.996773, theta = tensor([ 1.9990, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.2547, -2.1426,  2.5540])\n",
      "   Iteration 102: log_joint = 84.996872, theta = tensor([ 1.9990, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.2427, -1.9736,  2.3663])\n",
      "   Iteration 103: log_joint = 84.996956, theta = tensor([ 1.9989, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.2306, -1.8180,  2.1918])\n",
      "   Iteration 104: log_joint = 84.997032, theta = tensor([ 1.9989, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.2188, -1.6749,  2.0301])\n",
      "   Iteration 105: log_joint = 84.997124, theta = tensor([ 1.9989, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.2073, -1.5432,  1.8802])\n",
      "   Iteration 106: log_joint = 84.997192, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1961, -1.4225,  1.7416])\n",
      "   Iteration 107: log_joint = 84.997238, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1862, -1.3117,  1.6127])\n",
      "   Iteration 108: log_joint = 84.997253, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1745, -1.2092,  1.4937])\n",
      "   Iteration 109: log_joint = 84.997307, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1638, -1.1153,  1.3831])\n",
      "   Iteration 110: log_joint = 84.997345, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1540, -1.0275,  1.2808])\n",
      "   Iteration 111: log_joint = 84.997345, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1444, -0.9475,  1.1860])\n",
      "   Iteration 112: log_joint = 84.997414, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1363, -0.8743,  1.0981])\n",
      "   Iteration 113: log_joint = 84.997406, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.1282, -0.8066,  1.0169])\n",
      "   Iteration 114: log_joint = 84.997421, theta = tensor([ 1.9989, -1.4779,  0.8023])\n",
      "   Gradient: tensor([-0.1200, -0.7434,  0.9413])\n",
      "   Iteration 115: log_joint = 84.997429, theta = tensor([ 1.9989, -1.4779,  0.8023])\n",
      "   Gradient: tensor([-0.1125, -0.6859,  0.8717])\n",
      "   Iteration 116: log_joint = 84.997444, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.1057, -0.6322,  0.8072])\n",
      "   Iteration 117: log_joint = 84.997444, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0991, -0.5829,  0.7476])\n",
      "   Iteration 118: log_joint = 84.997444, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0930, -0.5375,  0.6923])\n",
      "   Iteration 119: log_joint = 84.997467, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0865, -0.4961,  0.6411])\n",
      "   Iteration 120: log_joint = 84.997459, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0815, -0.4571,  0.5936])\n",
      "   Iteration 121: log_joint = 84.997482, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0758, -0.4221,  0.5490])\n",
      "   Iteration 122: log_joint = 84.997490, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0712, -0.3901,  0.5082])\n",
      "   Iteration 123: log_joint = 84.997490, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0665, -0.3595,  0.4706])\n",
      "   Iteration 124: log_joint = 84.997475, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0617, -0.3315,  0.4358])\n",
      "   Iteration 125: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0572, -0.3059,  0.4036])\n",
      "   Iteration 126: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0533, -0.2822,  0.3735])\n",
      "   Iteration 127: log_joint = 84.997498, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0504, -0.2600,  0.3456])\n",
      "   Iteration 128: log_joint = 84.997490, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0472, -0.2397,  0.3198])\n",
      "   Iteration 129: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0442, -0.2213,  0.2957])\n",
      "   Iteration 130: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0407, -0.2037,  0.2736])\n",
      "   Iteration 131: log_joint = 84.997498, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0382, -0.1883,  0.2531])\n",
      "   Iteration 132: log_joint = 84.997490, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0362, -0.1734,  0.2345])\n",
      "   Iteration 133: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0332, -0.1598,  0.2173])\n",
      "   Iteration 134: log_joint = 84.997498, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0307, -0.1479,  0.2013])\n",
      "   Iteration 135: log_joint = 84.997498, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0279, -0.1371,  0.1860])\n",
      "   Iteration 136: log_joint = 84.997528, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0264, -0.1271,  0.1721])\n",
      "   Iteration 137: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0251, -0.1169,  0.1594])\n",
      "   Iteration 138: log_joint = 84.997475, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0233, -0.1078,  0.1472])\n",
      "   Iteration 139: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0218, -0.0994,  0.1363])\n",
      "   Iteration 140: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0198, -0.0923,  0.1260])\n",
      "   Iteration 141: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0179, -0.0849,  0.1165])\n",
      "   Iteration 142: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0170, -0.0788,  0.1077])\n",
      "   Iteration 143: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0168, -0.0721,  0.0997])\n",
      "   Iteration 144: log_joint = 84.997482, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0161, -0.0666,  0.0920])\n",
      "   Iteration 145: log_joint = 84.997482, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0153, -0.0612,  0.0855])\n",
      "   Iteration 146: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0143, -0.0568,  0.0793])\n",
      "   Iteration 147: log_joint = 84.997528, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0138, -0.0520,  0.0736])\n",
      "   Iteration 148: log_joint = 84.997528, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0124, -0.0486,  0.0681])\n",
      "   Iteration 149: log_joint = 84.997498, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0124, -0.0448,  0.0634])\n",
      "   Iteration 150: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0110, -0.0411,  0.0585])\n",
      "   Iteration 151: log_joint = 84.997528, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0096, -0.0387,  0.0540])\n",
      "   Iteration 152: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0091, -0.0359,  0.0500])\n",
      "   Iteration 153: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0081, -0.0333,  0.0463])\n",
      "   Iteration 154: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0070, -0.0304,  0.0427])\n",
      "   Iteration 155: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0061, -0.0273,  0.0397])\n",
      "   Iteration 156: log_joint = 84.997498, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0046, -0.0258,  0.0368])\n",
      "   Iteration 157: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0048, -0.0240,  0.0340])\n",
      "   Iteration 158: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0224,  0.0314])\n",
      "   Iteration 159: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0204,  0.0290])\n",
      "   Iteration 160: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0056, -0.0186,  0.0268])\n",
      "   Iteration 161: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0057, -0.0167,  0.0252])\n",
      "   Iteration 162: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0058, -0.0157,  0.0232])\n",
      "   Iteration 163: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0058, -0.0151,  0.0213])\n",
      "   Iteration 164: log_joint = 84.997528, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0060, -0.0142,  0.0193])\n",
      "   Iteration 165: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0133,  0.0180])\n",
      "   Iteration 166: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0124,  0.0166])\n",
      "   Iteration 167: log_joint = 84.997498, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0055, -0.0117,  0.0152])\n",
      "   Iteration 168: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0056, -0.0108,  0.0138])\n",
      "   Iteration 169: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0058, -0.0099,  0.0131])\n",
      "   Iteration 170: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0057, -0.0088,  0.0122])\n",
      "   Iteration 171: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0059, -0.0077,  0.0111])\n",
      "   Iteration 172: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0060, -0.0068,  0.0104])\n",
      "   Iteration 173: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0045, -0.0058,  0.0097])\n",
      "   Iteration 174: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0046, -0.0059,  0.0088])\n",
      "   Iteration 175: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0048, -0.0061,  0.0081])\n",
      "   Iteration 176: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0043, -0.0052,  0.0078])\n",
      "   Iteration 177: log_joint = 84.997528, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0043, -0.0053,  0.0074])\n",
      "   Iteration 178: log_joint = 84.997528, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0044, -0.0054,  0.0068])\n",
      "   Iteration 179: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0046, -0.0054,  0.0063])\n",
      "   Iteration 180: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0047, -0.0054,  0.0058])\n",
      "   Iteration 181: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0048, -0.0055,  0.0053])\n",
      "   Iteration 182: log_joint = 84.997513, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0049, -0.0056,  0.0047])\n",
      "   Iteration 183: log_joint = 84.997505, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0050, -0.0057,  0.0042])\n",
      "   Iteration 184: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0051, -0.0057,  0.0036])\n",
      "   Iteration 185: log_joint = 84.997520, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0052, -0.0059,  0.0033])\n",
      "   Iteration 186: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 187: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 188: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 189: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 190: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 191: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 192: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 193: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 194: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 195: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 196: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 197: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 198: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 199: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 200: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 201: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 202: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 203: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 204: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 205: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 206: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 207: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 208: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 209: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 210: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 211: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 212: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 213: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 214: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 215: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 216: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 217: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 218: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 219: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 220: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 221: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 222: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 223: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 224: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 225: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 226: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 227: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 228: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 229: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 230: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 231: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 232: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 233: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 234: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 235: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 236: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 237: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 238: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 239: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 240: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 241: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 242: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 243: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 244: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 245: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 246: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 247: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 248: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 249: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 250: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 251: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 252: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 253: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 254: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 255: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 256: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 257: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 258: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 259: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 260: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 261: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 262: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 263: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 264: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 265: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 266: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 267: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 268: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 269: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 270: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 271: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 272: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 273: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 274: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 275: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 276: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 277: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 278: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 279: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 280: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 281: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 282: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 283: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 284: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 285: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 286: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 287: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 288: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 289: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 290: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 291: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 292: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 293: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 294: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 295: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 296: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 297: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 298: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 299: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 300: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 301: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 302: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 303: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 304: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 305: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 306: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 307: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 308: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 309: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 310: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 311: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 312: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 313: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 314: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 315: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 316: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 317: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 318: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 319: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 320: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 321: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 322: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 323: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 324: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 325: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 326: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 327: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 328: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 329: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 330: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 331: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 332: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 333: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 334: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 335: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 336: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 337: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 338: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 339: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 340: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 341: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 342: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 343: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 344: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 345: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 346: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 347: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 348: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 349: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 350: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 351: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 352: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 353: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 354: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 355: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 356: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 357: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 358: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 359: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 360: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 361: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 362: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 363: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 364: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 365: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 366: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 367: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 368: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 369: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 370: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 371: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 372: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 373: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 374: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 375: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 376: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 377: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 378: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 379: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 380: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 381: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 382: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 383: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 384: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 385: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 386: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 387: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 388: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 389: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 390: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 391: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 392: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 393: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 394: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 395: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 396: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 397: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 398: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 399: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 400: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 401: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 402: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 403: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 404: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 405: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 406: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 407: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 408: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 409: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 410: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 411: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 412: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 413: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 414: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 415: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 416: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 417: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 418: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 419: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 420: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 421: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 422: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 423: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 424: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 425: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 426: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 427: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 428: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 429: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 430: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 431: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 432: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 433: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 434: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 435: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 436: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 437: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 438: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 439: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 440: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 441: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 442: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 443: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 444: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 445: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 446: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 447: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 448: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 449: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 450: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 451: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 452: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 453: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 454: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 455: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 456: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 457: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 458: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 459: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 460: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 461: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 462: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 463: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 464: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 465: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 466: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 467: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 468: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 469: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 470: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 471: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 472: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 473: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 474: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 475: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 476: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 477: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 478: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 479: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 480: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 481: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 482: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 483: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 484: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 485: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 486: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 487: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 488: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 489: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 490: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 491: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 492: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 493: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 494: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 495: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 496: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 497: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 498: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "   Iteration 499: log_joint = 84.997536, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0053, -0.0059,  0.0028])\n",
      "\n",
      "   Final theta: tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   True theta:  tensor([ 2.0000, -1.5000,  0.8000])\n",
      "   Error: 0.022125\n",
      "   Initial log joint: -35074.003906\n",
      "   Final log joint: 84.997536\n",
      "   Improvement: 35159.001442\n",
      "\n",
      "4. Comparing with analytical solution:\n",
      "   Analytical MAP: tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Optimized theta: tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Difference: 0.000001\n",
      "   Log joint (analytical): 84.997505\n",
      "   Log joint (optimized): 84.997536\n",
      "\n",
      "5. Testing robustness with different starting points:\n",
      "   Start 1: theta=tensor([1., 1., 1.]), log_joint=-34398.503906, grad_norm=25855.832031\n",
      "   Start 2: theta=tensor([-2.0000,  0.5000, -1.0000]), log_joint=-124982.375000, grad_norm=52713.066406\n",
      "   Start 3: theta=tensor([10., -5.,  2.]), log_joint=-430342.718750, grad_norm=98298.367188\n",
      "\n",
      "======================================================================\n",
      "Simple Linear Model Log Joint Testing Complete\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def test_simple_linear_model_log_joint():\n",
    "    \"\"\"\n",
    "    Test log joint computation with a simple linear model:\n",
    "    - Linear model: y = X @ theta + noise\n",
    "    - Gaussian likelihood: p(y | X, theta) = N(X @ theta, sigma^2)\n",
    "    - Gaussian prior: p(theta) = N(0, sigma_prior^2)\n",
    "    - Use gradient ascent to maximize log p(theta | X, y)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Testing Simple Linear Model Log Joint\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate synthetic linear data\n",
    "    n_samples, n_features = 100, 3\n",
    "    true_theta = torch.tensor([2.0, -1.5, 0.8])\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    noise = torch.randn(n_samples) * 0.1\n",
    "    y = X @ true_theta + noise\n",
    "    \n",
    "    print(f\"Data: {n_samples} samples, {n_features} features\")\n",
    "    print(f\"True theta: {true_theta}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    sigma_likelihood = torch.tensor(0.1)  # Known noise level\n",
    "    sigma_prior = torch.tensor(2.0)       # Prior variance for theta\n",
    "    \n",
    "    def log_gaussian_likelihood_simple(y, X, theta, sigma):\n",
    "        \"\"\"Simple Gaussian likelihood for linear model\"\"\"\n",
    "        pred = X @ theta\n",
    "        residuals = y - pred\n",
    "        log_prob = -0.5 * torch.log(2 * torch.pi * sigma**2) - 0.5 * (residuals**2 / sigma**2)\n",
    "        return torch.sum(log_prob) / n_samples\n",
    "    \n",
    "    def log_gaussian_prior_simple(theta, sigma):\n",
    "        \"\"\"Simple Gaussian prior for theta\"\"\"\n",
    "        log_prob = -0.5 * torch.log(2 * torch.pi * sigma**2) - 0.5 * (theta**2 / sigma**2)\n",
    "        return torch.sum(log_prob) / n_samples\n",
    "    \n",
    "    def log_joint_simple(theta, X, y, sigma_lik, sigma_prior):\n",
    "        \"\"\"Log joint probability\"\"\"\n",
    "        log_lik = log_gaussian_likelihood_simple(y, X, theta, sigma_lik)\n",
    "        log_prior = log_gaussian_prior_simple(theta, sigma_prior)\n",
    "        return log_lik + log_prior\n",
    "    \n",
    "    # Test 1: Evaluate log joint at true parameters\n",
    "    print(\"\\n1. Testing log joint evaluation:\")\n",
    "    try:\n",
    "        log_joint_true = log_joint_simple(true_theta, X, y, sigma_likelihood, sigma_prior)\n",
    "        print(f\"   Log joint at true theta: {log_joint_true.item():.6f}\")\n",
    "        \n",
    "        # Test individual components\n",
    "        log_lik_true = log_gaussian_likelihood_simple(y, X, true_theta, sigma_likelihood)\n",
    "        log_prior_true = log_gaussian_prior_simple(true_theta, sigma_prior)\n",
    "        print(f\"   Log likelihood: {log_lik_true.item():.6f}\")\n",
    "        print(f\"   Log prior: {log_prior_true.item():.6f}\")\n",
    "        print(f\"   Sum: {(log_lik_true + log_prior_true).item():.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in log joint evaluation: {e}\")\n",
    "    \n",
    "    # Test 2: Gradient computation\n",
    "    print(\"\\n2. Testing gradient computation:\")\n",
    "    try:\n",
    "        theta_test = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
    "        log_joint_val = log_joint_simple(theta_test, X, y, sigma_likelihood, sigma_prior)\n",
    "        \n",
    "        # Compute gradient using autograd\n",
    "        grad = torch.autograd.grad(log_joint_val, theta_test)[0]\n",
    "        print(f\"   Gradient at zero: {grad}\")\n",
    "        print(f\"   Gradient norm: {grad.norm().item():.6f}\")\n",
    "        \n",
    "        # The gradient should point towards the true parameters\n",
    "        print(f\"   Gradient direction vs true theta direction:\")\n",
    "        print(f\"   Normalized gradient: {grad / grad.norm()}\")\n",
    "        print(f\"   Normalized true theta: {true_theta / true_theta.norm()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient computation: {e}\")\n",
    "    \n",
    "    # Test 3: Gradient ascent optimization\n",
    "    print(\"\\n3. Testing gradient ascent optimization:\")\n",
    "    try:\n",
    "        # Initialize parameters\n",
    "        theta_opt = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
    "        learning_rate = 1e-5\n",
    "        n_iterations = 500\n",
    "        \n",
    "        log_joints = []\n",
    "        thetas = []\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            # Zero gradients\n",
    "            if theta_opt.grad is not None:\n",
    "                theta_opt.grad.zero_()\n",
    "            \n",
    "            # Forward pass\n",
    "            log_joint_val = log_joint_simple(theta_opt, X, y, sigma_likelihood, sigma_prior)\n",
    "            \n",
    "            # Backward pass\n",
    "            log_joint_val.backward()\n",
    "            \n",
    "            # Store values\n",
    "            log_joints.append(log_joint_val.item())\n",
    "            thetas.append(theta_opt.detach().clone())\n",
    "            \n",
    "            # Gradient ascent step (maximize log joint)\n",
    "            with torch.no_grad():\n",
    "                theta_opt += learning_rate * theta_opt.grad\n",
    "            \n",
    "            if i % 1 == 0:\n",
    "                print(f\"   Iteration {i}: log_joint = {log_joint_val.item():.6f}, theta = {theta_opt.detach()}\")\n",
    "                print(f\"   Gradient: {theta_opt.grad}\")\n",
    "\n",
    "        print(f\"\\n   Final theta: {theta_opt.detach()}\")\n",
    "        print(f\"   True theta:  {true_theta}\")\n",
    "        print(f\"   Error: {(theta_opt.detach() - true_theta).norm().item():.6f}\")\n",
    "        \n",
    "        # Check if optimization improved\n",
    "        print(f\"   Initial log joint: {log_joints[0]:.6f}\")\n",
    "        print(f\"   Final log joint: {log_joints[-1]:.6f}\")\n",
    "        print(f\"   Improvement: {log_joints[-1] - log_joints[0]:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient ascent: {e}\")\n",
    "    \n",
    "    # Test 4: Compare with analytical solution\n",
    "    print(\"\\n4. Comparing with analytical solution:\")\n",
    "    try:\n",
    "        # For linear regression with Gaussian prior, the MAP estimate is:\n",
    "        # theta_MAP = (X^T X + (sigma_lik^2 / sigma_prior^2) * I)^{-1} X^T y\n",
    "        lambda_reg = (sigma_likelihood / sigma_prior) ** 2\n",
    "        XtX = X.T @ X\n",
    "        Xty = X.T @ y\n",
    "        \n",
    "        theta_analytical = torch.linalg.solve(XtX + lambda_reg * torch.eye(n_features), Xty)\n",
    "        \n",
    "        print(f\"   Analytical MAP: {theta_analytical}\")\n",
    "        print(f\"   Optimized theta: {theta_opt.detach()}\")\n",
    "        print(f\"   Difference: {(theta_analytical - theta_opt.detach()).norm().item():.6f}\")\n",
    "        \n",
    "        # Evaluate log joint at analytical solution\n",
    "        log_joint_analytical = log_joint_simple(theta_analytical, X, y, sigma_likelihood, sigma_prior)\n",
    "        print(f\"   Log joint (analytical): {log_joint_analytical.item():.6f}\")\n",
    "        print(f\"   Log joint (optimized): {log_joints[-1]:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in analytical comparison: {e}\")\n",
    "    \n",
    "    # Test 5: Test gradient with different starting points\n",
    "    print(\"\\n5. Testing robustness with different starting points:\")\n",
    "    starting_points = [\n",
    "        torch.tensor([1.0, 1.0, 1.0]),\n",
    "        torch.tensor([-2.0, 0.5, -1.0]),\n",
    "        torch.tensor([10.0, -5.0, 2.0])\n",
    "    ]\n",
    "    \n",
    "    for i, start_point in enumerate(starting_points):\n",
    "        try:\n",
    "            theta_test = start_point.clone().detach().requires_grad_(True)\n",
    "            log_joint_val = log_joint_simple(theta_test, X, y, sigma_likelihood, sigma_prior)\n",
    "            grad = torch.autograd.grad(log_joint_val, theta_test)[0]\n",
    "            \n",
    "            print(f\"   Start {i+1}: theta={start_point}, log_joint={log_joint_val.item():.6f}, grad_norm={grad.norm().item():.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR with starting point {i+1}: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Simple Linear Model Log Joint Testing Complete\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run the test\n",
    "test_simple_linear_model_log_joint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dibs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
