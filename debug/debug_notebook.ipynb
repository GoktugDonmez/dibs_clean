{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/dibs.py\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    try:\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except RuntimeError:\n",
    "        trace, p = torch.tensor(0.0, device=g.device, dtype=g.dtype), g.clone()\n",
    "        for k in range(1, min(d + 1, 20)):\n",
    "            trace += (alpha ** k) * torch.trace(p) / k\n",
    "            if k < 19:\n",
    "                p = p @ g\n",
    "        return trace\n",
    "\n",
    "\n",
    "def log_gaussian_likelihood(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    residuals = x - pred_mean\n",
    "    #old incorrect log_prob = -0.5 * (np.log(2 * np.pi) -  (1/2)* torch.log(sigma_tensor**2) -  0.5*(residuals / sigma_tensor) ** 2) old\n",
    "    log_prob = -0.5 * (torch.log(2 * torch.pi * sigma_tensor**2)) - 0.5 * ((residuals / sigma_tensor)**2)\n",
    "    #normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    #log_prob = normal_dist.log_prob(x)\n",
    "\n",
    "    return torch.sum(log_prob)\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    raw_scores = alpha * torch.einsum('...ik,...jk->...ij', u, v)\n",
    "    *batch_dims, d, _ = z.shape[:-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    if batch_dims:\n",
    "        diag_mask = diag_mask.expand(*batch_dims, d, d)\n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    if probs.ndim == 3:\n",
    "        diag_mask = diag_mask.expand(probs.shape[0], d, d)\n",
    "    return probs * diag_mask\n",
    "\n",
    "def gumbel_soft_gmat(z: torch.Tensor,\n",
    "                     hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Soft Gumbel–Softmax adjacency  (Eq. B.6)\n",
    "\n",
    "        g_ij  = σ_τ( L_ij + α⟨u_i , v_j⟩ )\n",
    "\n",
    "    where  L_ij ~ Logistic(0,1)  and  τ = hparams['tau']. appendix b2\n",
    "    \"\"\"\n",
    "    raw = scores(z, hparams[\"alpha\"])\n",
    "\n",
    "    # Logistic(0,1) noise   L = log U - log(1-U)\n",
    "    u = torch.rand_like(raw)\n",
    "    L = torch.log(u) - torch.log1p(-u)\n",
    "\n",
    "    logits = (raw + L) / hparams[\"tau\"]\n",
    "    g_soft = torch.sigmoid(logits)\n",
    "\n",
    "    d = g_soft.size(-1)\n",
    "    mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    return g_soft * mask\n",
    "\n",
    "def log_full_likelihood(data: Dict[str, Any], soft_gmat: torch.Tensor, theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    ## TODO: Expert belief: update this to use interventions, change the full likelihood \n",
    "    # and also add log bernoulli likelihood calculatior\n",
    "    x_data = data['x']\n",
    "    effective_W = theta * soft_gmat\n",
    "    pred_mean = torch.matmul(x_data, effective_W)\n",
    "    sigma_obs = hparams.get('sigma_obs_noise', 0.1)\n",
    "    return log_gaussian_likelihood(x_data, pred_mean, sigma=sigma_obs)\n",
    "\n",
    "def log_theta_prior(theta_effective: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    return log_gaussian_likelihood(theta_effective, torch.zeros_like(theta_effective), sigma=sigma)\n",
    "\n",
    "def gumbel_acyclic_constr_mc(z: torch.Tensor, d: int, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    h_samples = []\n",
    "    for _ in range(hparams['n_nongrad_mc_samples']):\n",
    "        # FOR NOW, JUST GIVE THE SOFT MATRIX, AND BY ANNEALING IT TO HARD MATRIX\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "        h_samples.append(acyclic_constr(g_soft, d))\n",
    "        \n",
    "        # should gumbel soft gmat to hard gmat be done with >0.5 or with a sigmoid?  \n",
    "        #print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #g_hard = torch.bernoulli(g_soft)\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #h_samples.append(acyclic_constr(g_hard, d))\n",
    "        #g_hard = (g_soft > 0.5).float()\n",
    "        #how about this  mentioned in dibs       g_ST   = g_hard + (g_soft - g_soft.detach())   # straight-through\n",
    "        \n",
    "        #TODO fix above\n",
    "        # for now use g_soft\n",
    "        \n",
    "    h_samples = torch.stack(h_samples)\n",
    "\n",
    "\n",
    "    return torch.mean(h_samples, dim=0)\n",
    "\n",
    "def grad_z_log_joint_gumbel(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    d = z.shape[0]\n",
    "    theta_const = theta\n",
    "    \n",
    "    #z.requires_grad_(True)\n",
    "    # --- Part 1: Prior Gradient ---\n",
    "    # MC estimate of gradient of acyclicity constraint using Gumbel soft graphs\n",
    "\n",
    "    h_mean = gumbel_acyclic_constr_mc(z, d, hparams)\n",
    "    grad_h_mc = torch.autograd.grad(h_mean, z)[0]\n",
    "    grad_log_z_prior_total = -hparams['beta'] * grad_h_mc - (z / hparams['sigma_z']**2)\n",
    "\n",
    "    # --- Part 2: Likelihood Gradient ---\n",
    "    \n",
    "    # 1. We need to collect the log-probability AND the gradient for each sample.\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "\n",
    "    for _ in range( hparams['n_grad_mc_samples']):\n",
    "        # 2. Generate a single soft graph sample.\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "        # 3. Calculate the log-joint for this single sample.\n",
    "        log_density_one_sample = log_full_likelihood(data, g_soft, theta_const, hparams) + \\\n",
    "                                 log_theta_prior(theta_const * g_soft, hparams.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "        # 4. Calculate the gradient for this single sample.\n",
    "        # We must use retain_graph=True because we are doing a backward pass\n",
    "        # inside a loop, and PyTorch would otherwise free the graph memory.\n",
    "        grad, = torch.autograd.grad(log_density_one_sample, z, retain_graph=True)\n",
    "        \n",
    "        log_density_samples.append(log_density_one_sample)\n",
    "        grad_samples.append(grad)\n",
    "\n",
    "    # 5. After the loop, we can safely detach z_ from any further graph history.\n",
    "\n",
    "    # 6. Compute the final likelihood gradient using the stable weighted average.\n",
    "    # This correctly computes E[p*∇log(p)] / E[p]\n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grad_p = torch.stack(grad_samples)\n",
    "    grad_lik = weighted_grad(log_p, grad_p)\n",
    "\n",
    "\n",
    "    #if z.grad is not None:\n",
    "    #    z.grad.zero_()\n",
    "    #z.requires_grad_(False)\n",
    "    \n",
    "    # Final combined gradient\n",
    "    \n",
    "\n",
    "\n",
    "    total = grad_log_z_prior_total + grad_lik\n",
    "    # 3) Combine\n",
    "    # ------------------------------------------------\n",
    "    return total.detach()\n",
    "\n",
    "\n",
    "## SCORE BASED ESTIMATOR FOR GRADIENT Z \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Score-function estimator for ∇_Z log p(Z,Θ | D)\n",
    "#  (Section B.2 of the paper, b = 0)\n",
    "# ------------------------------------------------------------\n",
    "def analytic_score_g_given_z(z, g, hparams):\n",
    "    # 1. logits and probabilities\n",
    "    probs = bernoulli_soft_gmat(z, hparams)\n",
    "    diff   = g - probs                 # (g_ij − σ(s_ij))\n",
    "    u, v   = z[..., 0], z[..., 1]      # (d,k)\n",
    "\n",
    "    # 2. gradients wrt u and v\n",
    "    grad_u = hparams['alpha'] * torch.einsum('ij,jk->ik', diff, v)   # (d,k)\n",
    "    grad_v = hparams['alpha'] * torch.einsum('ij,ik->jk', diff, u)   # (d,k)\n",
    "\n",
    "    return torch.stack([grad_u, grad_v], dim=-1)          # (d,k,2)\n",
    "\n",
    "\n",
    "def grad_z_log_joint_score(z: torch.Tensor,\n",
    "                           theta: torch.Tensor,\n",
    "                           data: Dict[str, Any],\n",
    "                           hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ∇_Z log p(Z,Θ | D)  using the score-function (REINFORCE) estimator.\n",
    "\n",
    "    This replaces the Gumbel-soft estimator.\n",
    "    \"\"\"\n",
    "    sigma_z2 = hparams['sigma_z'] ** 2\n",
    "    beta     = hparams['beta']\n",
    "\n",
    "    M = hparams['n_grad_mc_samples'] # M = 50\n",
    "    d = z.shape[0]                   # d = 4\n",
    "    theta_const = theta\n",
    "\n",
    "\n",
    "    # 1. sample hard graphs \n",
    "    with torch.no_grad():\n",
    "        g_hard_samples = [torch.bernoulli(bernoulli_soft_gmat(z, hparams)) for _ in range(M)]\n",
    "\n",
    "    ll = []\n",
    "    scores = []\n",
    "    for g in g_hard_samples:\n",
    "        log_lik = log_full_likelihood(data, g, theta_const, hparams)\n",
    "        theta_eff = theta_const * g\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        \n",
    "        # log likelihood \n",
    "        ll.append(log_lik + log_theta_prior_val)\n",
    "        \n",
    "        # score \n",
    "        scores.append(analytic_score_g_given_z(z, g, hparams))\n",
    "    \n",
    "    log_p = torch.stack(ll)\n",
    "    grad_p = torch.stack(scores)\n",
    "\n",
    "    log_p_max = log_p.max()\n",
    "    log_p_shifted = log_p - log_p_max\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    unnormalized_w = torch.exp(log_p_shifted/10)\n",
    "    #print(f'unnormalized_w shape: {unnormalized_w.shape}, values: \\n {unnormalized_w}')\n",
    "    w = unnormalized_w / unnormalized_w.sum()\n",
    "                     # (M,1,1,...)\n",
    "    #print(f'w shape: {w.shape}, values: \\n {w}')\n",
    "\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                    \n",
    "    \n",
    "    ## compute the weighted avg \n",
    "    grad_lik = (w * grad_p).sum(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Z-prior: Gaussian + acyclicity penalty --------------\n",
    "    # gumbel is possible cuz no differentiable function in expectation \n",
    "    z_ = z.detach().clone().requires_grad_(True)\n",
    "    h_mean = gumbel_acyclic_constr_mc(z_, d, hparams)       # differentiable w.r.t z_\n",
    "    grad_h, = torch.autograd.grad(h_mean, z_, retain_graph=False)\n",
    "    grad_prior = -beta * grad_h - z_ / sigma_z2\n",
    "\n",
    "    return (grad_lik + grad_prior).detach()\n",
    "\n",
    "\n",
    "\n",
    "def weighted_grad(log_p: torch.Tensor,\n",
    "                  grad_p: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return   Σ softmax(log_p)_m * grad_p[m]\n",
    "    Shapes\n",
    "        log_p   : (M,)\n",
    "        grad_p  : (M, …)   (any extra dims)\n",
    "    \"\"\"\n",
    "    # 1. numerically stable soft-max weights\n",
    "    #print(f'log_p shape: {log_p.shape}, values:\\n {log_p}')\n",
    "    #print(f'grad_p shape: {grad_p.shape}, values: \\n{grad_p}')\n",
    "    log_p_shifted = log_p - log_p.max()          # (M,)\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    w = torch.exp(log_p_shifted)\n",
    "    #print(f'w shape: {w.shape}, values:\\n {w}')\n",
    "    w = w / w.sum()\n",
    "    #print(f'w after normalization shape: {w.shape}, values:\\n {w}')\n",
    "\n",
    "    # 2. broadcast weights onto grad tensor\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                      # (M,1,1,...)\n",
    "\n",
    "    return (w * grad_p).sum(dim=0)               # same shape as grad slice\n",
    "\n",
    "\n",
    "\n",
    "def grad_theta_log_joint(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    #theta.requires_grad_(True)\n",
    "    n_samples = hparams.get('n_grad_mc_samples', 1)\n",
    "    theta_ = theta.clone().detach().requires_grad_(True)\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        g_soft = bernoulli_soft_gmat(z, hparams)\n",
    "        #print(f\"g_soft values: {g_soft}\")\n",
    "        g_hard = torch.bernoulli(g_soft)\n",
    "        #print(f\"g_hard values: {g_hard}\")\n",
    "\n",
    "        # tryign with gumbel to be consistent with grad z and gumbel mc acylci impelmentation\n",
    "        #g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        log_lik_val = log_full_likelihood(data, g_hard, theta_, hparams)\n",
    "        theta_eff = theta_ * g_hard\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        #ll_grad, = torch.autograd.grad(log_lik_val, theta_, retain_graph=True)\n",
    "        #log_theta_prior_grad, = torch.autograd.grad(log_theta_prior_val, theta_ , retain_graph=True)\n",
    "        #print(f\"ll_grad shape: {ll_grad.shape}, values: {ll_grad}\")\n",
    "        #print(f\"log_theta_prior_grad shape: {log_theta_prior_grad.shape}, values: {log_theta_prior_grad}\")\n",
    "\n",
    "        current_log_density = log_lik_val + log_theta_prior_val\n",
    "        current_grad ,= torch.autograd.grad(current_log_density, theta_)\n",
    "        log_density_samples.append(current_log_density) \n",
    "\n",
    "        grad_samples.append(current_grad)\n",
    "    #print(f\" END OF Grad_theta mc_samples, iter number: {hparams.get('current_iteration',1)}  \\n\")\n",
    "\n",
    "    log_p_tensor = torch.stack(log_density_samples)\n",
    "    grad_p_tensor = torch.stack(grad_samples)\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    #if theta.grad is not None:\n",
    "    #    theta.grad.zero_()\n",
    "    #theta.requires_grad_(False)\n",
    "\n",
    "    grad =weighted_grad(log_p_tensor, grad_p_tensor)\n",
    "    #grad = stable_gradient_estimator(log_p_tensor, grad_p_tensor)\n",
    "    #print(f\"Grad_theta shape: {grad.shape}, values: \\n {grad}\")\n",
    "\n",
    "    return  grad.detach()\n",
    "\n",
    "\n",
    "def grad_log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "    grad_z = grad_z_log_joint_gumbel(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    #grad_z = grad_z_log_joint_score(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    grad_theta = grad_theta_log_joint(params[\"z\"].detach(), params[\"theta\"], data, hparams)\n",
    "    \n",
    "    return {\"z\": grad_z, \"theta\": grad_theta}\n",
    "\n",
    "def log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    hparams_updated = update_dibs_hparams(hparams, params[\"t\"].item())\n",
    "    z, theta = params['z'], params['theta']\n",
    "    d = z.shape[0]\n",
    "\n",
    "    g_soft = bernoulli_soft_gmat(z, hparams_updated)\n",
    "    log_lik = log_full_likelihood(data, g_soft, theta, hparams_updated)\n",
    "\n",
    "    log_prior_z_gaussian = torch.sum(Normal(0.0, hparams_updated['sigma_z']).log_prob(z))\n",
    "    expected_h_val = gumbel_acyclic_constr_mc(z, d, hparams_updated)\n",
    "    log_prior_z_acyclic = -hparams_updated['beta'] * expected_h_val\n",
    "    log_prior_z = log_prior_z_gaussian + log_prior_z_acyclic\n",
    "    \n",
    "    theta_eff = theta * g_soft\n",
    "    log_prior_theta = log_theta_prior(theta_eff, hparams_updated.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "    if (hparams_updated['current_iteration'] > 850 and hparams_updated['current_iteration'] < 1200):\n",
    "        with torch.no_grad():\n",
    "            log_terms = {\n",
    "                \"log_lik\":      log_lik.item(),\n",
    "                \"z_prior_gauss\":log_prior_z_gaussian.item(),\n",
    "                \"z_prior_acyc\": log_prior_z_acyclic.item(),   # usually ≤ 0\n",
    "                \"theta_prior\":  log_prior_theta.item(),\n",
    "                \"log_joint\": log_lik + log_prior_theta + log_prior_z + log_prior_z_acyclic,\n",
    "                \"penalty\": -hparams_updated['beta'] * expected_h_val.item()\n",
    "            }\n",
    "        print(f\"[dbg] {log_terms}\")\n",
    "\n",
    "    \n",
    "    return log_lik + log_prior_z + log_prior_theta\n",
    "\n",
    "def update_dibs_hparams(hparams: Dict[str, Any], t_step: float) -> Dict[str, Any]:\n",
    "\n",
    "    hparams['beta'] = hparams['beta_base'] * t_step # linear \n",
    "\n",
    "    hparams['alpha'] = hparams['alpha_base'] * t_step  # linear slope 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hparams['current_iteration'] = t_step # Store current iteration\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def hard_gmat_from_z(z: torch.Tensor, alpha: float = 1.0) -> torch.Tensor:\n",
    "    s = scores(z, alpha)\n",
    "    return (s > 0).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing acyclic_constr function\n",
      "============================================================\n",
      "\n",
      "1. Testing small acyclic graph (d=3):\n",
      "   Acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0 (should be <= 0 for acyclic)\n",
      "\n",
      "2. Testing small cyclic graph (d=3):\n",
      "   Cyclic graph constraint: 0.011667\n",
      "   Expected: > 0 (penalizes cycles)\n",
      "\n",
      "3. Testing identity/no edges (d=4):\n",
      "   Identity matrix constraint: 5.765625\n",
      "   Expected: close to 0\n",
      "\n",
      "4. Testing large graph (d=12, eigenvalue path):\n",
      "   Large acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0\n",
      "\n",
      "5. Testing large graph with potential eigenvalue issues (d=15):\n",
      "   Problematic graph constraint: 306.006744\n",
      "   Expected: large positive value (many cycles)\n",
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.042117\n",
      "   Gradient computed successfully: False\n",
      "   ERROR in gradient computation: 'NoneType' object has no attribute 'norm'\n",
      "\n",
      "7. Testing edge cases:\n",
      "   Very small values constraint: 0.0000000000\n",
      "   Large values constraint: 422.222290\n",
      "\n",
      "============================================================\n",
      "acyclic_constr testing complete\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13248/216125626.py:91: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
      "/tmp/ipykernel_13248/216125626.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "def test_acyclic_constr():\n",
    "    \"\"\"\n",
    "    Test the acyclic_constr function with various scenarios to debug potential issues\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing acyclic_constr function\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test case 1: Small acyclic graph (d <= 10)\n",
    "    print(\"\\n1. Testing small acyclic graph (d=3):\")\n",
    "    d = 3\n",
    "    g_acyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.3],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.0, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_acyclic = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"   Acyclic graph constraint: {h_acyclic.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0 (should be <= 0 for acyclic)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 2: Small cyclic graph\n",
    "    print(\"\\n2. Testing small cyclic graph (d=3):\")\n",
    "    g_cyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.0],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.3, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_cyclic = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"   Cyclic graph constraint: {h_cyclic.item():.6f}\")\n",
    "        print(f\"   Expected: > 0 (penalizes cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 3: Identity matrix (no edges)\n",
    "    print(\"\\n3. Testing identity/no edges (d=4):\")\n",
    "    d = 4\n",
    "    g_identity = torch.eye(d, dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_identity = acyclic_constr(g_identity, d)\n",
    "        print(f\"   Identity matrix constraint: {h_identity.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 4: Large graph (d > 10, triggers eigenvalue computation)\n",
    "    print(\"\\n4. Testing large graph (d=12, eigenvalue path):\")\n",
    "    d = 12\n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "    g_large = torch.randn(d, d) * 0.1\n",
    "    g_large = torch.triu(g_large, diagonal=1)  # Upper triangular (acyclic)\n",
    "    \n",
    "    try:\n",
    "        h_large = acyclic_constr(g_large, d)\n",
    "        print(f\"   Large acyclic graph constraint: {h_large.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "        \n",
    "    # Test case 5: Large graph that might cause eigenvalue issues\n",
    "    print(\"\\n5. Testing large graph with potential eigenvalue issues (d=15):\")\n",
    "    d = 15\n",
    "    g_problematic = torch.ones(d, d) * 0.5\n",
    "    g_problematic.fill_diagonal_(0)  # No self-loops\n",
    "    \n",
    "    try:\n",
    "        h_problematic = acyclic_constr(g_problematic, d)\n",
    "        print(f\"   Problematic graph constraint: {h_problematic.item():.6f}\")\n",
    "        print(f\"   Expected: large positive value (many cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR (expected - should fallback to series): {e}\")\n",
    "    \n",
    "    # Test case 6: Check gradients work\n",
    "    print(\"\\n6. Testing gradient computation:\")\n",
    "    d = 4\n",
    "    g_test = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "    g_test.data.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_test = acyclic_constr(g_test, d)\n",
    "        h_test.backward()\n",
    "        print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "        print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient computation: {e}\")\n",
    "    \n",
    "    # Test case 7: Edge cases\n",
    "    print(\"\\n7. Testing edge cases:\")\n",
    "    \n",
    "    # Very small values\n",
    "    d = 3\n",
    "    g_small = torch.ones(d, d) * 1e-10\n",
    "    g_small.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_small = acyclic_constr(g_small, d)\n",
    "        print(f\"   Very small values constraint: {h_small.item():.10f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with small values: {e}\")\n",
    "    \n",
    "    # Very large values (might cause overflow)\n",
    "    g_large_vals = torch.ones(d, d) * 10.0\n",
    "    g_large_vals.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_large_vals = acyclic_constr(g_large_vals, d)\n",
    "        print(f\"   Large values constraint: {h_large_vals.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large values: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"acyclic_constr testing complete\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run the test\n",
    "test_acyclic_constr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.000009\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.013367\n",
      "\n",
      "6b. Testing gradient computation (alternative method):\n",
      "   Constraint value: 0.016230\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.014625\n",
      "\n",
      "6c. Testing gradient computation (simple test):\n",
      "   Constraint value: 0.020845\n",
      "   Gradient computed using autograd.grad: True\n",
      "   Gradient norm: 0.276399\n",
      "   Gradient shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test case 6: Check gradients work\n",
    "print(\"\\n6. Testing gradient computation:\")\n",
    "d = 4\n",
    "\n",
    "# Method 1: Create tensor properly to maintain leaf status\n",
    "g_test = torch.randn(d, d) * 0.1\n",
    "# Zero out diagonal elements without modifying .data\n",
    "mask = ~torch.eye(d, dtype=torch.bool)\n",
    "g_test = g_test * mask.float()\n",
    "g_test.requires_grad_(True)\n",
    "\n",
    "try:\n",
    "    h_test = acyclic_constr(g_test, d)\n",
    "    h_test.backward()\n",
    "    print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "    if g_test.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None - tensor may not be leaf\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 2: Alternative approach using retain_grad()\n",
    "print(\"\\n6b. Testing gradient computation (alternative method):\")\n",
    "g_test2 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "g_test2.data.fill_diagonal_(0)\n",
    "g_test2.retain_grad()  # This ensures gradients are kept even for non-leaf tensors\n",
    "\n",
    "try:\n",
    "    h_test2 = acyclic_constr(g_test2, d)\n",
    "    h_test2.backward()\n",
    "    print(f\"   Constraint value: {h_test2.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test2.grad is not None}\")\n",
    "    if g_test2.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test2.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 3: Test with a simple differentiable operation\n",
    "print(\"\\n6c. Testing gradient computation (simple test):\")\n",
    "g_test3 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "# Create off-diagonal matrix using multiplication\n",
    "off_diag_mask = 1.0 - torch.eye(d)\n",
    "g_test3_masked = g_test3 * off_diag_mask\n",
    "\n",
    "try:\n",
    "    h_test3 = acyclic_constr(g_test3_masked, d)\n",
    "    grad_g3 = torch.autograd.grad(h_test3, g_test3, retain_graph=False)[0]\n",
    "    print(f\"   Constraint value: {h_test3.item():.6f}\")\n",
    "    print(f\"   Gradient computed using autograd.grad: {grad_g3 is not None}\")\n",
    "    if grad_g3 is not None:\n",
    "        print(f\"   Gradient norm: {grad_g3.norm().item():.6f}\")\n",
    "        print(f\"   Gradient shape: {grad_g3.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores z to prob and Bernouilli soft_gmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing scores and bernoulli_soft_gmat functions\n",
      "======================================================================\n",
      "\n",
      "1. Testing simple 2D case with known values:\n",
      "   Z shape: torch.Size([2, 3, 2])\n",
      "   Z:\n",
      "tensor([[[1.0000, 0.5000],\n",
      "         [0.0000, 1.0000],\n",
      "         [1.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 1.0000],\n",
      "         [1.0000, 0.5000],\n",
      "         [1.0000, 0.0000]]])\n",
      "   Scores shape: torch.Size([2, 2])\n",
      "   Scores:\n",
      "tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "   Manual score (0->1): 1.000000\n",
      "   Computed score (0->1): 1.000000\n",
      "   Manual score (1->0): 1.000000\n",
      "   Computed score (1->0): 1.000000\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "\n",
      "2. Testing bernoulli_soft_gmat:\n",
      "   Probabilities shape: torch.Size([2, 2])\n",
      "   Probabilities:\n",
      "tensor([[0.0000, 0.7311],\n",
      "        [0.7311, 0.0000]])\n",
      "   Manual prob (0->1): 0.731059\n",
      "   Computed prob (0->1): 0.731059\n",
      "   Manual prob (1->0): 0.731059\n",
      "   Computed prob (1->0): 0.731059\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "   All probs in [0,1]: True\n",
      "\n",
      "3. Testing with different alpha values:\n",
      "   Alpha = 0.1:\n",
      "     Max score: 0.100000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.524979\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 1.0:\n",
      "     Max score: 1.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.731059\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 5.0:\n",
      "     Max score: 5.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.993307\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 10.0:\n",
      "     Max score: 10.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.999955\n",
      "     Min prob: 0.000000\n",
      "\n",
      "4. Testing gradient flow:\n",
      "   ERROR in gradient flow: 'NoneType' object has no attribute 'shape'\n",
      "\n",
      "5. Testing consistency between scores and probabilities:\n",
      "   Max difference between manual and direct computation: 0.0000000000\n",
      "   Are they approximately equal: True\n",
      "\n",
      "6. Testing edge cases:\n",
      "   Zero embeddings - scores: tensor([0.])\n",
      "   Zero embeddings - probs: tensor([0.0000, 0.5000])\n",
      "   Zero embeddings - all probs should be 0.5: False\n",
      "   Large embeddings - max score: 200.000000\n",
      "   Large embeddings - max prob: 1.000000\n",
      "   Large embeddings - are probs valid: True\n",
      "\n",
      "7. Testing batched operation:\n",
      "   Batch scores shape: torch.Size([2, 3, 3])\n",
      "   Expected shape: (2, 3, 3)\n",
      "   Shapes match: True\n",
      "\n",
      "======================================================================\n",
      "scores and bernoulli_soft_gmat testing complete\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13248/3757974893.py:113: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n"
     ]
    }
   ],
   "source": [
    "def test_scores_and_bernoulli_soft_gmat():\n",
    "    \"\"\"\n",
    "    Test the scores and bernoulli_soft_gmat functions based on the mathematical formulation\n",
    "    from section 4.2 of the paper.\n",
    "    \n",
    "    Mathematical background:\n",
    "    - Z = [U, V] where U, V ∈ R^(k×d)\n",
    "    - scores should compute α * u_i^T v_j for all i,j\n",
    "    - bernoulli_soft_gmat should compute σ_α(u_i^T v_j) = 1/(1 + exp(-α * u_i^T v_j))\n",
    "    - Diagonal elements should be 0 (no self-loops)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Testing scores and bernoulli_soft_gmat functions\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test case 1: Simple 2D case with known values\n",
    "    print(\"\\n1. Testing simple 2D case with known values:\")\n",
    "    d, k = 2, 3\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Create simple Z = [U, V] with known values\n",
    "    U = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]], dtype=torch.float32)  # (k, d)\n",
    "    V = torch.tensor([[0.5, 1.0], [1.0, 0.5], [0.0, 0.0]], dtype=torch.float32)  # (k, d)\n",
    "    Z = torch.stack([U.T, V.T], dim=-1)  # (d, k, 2)\n",
    "    \n",
    "    print(f\"   Z shape: {Z.shape}\")\n",
    "    print(f\"   Z:\\n{Z}\")\n",
    "    \n",
    "    # Test scores function\n",
    "    try:\n",
    "        scores_result = scores(Z, alpha)\n",
    "        print(f\"   Scores shape: {scores_result.shape}\")\n",
    "        print(f\"   Scores:\\n{scores_result}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        u1, v1 = Z[0, :, 0], Z[0, :, 1]  # u1, v1 for node 0\n",
    "        u2, v2 = Z[1, :, 0], Z[1, :, 1]  # u2, v2 for node 1\n",
    "        \n",
    "        manual_score_01 = alpha * torch.dot(u1, v2)\n",
    "        manual_score_10 = alpha * torch.dot(u2, v1)\n",
    "        \n",
    "        print(f\"   Manual score (0->1): {manual_score_01.item():.6f}\")\n",
    "        print(f\"   Computed score (0->1): {scores_result[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual score (1->0): {manual_score_10.item():.6f}\")\n",
    "        print(f\"   Computed score (1->0): {scores_result[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(scores_result)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in scores: {e}\")\n",
    "    \n",
    "    # Test bernoulli_soft_gmat function\n",
    "    print(\"\\n2. Testing bernoulli_soft_gmat:\")\n",
    "    hparams = {\"alpha\": alpha}\n",
    "    \n",
    "    try:\n",
    "        probs = bernoulli_soft_gmat(Z, hparams)\n",
    "        print(f\"   Probabilities shape: {probs.shape}\")\n",
    "        print(f\"   Probabilities:\\n{probs}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        manual_prob_01 = torch.sigmoid(manual_score_01)\n",
    "        manual_prob_10 = torch.sigmoid(manual_score_10)\n",
    "        \n",
    "        print(f\"   Manual prob (0->1): {manual_prob_01.item():.6f}\")\n",
    "        print(f\"   Computed prob (0->1): {probs[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual prob (1->0): {manual_prob_10.item():.6f}\")\n",
    "        print(f\"   Computed prob (1->0): {probs[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(probs)}\")\n",
    "        \n",
    "        # Check probabilities are in [0, 1]\n",
    "        print(f\"   All probs in [0,1]: {torch.all((probs >= 0) & (probs <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in bernoulli_soft_gmat: {e}\")\n",
    "    \n",
    "    # Test case 3: Test with different alpha values\n",
    "    print(\"\\n3. Testing with different alpha values:\")\n",
    "    alphas = [0.1, 1.0, 5.0, 10.0]\n",
    "    \n",
    "    for alpha_test in alphas:\n",
    "        hparams_test = {\"alpha\": alpha_test}\n",
    "        try:\n",
    "            scores_test = scores(Z, alpha_test)\n",
    "            probs_test = bernoulli_soft_gmat(Z, hparams_test)\n",
    "            \n",
    "            print(f\"   Alpha = {alpha_test}:\")\n",
    "            print(f\"     Max score: {scores_test.max().item():.6f}\")\n",
    "            print(f\"     Min score: {scores_test.min().item():.6f}\")\n",
    "            print(f\"     Max prob: {probs_test.max().item():.6f}\")\n",
    "            print(f\"     Min prob: {probs_test.min().item():.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR with alpha {alpha_test}: {e}\")\n",
    "    \n",
    "    # Test case 4: Test gradient flow\n",
    "    print(\"\\n4. Testing gradient flow:\")\n",
    "    d, k = 3, 4\n",
    "    Z_grad = torch.randn(d, k, 2, requires_grad=True) * 0.5\n",
    "    hparams_grad = {\"alpha\": 2.0}\n",
    "    \n",
    "    try:\n",
    "        scores_grad = scores(Z_grad, hparams_grad[\"alpha\"])\n",
    "        probs_grad = bernoulli_soft_gmat(Z_grad, hparams_grad)\n",
    "        \n",
    "        # Compute some loss and backpropagate\n",
    "        loss = torch.sum(probs_grad ** 2)\n",
    "        loss.backward()\n",
    "        \n",
    "        print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n",
    "        print(f\"   Z gradient norm: {Z_grad.grad.norm().item():.6f}\")\n",
    "        print(f\"   Loss value: {loss.item():.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient flow: {e}\")\n",
    "    \n",
    "    # Test case 5: Test consistency between scores and probabilities\n",
    "    print(\"\\n5. Testing consistency between scores and probabilities:\")\n",
    "    d, k = 4, 3\n",
    "    Z_test = torch.randn(d, k, 2) * 0.3\n",
    "    alpha_test = 1.5\n",
    "    hparams_test = {\"alpha\": alpha_test}\n",
    "    \n",
    "    try:\n",
    "        scores_manual = scores(Z_test, alpha_test)\n",
    "        probs_from_scores = torch.sigmoid(scores_manual)\n",
    "        \n",
    "        # Zero out diagonal\n",
    "        diag_mask = 1.0 - torch.eye(d)\n",
    "        probs_from_scores = probs_from_scores * diag_mask\n",
    "        \n",
    "        probs_direct = bernoulli_soft_gmat(Z_test, hparams_test)\n",
    "        \n",
    "        # Check if they match\n",
    "        max_diff = torch.max(torch.abs(probs_from_scores - probs_direct))\n",
    "        print(f\"   Max difference between manual and direct computation: {max_diff.item():.10f}\")\n",
    "        print(f\"   Are they approximately equal: {torch.allclose(probs_from_scores, probs_direct, atol=1e-6)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in consistency test: {e}\")\n",
    "    \n",
    "    # Test case 6: Test edge cases\n",
    "    print(\"\\n6. Testing edge cases:\")\n",
    "    \n",
    "    # Zero embeddings\n",
    "    Z_zero = torch.zeros(3, 2, 2)\n",
    "    hparams_zero = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_zero = scores(Z_zero, 1.0)\n",
    "        probs_zero = bernoulli_soft_gmat(Z_zero, hparams_zero)\n",
    "        \n",
    "        print(f\"   Zero embeddings - scores: {scores_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - probs: {probs_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - all probs should be 0.5: {torch.allclose(probs_zero, torch.ones_like(probs_zero) * 0.5)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with zero embeddings: {e}\")\n",
    "    \n",
    "    # Very large embeddings (test numerical stability)\n",
    "    Z_large = torch.ones(2, 2, 2) * 10.0\n",
    "    hparams_large = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_large = scores(Z_large, 1.0)\n",
    "        probs_large = bernoulli_soft_gmat(Z_large, hparams_large)\n",
    "        \n",
    "        print(f\"   Large embeddings - max score: {scores_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - max prob: {probs_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - are probs valid: {torch.all((probs_large >= 0) & (probs_large <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large embeddings: {e}\")\n",
    "    \n",
    "    # Test case 7: Test batched operation\n",
    "    print(\"\\n7. Testing batched operation:\")\n",
    "    batch_size = 2\n",
    "    d, k = 3, 2\n",
    "    Z_batch = torch.randn(batch_size, d, k, 2) * 0.5\n",
    "    hparams_batch = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_batch = scores(Z_batch, 1.0)\n",
    "        # Note: bernoulli_soft_gmat might not support batching directly\n",
    "        print(f\"   Batch scores shape: {scores_batch.shape}\")\n",
    "        print(f\"   Expected shape: ({batch_size}, {d}, {d})\")\n",
    "        print(f\"   Shapes match: {scores_batch.shape == (batch_size, d, d)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in batch operation: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"scores and bernoulli_soft_gmat testing complete\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run the test\n",
    "test_scores_and_bernoulli_soft_gmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.034s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running tests for acyclic_constr ---\n",
      "\n",
      "--- Running tests for Graph Generation ---\n",
      "Large acyclic graph (d=12) H(G): 0.000000\n",
      "Graph with self-loop H(G): 1.370371\n",
      "Acyclic graph H(G): 0.000000\n",
      "Graph with 2-cycle H(G): 0.666667\n",
      "\n",
      "Calculated soft G-matrix:\n",
      "tensor([[0.0000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.0000]])\n",
      "Expected soft G-matrix:\n",
      "tensor([[0.5000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.5000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.5000]])\n",
      "\n",
      "Calculated scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n",
      "Expected scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standalone test cell for debug_notebook.ipynb\n",
    "#\n",
    "# To use this, you would typically have the functions available \n",
    "# in the same notebook or imported from your 'models/dibs.py' script.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import unittest\n",
    "import logging\n",
    "\n",
    "# --- Setup basic logger ---\n",
    "# This is to prevent errors if log.warning is called in acyclic_constr\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# --- Functions to be tested ---\n",
    "# Pasted here for stand-alone execution.\n",
    "# In a real scenario, you would import these from your scripts.\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    # Ensure g is a floating point tensor for matrix operations\n",
    "    g = g.float()\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    # Using matrix_power for d <= 10 as it's generally stable for smaller matrices\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    # For larger d, eigenvalues are more efficient but can be numerically unstable\n",
    "    try:\n",
    "        # Eigenvalue decomposition is faster for large d\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        # The constraint is based on the sum of the d-th power of eigenvalues\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except torch.linalg.LinAlgError:\n",
    "        # Fallback to series expansion if eigenvalue computation fails\n",
    "        # This is a less precise but more stable approximation\n",
    "        log.warning(f\"Eigenvalue computation failed for d={d}. Falling back to series expansion.\")\n",
    "        trace = torch.tensor(0.0, device=g.device, dtype=g.dtype)\n",
    "        p = eye.clone() # Start with identity matrix for power calculation\n",
    "        for k in range(1, min(d + 1, 20)): # Limit to 20 terms for practical purposes\n",
    "            p = p @ m\n",
    "            trace += torch.trace(p) / k\n",
    "        return trace\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    \"\"\"Calculates the raw edge scores from latent embeddings.\"\"\"\n",
    "    # z has shape [d, k, 2]\n",
    "    # u and v have shape [d, k]\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    \n",
    "    # einsum performs batch matrix multiplication of u and v.T\n",
    "    # 'ik,jk->ij' means: sum over k for each i and j\n",
    "    raw_scores = alpha * torch.einsum('ik,jk->ij', u, v)\n",
    "    \n",
    "    # Ensure no self-loops by masking the diagonal\n",
    "    d = z.shape[0]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    \n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: dict) -> torch.Tensor:\n",
    "    \"\"\"Generates a soft adjacency matrix using a Bernoulli parameterization.\"\"\"\n",
    "    # Get probabilities by applying a sigmoid to the raw scores\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    \n",
    "    # The scores function already handles the diagonal masking, but as a safeguard:\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    \n",
    "    return probs * diag_mask\n",
    "\n",
    "\n",
    "# --- Test Cases ---\n",
    "\n",
    "class TestAcyclicConstraint(unittest.TestCase):\n",
    "\n",
    "    def test_strictly_acyclic_graph(self):\n",
    "        \"\"\"Tests a graph with no cycles (a Directed Acyclic Graph).\"\"\"\n",
    "        g_acyclic = torch.tensor([[0., 1., 1.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_acyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"Acyclic graph H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=5, msg=\"Acyclic graph should have H(G) = 0\")\n",
    "\n",
    "    def test_self_loop_cycle(self):\n",
    "        \"\"\"Tests a graph with a self-loop (the simplest cycle).\"\"\"\n",
    "        g_cyclic = torch.tensor([[1., 1., 0.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with self-loop H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "\n",
    "    def test_two_node_cycle(self):\n",
    "        \"\"\"Tests a graph with a 2-cycle (A -> B, B -> A).\"\"\"\n",
    "        g_cyclic = torch.tensor([[0., 1., 0.], [1., 0., 0.], [0., 1., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with 2-cycle H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "        \n",
    "    def test_large_graph_eigenvalue_path(self):\n",
    "        \"\"\"Tests the eigenvalue code path with a larger (d=12) acyclic graph.\"\"\"\n",
    "        d = 12\n",
    "        g_large_acyclic = torch.triu(torch.ones(d, d), diagonal=1)\n",
    "        h_val = acyclic_constr(g_large_acyclic, d)\n",
    "        print(f\"Large acyclic graph (d=12) H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=4, msg=\"Large acyclic graph should have H(G) near 0\")\n",
    "\n",
    "\n",
    "class TestGraphGeneration(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up common variables for the tests.\"\"\"\n",
    "        self.d = 3  # Number of nodes\n",
    "        self.k = 2  # Latent dimension\n",
    "        # Latent variable Z = [U, V]\n",
    "        self.z = torch.arange(self.d * self.k * 2, dtype=torch.float32).view(self.d, self.k, 2)\n",
    "        # z will be:\n",
    "        # [[[ 0,  1], [ 2,  3]],\n",
    "        #  [[ 4,  5], [ 6,  7]],\n",
    "        #  [[ 8,  9], [10, 11]]]\n",
    "        self.alpha = 0.5\n",
    "        self.hparams = {\"alpha\": self.alpha}\n",
    "\n",
    "    def test_scores_calculation(self):\n",
    "        \"\"\"Tests the bilinear score calculation G_ij = alpha * u_i^T v_j.\"\"\"\n",
    "        u = self.z[..., 0] # [[[0, 2], [4, 6], [8, 10]]]\n",
    "        v = self.z[..., 1] # [[[1, 3], [5, 7], [9, 11]]]\n",
    "        \n",
    "        # Manually calculate expected scores\n",
    "        expected_scores = self.alpha * torch.matmul(u, v.T)\n",
    "        # Set diagonal to zero\n",
    "        expected_scores.fill_diagonal_(0)\n",
    "        \n",
    "        # Get scores from function\n",
    "        s = scores(self.z, self.alpha)\n",
    "        print(f\"\\nCalculated scores:\\n{s}\")\n",
    "        print(f\"Expected scores:\\n{expected_scores}\")\n",
    "        \n",
    "        self.assertTrue(torch.allclose(s, expected_scores), \"Scores do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(s) == 0), \"Diagonal of scores matrix should be zero.\")\n",
    "\n",
    "    def test_bernoulli_soft_gmat(self):\n",
    "        \"\"\"Tests the sigmoid transformation of scores to get probabilities.\"\"\"\n",
    "        # Calculate scores first\n",
    "        s = scores(self.z, self.alpha)\n",
    "        \n",
    "        # Manually calculate expected probabilities\n",
    "        expected_probs = torch.sigmoid(s)\n",
    "        \n",
    "        # Get probabilities from function\n",
    "        g_soft = bernoulli_soft_gmat(self.z, self.hparams)\n",
    "        print(f\"\\nCalculated soft G-matrix:\\n{g_soft}\")\n",
    "        print(f\"Expected soft G-matrix:\\n{expected_probs}\")\n",
    "        expected_probs.fill_diagonal_(0)\n",
    "        self.assertTrue(torch.allclose(g_soft, expected_probs), \"Soft G-matrix probabilities do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(g_soft) == 0), \"Diagonal of soft G-matrix should be zero.\")\n",
    "\n",
    "\n",
    "# --- Running the tests ---\n",
    "# This allows running the tests directly from the cell.\n",
    "suite = unittest.TestSuite()\n",
    "print(\"--- Running tests for acyclic_constr ---\")\n",
    "suite.addTest(unittest.makeSuite(TestAcyclicConstraint))\n",
    "print(\"\\n--- Running tests for Graph Generation ---\")\n",
    "suite.addTest(unittest.makeSuite(TestGraphGeneration))\n",
    "\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_gaussian_likelihood_manual(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    residuals = x - pred_mean\n",
    "    #old incorrect log_prob = -0.5 * (np.log(2 * np.pi) -  (1/2)* torch.log(sigma_tensor**2) -  0.5*(residuals / sigma_tensor) ** 2) old\n",
    "    log_prob = -0.5 * (torch.log(2 * torch.pi * sigma_tensor**2)) - 0.5 * ((residuals / sigma_tensor)**2)\n",
    "    #normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    #log_prob = normal_dist.log_prob(x)\n",
    "\n",
    "    return torch.sum(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 12:33:12,317 - INFO - Generated data with 200 samples and 5 features.\n",
      "2025-06-26 12:33:12,320 - INFO - True theta:\n",
      "tensor([[ 2.5000, -1.0000,  3.3000,  0.0000, -4.1000]])\n",
      "2025-06-26 12:33:12,321 - INFO - Starting gradient ascent to maximize log joint probability...\n",
      "2025-06-26 12:33:12,324 - INFO - Epoch 000 | Log Joint: -4419756544.0000 | Loss: 4419756544.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 12:33:12,371 - INFO - Epoch 050 | Log Joint: -3310176256.0000 | Loss: 3310176256.0000\n",
      "2025-06-26 12:33:12,435 - INFO - Epoch 100 | Log Joint: -2472822016.0000 | Loss: 2472822016.0000\n",
      "2025-06-26 12:33:12,487 - INFO - Epoch 150 | Log Joint: -1837675264.0000 | Loss: 1837675264.0000\n",
      "2025-06-26 12:33:12,535 - INFO - Epoch 200 | Log Joint: -1349286784.0000 | Loss: 1349286784.0000\n",
      "2025-06-26 12:33:12,574 - INFO - Epoch 250 | Log Joint: -976052864.0000 | Loss: 976052864.0000\n",
      "2025-06-26 12:33:12,616 - INFO - Epoch 300 | Log Joint: -695617088.0000 | Loss: 695617088.0000\n",
      "2025-06-26 12:33:12,664 - INFO - Epoch 350 | Log Joint: -488877888.0000 | Loss: 488877888.0000\n",
      "2025-06-26 12:33:12,708 - INFO - Epoch 400 | Log Joint: -339217632.0000 | Loss: 339217632.0000\n",
      "2025-06-26 12:33:12,752 - INFO - Epoch 450 | Log Joint: -232652176.0000 | Loss: 232652176.0000\n",
      "2025-06-26 12:33:12,791 - INFO - Training finished.\n",
      "2025-06-26 12:33:12,794 - INFO - Learned theta:\n",
      "tensor([[ 2.1981, -1.1018,  2.9007, -0.1168, -2.7950]])\n",
      "2025-06-26 12:33:12,795 - INFO - True theta:\n",
      "tensor([[ 2.5000, -1.0000,  3.3000,  0.0000, -4.1000]])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA00AAAIhCAYAAACSQlG5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABrtElEQVR4nO3deVxU9eLG8WfYdxAREEHFXVxwV2xxC1NLs12ztLJbZrbabS+XFtu3X/u1tMXSumlZtmhWLllp7vu+goqIsi/DzPn9Qc6VQAQEzgx83q8XL5kzZ2aeGb6D83DO+R6LYRiGAAAAAAClcjM7AAAAAAA4M0oTAAAAAJSB0gQAAAAAZaA0AQAAAEAZKE0AAAAAUAZKEwAAAACUgdIEAAAAAGWgNAEAAABAGShNAAAAAFAGShMAlzJz5kxZLBb99ddfZkfR5MmTZbFYlJqaWunbVsaWLVs0efJk7du3r1zrn3rNTn15eHgoOjpaN910k5KSkiqV4UwsFosmTJhQZfe3b98+WSwWvfjii2dd99TzPP11ufHGG9W0adNi6zVt2lQ33nij43JycrImT56sdevWVU3osziXn70Z91uWUz+f8nyVd7yeSWk/y/IqbWwAQEV4mB0AAOqiW265RYMGDarUbbds2aIpU6aob9++FfoQOWPGDLVp00a5ublaunSppk2bpiVLlmjjxo3y9/evVBZncskll+j3339Xw4YNy1xv3rx5CgoKclxOTk7WlClT1LRpU3Xq1KmaU1afcxlTldWwYUP9/vvvxZaNHz9e6enpmjVrVol1z8Xjjz+uu+++u1K3Le/YAIAzoTQBgAmio6MVHR1do4/Zvn17devWTZLUr18/2Ww2Pfnkk/rqq680atSoUm+Tk5MjPz+/moxZaQ0aNFCDBg3Oul7nzp1rIE3NOfUzMmNMeXt7q1evXsWWBQUFqaCgoMTyf8rNzZWvr2+5H6t58+aVyiiVf2wAwJmwex6AWmn58uUaMGCAAgMD5efnp969e2vBggWlrpeQkCAfHx81atRIjz/+uKZPn35Ou/J88MEHio+Pl4+Pj0JDQ3X55Zdr69atxdYpbVeqpk2b6tJLL9UPP/ygLl26yNfXV23atNEHH3zgWGfmzJm6+uqrJRUVn1O7Ps2cObPCOU99qN2/f7+kot2fAgICtHHjRg0cOFCBgYEaMGCAJCktLU3jx49Xo0aN5OXlpWbNmunRRx9Vfn5+qff97rvvqlWrVvL29lZcXJxmz55d7Ppjx45p/PjxiouLU0BAgMLDw9W/f38tW7as1Puz2+16+umn1bhxY/n4+Khbt25avHhxsXXKuwvW6bvn/frrr+revbsk6aabbnK8npMnT9bHH38si8VSYkuKJE2dOlWenp5KTk4u87EWLFigTp06ydvbW7GxsaXuZnhqF7fSfoanspxyatysWbNGV111lerVq+coE5UdU6dUx3vhnznmzp2rzp07y8fHR1OmTJEkvfnmm7rwwgsVHh4uf39/dejQQc8//7ysVmux+yht97xTu4N+/PHHatu2rfz8/BQfH69vv/222HqljY2+ffuqffv2WrVqlS644AL5+fmpWbNmevbZZ2W324vdfvPmzRo4cKD8/PzUoEED3XHHHVqwYIEsFot+/fXXc3ptALgGtjQBqHWWLFmixMREdezYUe+//768vb311ltvaejQofrss8907bXXSpI2bNigxMREtWrVSh9++KH8/Pz0zjvv6JNPPqn0Y0+bNk2PPPKIRo4cqWnTpun48eOaPHmyEhIStGrVKrVs2bLM269fv14TJ07UQw89pIiICE2fPl1jx45VixYtdOGFF+qSSy7RM888o0ceeURvvvmmunTpIqlyf4XftWuXJBX7C3xBQYGGDRum2267TQ899JAKCwuVl5enfv36affu3ZoyZYo6duyoZcuWadq0aVq3bl2JMjp//nz98ssvmjp1qvz9/fXWW29p5MiR8vDw0FVXXSWpqIRJ0qRJkxQZGamsrCzNmzdPffv21eLFi9W3b99i9/nGG2+oSZMmevXVV2W32/X8889r8ODBWrJkiRISEir83E/p0qWLZsyYoZtuukmPPfaYLrnkEklFWwLDw8P1wAMP6M033yz2GIWFhXr33Xd1+eWXKyoq6oz3vXjxYl122WVKSEjQ7NmzZbPZ9Pzzz+vo0aOVznvKFVdcoREjRmjcuHHKzs4uc92zjSmpet4L/7RmzRpt3bpVjz32mGJjYx27hO7evVvXXXedYmNj5eXlpfXr1+vpp5/Wtm3bSi13/7RgwQKtWrVKU6dOVUBAgJ5//nldfvnl2r59u5o1a1bmbY8cOaJRo0Zp4sSJmjRpkubNm6eHH35YUVFRGj16tCTp8OHD6tOnj/z9/fX2228rPDxcn332WZUeuwfABRgA4EJmzJhhSDJWrVp1xnV69eplhIeHG5mZmY5lhYWFRvv27Y3o6GjDbrcbhmEYV199teHv728cO3bMsZ7NZjPi4uIMScbevXvLzDJp0iRDkuP2J06cMHx9fY0hQ4YUW+/AgQOGt7e3cd1115W47emaNGli+Pj4GPv373csy83NNUJDQ43bbrvNseyLL74wJBm//PJLmflOOfWa/fHHH4bVajUyMzONb7/91mjQoIERGBhoHDlyxDAMwxgzZowhyfjggw+K3f6dd94xJBmff/55seXPPfecIclYuHChY5kkw9fX13GfhlH02rdp08Zo0aLFGTMWFhYaVqvVGDBggHH55Zc7lu/du9eQZERFRRm5ubmO5RkZGUZoaKhx0UUXlXiep//cxowZYzRp0qTYYzVp0sQYM2aM4/KqVasMScaMGTNK5Jo0aZLh5eVlHD161LFszpw5hiRjyZIlZ3w+hmEYPXv2PGPu03/2p55jaY8vyZg0aVKxPJKMJ554otSslR1T5/peOF2fPn2Mdu3alcjh7u5ubN++vczb2mw2w2q1Gh999JHh7u5upKWlOa4r7WcpyYiIiDAyMjIcy44cOWK4ubkZ06ZNcywrbWz06dPHkGT8+eefxe4zLi7OuPjiix2X//3vfxsWi8XYvHlzsfUuvvjiCr0PAbi2Ort73tKlSzV06FBFRUXJYrHoq6++qvB9fP755+rUqZP8/PzUpEkTvfDCC1UfFECFZGdn688//9RVV12lgIAAx3J3d3fdcMMNOnTokLZv3y6paItU//79FRYW5ljPzc1N11xzTaUe+/fff1dubm6xmdkkKSYmRv379y+xO1lpOnXqpMaNGzsu+/j4qFWrVo5d6M5Fr1695OnpqcDAQF166aWKjIzU999/r4iIiGLrXXnllcUu//zzz/L393dsJTrl1PP85/MaMGBAsft0d3fXtddeq127dunQoUOO5e+88466dOkiHx8feXh4yNPTU4sXLy6xK6NUtGXFx8fHcTkwMFBDhw7V0qVLZbPZKvZCVMDtt98uSfrPf/7jWPbGG2+oQ4cOjq00pcnOztaqVavOmPtc/fNnVJbyjKmqfi+UpmPHjmrVqlWJ5WvXrtWwYcNUv359ubu7y9PTU6NHj5bNZtOOHTvOer/9+vVTYGCg43JERITCw8PL9Z6JjIxUjx49SuT852vTvn17xcXFFVtv5MiRZ71/ALVHnS1N2dnZio+P1xtvvFGp23///fcaNWqUxo0bp02bNumtt97Syy+/XOn7A1A1Tpw4IcMwSp0l69SuVMePH3f8+8/CIKnUZeVx6n7P9Ninri9L/fr1Syzz9vZWbm5upTKd7qOPPtKqVau0du1aJScna8OGDTrvvPOKrePn51dsZjmp6HlFRkaWOF4mPDxcHh4eJZ5XZGRkicc+tezUui+//LJuv/129ezZU19++aX++OMPrVq1SoMGDSr1uZ7pPgsKCpSVlVWOZ185ERERuvbaa/Xuu+/KZrNpw4YNWrZs2Vl3zTpx4oTsdnuZr8W5qMgscOUZU1X9XihNaZkPHDigCy64QElJSXrttde0bNkyrVq1Sm+++aYklWvcn8t7xlleGwDOr84e0zR48GANHjz4jNcXFBToscce06xZs3Ty5Em1b99ezz33nGM/+48//ljDhw/XuHHjJEnNmjXTgw8+qOeee0533HFHjZ8rA0CRevXqyc3NTYcPHy5x3amD9k/9Nb1+/fqlHl9y5MiRSj32qQ9gZ3rs0/+Kb4a2bds6Zs87k9J+d9WvX19//vmnDMModn1KSooKCwtLPK/SXr9Ty069Rp988on69u2rt99+u9h6mZmZpeY60316eXkV26JYHe6++259/PHH+vrrr/XDDz8oJCTkjLMNnlKvXj1ZLJYyX4tTTm2J+uekGmWV7Kr+P6aq3wulKS3zV199pezsbM2dO1dNmjRxLK+pc2aVR028NgCcX53d0nQ2N910k3777TfNnj1bGzZs0NVXX61BgwZp586dkor+czt9lwtJ8vX11aFDh6pkNxoAlePv76+ePXtq7ty5xf5abLfb9cknnyg6Otqxi1CfPn30888/Fzs5rd1u1xdffFGpx05ISJCvr2+Jg+cPHTqkn3/+2TET3bny9vaWVL6/wleFAQMGKCsrq8RuzB999JHj+tMtXry42IdMm82mOXPmqHnz5o4psS0Wi+N5nLJhw4ZSZ6qTpLlz5yovL89xOTMzU998840uuOACubu7V/q5SWd/Pbt27arevXvrueee06xZs3TjjTee9bxW/v7+6tGjxxlzny4iIkI+Pj7asGFDseVff/11ZZ5OpVT1e6G8ThWp08eCYRjFdoc0W58+fbRp0yZt2bKl2PJ/zggJoHars1uayrJ792599tlnOnTokGN3nvvvv18//PCDZsyYoWeeeUYXX3yx7r33Xt14443q16+fdu3apVdffVVS0V+ZK3vWcgDl8/PPP5c6DfKQIUM0bdo0JSYmql+/frr//vvl5eWlt956S5s2bdJnn33m+KD26KOP6ptvvtGAAQP06KOPytfXV++8845jNjI3t/L9XenU/YWEhOjxxx/XI488otGjR2vkyJE6fvy4pkyZIh8fH02aNKlKnnv79u0lSe+9954CAwPl4+Oj2NjYUnc1qgqjR4/Wm2++qTFjxmjfvn3q0KGDli9frmeeeUZDhgzRRRddVGz9sLAw9e/fX48//rhj9rxt27YV+5B56aWX6sknn9SkSZPUp08fbd++XVOnTlVsbKwKCwtLZHB3d1diYqLuu+8+2e12Pffcc8rIyHBMW30umjdvLl9fX82aNUtt27ZVQECAoqKiis2Md/fdd+vaa6+VxWLR+PHjy3W/Tz75pAYNGqTExERNnDhRNptNzz33nPz9/R2zB0pF4+f666/XBx98oObNmys+Pl4rV67Up59+es7Prbyq6r1QUYmJifLy8tLIkSP1wAMPKC8vT2+//bZOnDhRLY9XGffcc48++OADDR48WFOnTlVERIQ+/fRTbdu2TVL1vTYAnAvv9FKsWbNGhmGoVatWCggIcHwtWbJEu3fvliT961//0oQJE3TppZfKy8tLvXr10ogRIyTpnP/qCeDsHnzwQV199dUlvlJSUhx/Nff399eNN96oESNGKD09XfPnz3dMNy5J8fHxWrRokXx9fTV69GjdeuutateuneNDcXBwcJkZcnJyJBX/K/nDDz+s6dOna/369Ro+fLgmTJigdu3aacWKFWedbry8YmNj9eqrr2r9+vXq27evunfvXmLrRVXy8fHRL7/8olGjRumFF17Q4MGDNXPmTN1///2aO3duifWHDRumCRMm6LHHHtOVV16pffv2adasWcVe+0cffVQTJ07U+++/r0suuUTTp0/XO++8o/PPP7/UDBMmTFBiYqLuuusuXXfddSosLNSCBQtKHJNVGX5+fvrggw90/PhxDRw4UN27d9d7771XbJ3hw4fL29tbF198cbl/jomJifrqq6+UkZGha6+9Vvfdd5+uvPJK3XzzzSXWfemll3T99dfr+eef12WXXabff/+9xLmGqtO5vhcqq02bNvryyy914sQJXXHFFbrzzjvVqVMnvf7669XyeJURFRWlJUuWqFWrVho3bpxGjRolLy8vTZ06VVLRH0sA1H4WwzAMs0OYzWKxaN68eRo+fLgkac6cORo1apQ2b95cogAFBAQUO4jXZrPpyJEjatCggRYvXqwhQ4bo6NGjCg8Pr8mnAKAKDRw4UPv27TvrzF2nPtympKTUUDKY5ZtvvtGwYcO0YMECDRkyxOw4Naa874W66NZbb9Vnn32m48ePy8vLy+w4AKoZu+eVonPnzrLZbEpJSdEFF1xQ5rru7u5q1KiRJOmzzz5TQkIChQlwIffdd586d+6smJgYpaWladasWVq0aJHef//9M95m+fLlWrFihb777jtOcFnLbdmyRfv379fEiRPVqVOnMicQcnWVeS/UFVOnTlVUVJSaNWumrKwsffvtt5o+fboee+wxChNQR9TZ0pSVlaVdu3Y5Lu/du1fr1q1TaGioWrVqpVGjRmn06NF66aWX1LlzZ6Wmpurnn39Whw4dNGTIEKWmpuq///2v+vbtq7y8PM2YMUNffPGFlixZYuKzAlBRNptNTzzxhI4cOSKLxaK4uDh9/PHHuv766894mwsvvFANGjTQrbfeqmeeeaYG06KmjR8/Xr/99pu6dOmiDz/8sFbPjFqZ90Jd4enpqRdeeEGHDh1SYWGhWrZsqZdffll333232dEA1JA6u3ver7/+qn79+pVYPmbMGM2cOVNWq1VPPfWUPvroIyUlJal+/fpKSEjQlClT1KFDB6Wmpmro0KHauHGjDMNQQkKCnn76afXs2dOEZwMAAACgutTZ0gQAAAAA5cHseQAAAABQBkoTAAAAAJShTk0EYbfblZycrMDAwFp9MC8AAACAshmGoczMTEVFRZ31RNV1qjQlJycrJibG7BgAAAAAnMTBgwcVHR1d5jouWZreeustvfDCCzp8+LDatWunV1999aznU5KkwMBASUUvTFBQUHXHPCur1aqFCxdq4MCB8vT0NDsOXABjBhXFmEFFMWZQUYwZVJSzjJmMjAzFxMQ4OkJZXK40zZkzR/fcc4/eeustnXfeeXr33Xc1ePBgbdmyRY0bNy7ztqd2yQsKCnKa0uTn56egoCB+yaBcGDOoKMYMKooxg4pizKCinG3MlOewHZebCOLll1/W2LFjdcstt6ht27Z69dVXFRMTo7ffftvsaAAAAABqIZfa0lRQUKDVq1froYceKrZ84MCBWrFiRYn18/PzlZ+f77ickZEhqajdWq3W6g1bDqcyOEMWuAbGDCqKMYOKYsygohgzqChnGTMVeXyXKk2pqamy2WyKiIgotjwiIkJHjhwpsf60adM0ZcqUEssXLlwoPz+/astZUYsWLTI7AlwMYwYVxZhBRTFmUFGMGVSU2WMmJyen3Ou6VGk65Z/7HRqGUeq+iA8//LDuu+8+x+VTB3sNHDjQaY5pWrRokRITE51if044P8YMKooxg4pizKCiGDOoKGcZM6f2QisPlypNYWFhcnd3L7FVKSUlpcTWJ0ny9vaWt7d3ieWenp5O9aZ2tjxwfowZVBRjBhXFmEFFMWZQUWaPmYo8tktNBOHl5aWuXbuW2JS3aNEi9e7d26RUAAAAAGozl9rSJEn33XefbrjhBnXr1k0JCQl67733dODAAY0bN87saAAAAABqIZcrTddee62OHz+uqVOn6vDhw2rfvr2+++47NWnSxOxoAAAAAGohlytNkjR+/HiNHz/e7BgAAAAA6gCXOqYJAAAAAGoapQkAAAAAykBpAgAAAIAyUJoAAAAAoAyUJgAAAAAoA6UJAAAAAMpAaQIAAACAMrjkeZoAAAAAuIZCm11p2QU6lpWv1KwCHT2ZI0/D7FQVQ2kCAAAAUCF2u6ETOQVKyczXscx8pWTmKyUzT8cyi4pRama+UrOKvk7kWEvc/pluJoQ+B5QmAAAAAJKk/EKbjhUrQvl/X87737KMojJUaC//5iI3ixTq762wAC/V9/dSoXG0Gp9F1aM0AQAAALWc1WZXSma+jqTnFX1l5OloxqkilKeUjHwdy8rXyVK2CpUl1N9L4YHeanD6V0DRv2EB3qof4KWwAG/V8/OSu5ulKIvVqu+++646nma1oTQBAAAALiw7v1BHMvKKFaJ//pualS+jnBuGvNzdikpPoLfC//5qEOit8ECf/30fVFSKPN3rxrxylCYAAADASWXnFyrpZK6STuYq+WRuqcUoM6+wXPfl6W5ReKCPIoOLviICfRQRdFohCiraShTi5ymLxVLNz8y1UJoAAAAAE9jtho5l5ReVohNFpSj5ZK6STuY5SlJ6bvl2lwvw9lBEkLcaBvsqIshHDYN9FBHso8hT3wf5qL6/l9zcKEOVQWkCAAAAqkFugc1RfpL/3lr0v8t5OpyeK6vt7PvMBfl4qFE9P0UF+6hhSFERigz2/ftfb0UE+SjQx7MGnlHdRWkCAAAAKqHQZtfh9DwdPJGjQ2m5OpCWo4MncnQwLUcH0nKVmpV/1vtwd7MoMshHUSE+ahTiq6i/vxqF+KpRPV81DKYQOQNKEwAAAFAKwzCUll2ggyf+LkRpOTp0Iufv74u2GJ1t2u0Ab4+/y5BPURmq5+soR41CfBUe6C2POjKZgiujNAEAAKDOstkNHU7P1b7UHO07nq19qdnad/x/5SinwFbm7b3c3dSonq9iQv0UU89XjUP9/v7eTzGhvgrx86qhZ4LqRGkCAABArWazG0o+mav9x3O093i29qdmFxWk4zk6cDxHBTZ7mbePDPJRTKjv30XI738Fqb6fIgJ9mFyhDqA0AQAAwOX9sxjtS83W/uPZ2puarYNpuWUWI093i2JC/RRb319N6vuraZifY4tRoxBf+Xi61+AzgTOiNAEAAMBlZOcXas+xbO1JzdLulCztPpat3ceytCc1WwWFZy5GXu5uign1VWzY38Wovp+ahvmraX1/RYX4yp2tRSgDpQkAAABOxTAMHcnI0+6UvwvRsf+Vo8PpeWe8nZe7mxrX9ysqRPX91STM/++tR34UI5wTShMAAABMUWiza9/xHO08mqmdKf8rR3uOZSm7jAkYwgK81KxBgJo3CFDzBv5//xugRvUoRqgelCYAAABUK5vd0IG0HO04mqmdRzO142iWdhzN1J5j2Wc81sjDzaLG9f0chah5A381Dw9Q87AABftx3iLULEoTAAAAqoTdbijpZK62H8nUjpRM7fy7HO1KyVL+GY438vNyV8vwALUID1SL8P+Vo8ahfvLk/EVwEpQmAAAAVNjxrHxtPZypzUkntHiXm6a/84d2H8s+43mNvD3c1CI8QK0jAtUyIlCtIgLUKiJQjUJ8mbIbTo/SBAAAgDMqtNm1JzVbWw9naOvhzL//zVBKZv5pa7lJypBUNBlDswb+avV3MWoZEajWEYGKCfXjeCO4LEoTAAAAJEkncwq05bRytO1IhnYczTrjVN5N6vupdUSA3DKO6JLzO6ttVIia1veTB7vVoZahNAEAANQxhmHocHqeNiala3NSujYlF209OtN03v5e7modGai2DYP+/gpU68ggBXh7yGq16rvvvtOgdhHy9GSCBtROlCYAAIBazDAMHUzL1abkdG1KSi8qSskZSssuKHX9mFBftY38Xzlq2zBIMfX8OO4IdRqlCQAAoJaw2w3tO56tTckZ2vx3QdqUlK6MvMIS63q4WdQyIlDto4LUvlGw2kUFqXVkoAJ92FoE/BOlCQAAwAUZhqFDJ3K1/tBJrT94UusPpWtLcoay8ksWJC93N7WODFT7RkUFqX1UsFpHBsrH092E5IDroTQBAAC4gJM5BVp/KF3rDpx0FKXjpexi5+3hprYNg9S+UZA6NApWu6hgtYoIlJcHkzMAlUVpAgAAcDJ5Vpu2HM7Q+oMnte5gUUHadzynxHoebha1bRikTjEh6hgdrA7RwWrRIIDZ64AqRmkCAAAwkWEY2nc8R6v3n3CUpK2HM1RoN0qs27S+nzrFhCj+76+4hkHsYgfUAEoTAABADcqz2rThULpW7z+h1ftPaM2BE6XOZFff36tYQYqPDlaIn5cJiQFQmgAAAKrRkfQ8R0FafeCENiell9iK5OXhpg6NgtWl8amCFKLoer6yWJjmG3AGlCYAAIAqUmiza+vhTK3en6bVB05qzf4TSjqZW2K9BoHe6taknro2qacuTeqpXVSQvD3YzQ5wVpQmAACASsovLNrVbuXeNP25N02r96Upu8BWbB03i9S2YZC6nipJjeuxFQlwMZQmAACAcsopKNTaAyf15940/bnnuNYdPKn8QnuxdQJ9PNSlcT1HSYqPCVGANx+5AFfGOxgAAOAM0nOtWr2/aCvSyr1p2nio5PFI9f291CM21PHVJjJI7m5sRQJqE0oTAADA3zLzrFq5N00rdh/X77uPa+uRDBn/mPk7Ktjn74JUXz1iQ9W8gT+72gG1HKUJAADUWbkFNq3ef0Irdqdqxe7j2piULts/tiQ1C/MvtiUpup6fSWkBmIXSBAAA6oyCQrvWHzqpFbuOa8XuVK09cFIFtuLHJMWG+SuheX0lNKuvns1CFR7oY1JaAM6C0gQAAGotm93QluQMx5akVfvSlPOP2e0ig3zUu0V99W4epoTm9dUoxNektACcFaUJAADUKofTc7VsR6qW7Dym33al6mSOtdj1of5eSmhW31GUmtb345gkAGWiNAEAAJeWW2DTn3uPa+mOVC3beUw7U7KKXR/g7aFezUKV0DxMvZvXV+uIQLkxux2ACqA0AQAAl2IYhrYdydTSHce0bGeqVu5LU8Fp50pys0gdo0N0YcswXdiqgeJjQuTp7mZiYgCujtIEAACc3vGsfC3bmaqlO4uK0rHM/GLXRwX76MJWDXRBywY6r0V9hfh5mZQUQG1EaQIAAE7HMAxtTs7Qz9tS9PO2FK0/dLLY+ZJ8Pd3Vq1moLmjZQBe2asC5kgBUK0oTAABwCln5hVq+85h+3paiX7cfU8o/tia1bRikPq0a6MKWYeratJ68PdxNSgqgrqE0AQAAUxiGoT2p2fplW4p+2Z6ilXvTZLX9b3OSn5e7zm8Rpv5twtW3dbgigzlfEgBzUJoAAECNsdrsWrk3TYu2HNUv21O0/3hOseub1vdTvzbh6t8mXD1iQ9maBMApUJoAAEC1ysyzasmOY0VFaVuKMvIKHdd5ulvUM7a+oyjFhvmbmBQASkdpAgAAVe5oRp4WbTmqRVuO6vfdx1Vg+9+U4PX9vTSgbbj6t4nQ+S3DFODNxxEAzo3fUgAA4JwZhqEdR7O0aMsRLdpyVOsPpRe7PjbMXwPjIpQYF6HOjevJnZPLAnAhlCYAAFApdruhtQdP6IdNR7Rwy9ESxyd1bhyixLgIDYyLUPMGAUwJDsBlUZoAAEC52eyG/tqXpu83HdH3mw7raMb/pgX38nDTec3rKzEuUhe1DVd4ELPdAagdKE0AAKBMhTa7/tybpu82HtaPm48oNavAcV2At4cGtA3Xxe0i1adVA/lzfBKAWojfbAAAoISCQrt+25Oi7zce0cItR3Qix+q4LsjHQ4lxkRrSIVLntQiTjyfTggOo3ShNAABAUlFR+mX7MX2yy02Pr/212NTg9fw8dXG7SA3u0FAJzerLy8PNxKQAULMoTQAA1GE2u6E/9hzXN+uT9f2mI0rPtUpyk1SosABvDWofoSHtG6pHbKg83ClKAOomShMAAHWMYRhac+CEvll/WN9uOKzUrP9N5tAgwEttA/I07pKe6tm8AVODA4AoTQAA1AmGYWjL4QzNX5+sb9cfVtLJXMd1wb6eGtIhUkPjo9QlOkg//vC9ujflXEoAcAqlCQCAWmz3sSzNX5esbzYka8+xbMdyfy93DWwXqaHxDXV+iwaOY5SsVuuZ7goA6ixKEwAAtUxqVr7mr0vWvLVJ2piU7lju5eGmAW3CNTQ+Sv1ah8vXi1nvAKA8KE0AANQCeVabFm05qnlrk7RkxzHZ7IYkyd3NogtahmlYfJQS4yIU6ONpclIAcD2UJgAAXJTdbmjVvjTNXZOk7zYeVmb+/6YIj48O1uWdG2lofJTqB3ibmBIAXB+lCQAAF7P7WJbmrUnSvLVJxSZ0aBTiq+Gdo3R552i1CA8wMSEA1C6UJgAAXMDJnALNX5+sL9ckaf3Bk47lgd4eGtKhoS7v0kg9mobKjRnvAKDKUZoAAHBSdruh33anas6qg1q4+agKbHZJRccp9WnVQJd3bqTEuAj5eDKhAwBUJ0oTAABO5mBajv67+pD+u/pQsd3v2jYM0tVdozU0PkoNAjlOCQBqCqUJAAAnkGe16cfNR/TFX4f02+5UGUWT3ynIx0PDOzfSNd1i1L5RsLkhAaCOojQBAGCiTUnp+vyvg/pqbZIy8v43+935LcJ0dbdoXdwukt3vAMBklCYAAGpYeq5VX61N0pxVB7XlcIZjeaMQX13VNVpXdY1WTKifiQkBAKejNAEAUAMMw9CGQ+ma9ed+zV+frDxr0aQOXu5uGtguQtd2j1Hv5mFyZ/Y7AHA6lCYAAKpRdn6hvl6XrFl/7tfm5P9tVWoVEaCRPRrr8s6NFOLnZWJCAMDZUJoAAKgGW5IzNOvP/fp6XbKy8ouOVfLycNMlHRpqVM/G6tqkniwWtioBgCugNAEAUEVyC2z6dkOyZv15QOtOOwFtszB/Xdezsa7sEq16/mxVAgBXQ2kCAOAc7T+erY9/36/P/zromAHP092ii9tF6rqejZXQrD5blQDAhVGaAACoBLvd0NKdx/TR7/v1y/YUx3mVYkJ9NbJHY13dNYYT0AJALUFpAgCgAjLyrPrvX4f08R/7tTc127G8b+sGGpPQVH1aNZAbM+ABQK1CaQIAoBx2Hs3Uh7/v09w1ScopsEmSAr09dHW3GN2Q0ESxYf4mJwQAVBdKEwAAZ2CzG/pp61F9uGKfVuw+7ljeKiJAoxOa6vLOjeTvzX+lAFDb8ZseAIB/yMyz6vO/Dmnmir06mJYrSXKzSIlxERrTuykTOwBAHUNpAgDgb4dO5OjDFfs0e+VBZf59bqV6fp4a0aOxru/VRI1CfE1OCAAwA6UJAFDnrT1wQtOX79UPm47IZi+aBq95A3+NPb+ZLu/cSL5e7iYnBACYidIEAKiTCm12LdxyVNOX7dGaAycdy89vEaaxF8SqT0tmwQMAFHGp0vT0009rwYIFWrdunby8vHTy5EmzIwEAXMyp45Vm/LZXh04UHa/k5e6mYZ2iNPb8WLVtGGRyQgCAs3Gp0lRQUKCrr75aCQkJev/9982OAwBwISmZeZrx2z598vv+Yscr3dCria5PaKLwQB+TEwIAnJVLlaYpU6ZIkmbOnGluEACAy9ibmq33lu7Rl2sOqaDQLqnoeKVbLig6XsnHk+OVAABlc6nSVFH5+fnKz893XM7IyJAkWa1WWa1Ws2I5nMrgDFngGhgzqKi6PGY2HErXf5bv049bjsoomttBXRqH6Nbzm6pf61PHK9lltdpNzels6vKYQeUwZlBRzjJmKvL4FsM49V+J65g5c6buueeesx7TNHnyZMfWqdN9+umn8vPzq6Z0AACzGIa0Ld2ixUkW7cxwcyxvV8+uAVF2NedwJQDA33JycnTdddcpPT1dQUFl/wdh+pamMxWb061atUrdunWr8H0//PDDuu+++xyXMzIyFBMTo4EDB571hakJVqtVixYtUmJiojw9Pc2OAxfAmEFF1ZUxU2iz6/vNR/WfZfu09UimJMnDzaKhHSN1y/lN1Soi0OSErqOujBlUHcYMKspZxsypvdDKw/TSNGHCBI0YMaLMdZo2bVqp+/b29pa3t3eJ5Z6enk71pna2PHB+jBlUVG0dM/mFNn25OklvL9mlg2lFM+H5eblrRPfGGntBLCejPQe1dcyg+jBmUFFmj5mKPLbppSksLExhYWFmxwAAuJA8q02zVx7Qu0v36HB6niQp1N9LN/ZuqtEJTRTi52VyQgBAbWJ6aaqIAwcOKC0tTQcOHJDNZtO6deskSS1atFBAQIC54QAA1S47v1Cz/tyv95buVWpW0UQ/kUE+uq1PM43o3li+XsyEBwCoei5Vmp544gl9+OGHjsudO3eWJP3yyy/q27evSakAANUtI8+qj1bs0/vL9+pETtFsR41CfDW+X3Nd1TVa3h6UJQBA9XGp0jRz5kzO0QQAdciJ7ALN+G2vZqzYp8y8ohPSxob5a3zf5hreuZE83d3Ocg8AAJw7lypNAIC6ITUrX/9Ztkef/L5f2QU2SVLL8ABN6N9Cl3aMkrubxeSEAIC6hNIEAHAaJ7IL9O7SPfpwxT7lWovKUlzDIN3Zv4Uubhf59wlpAQCoWZQmAIDp0nOsmr58jz5YvtexZaljdLDuHtBS/duEy2KhLAEAzENpAgCYJiPPqg+W79X7y/YqM7/omKV2UUG6L7EVZQkA4DQoTQCAGpeVX6gPV+zTe0v3KD23aDa8NpGBuueiVrq4XQRlCQDgVChNAIAak1NQqI9+3693l+x2TB3eIjxA91zUUkPaN+SYJQCAU6I0AQCqXZ7Vpk/+2K93luxWalaBpKKpw++5qCWz4QEAnB6lCQBQbWx2Q3PXHNKrP+1U0slcSVLjUD/dNaClhneKkgfnWQIAuABKEwCgyhmGoYVbjurFH7drZ0qWJCkyyEd3X9RSV3WN5qS0AACXQmkCAFSp33cf13M/bNO6gyclScG+nrqjX3ONTmgqH093c8MBAFAJlCYAQJXYlJSu53/crqU7jkmSfD3dNfb8WP3rwmYK9vU0OR0AAJVHaQIAnJN9qdl6adEOfbM+WZLk4WbRdT0ba0L/FgoP9DE5HQAA547SBAColGOZ+Xr1px2as+qgCu2GLBbpsvgo3ZvYSk3q+5sdDwCAKkNpAgBUSG6BTe8v36O3f92t7AKbJKl/m3DdP7C14qKCTE4HAEDVozQBAMrFZjc0b22SXvxxu45k5EmS4qOD9ciQturZrL7J6QAAqD6UJgDAWf22K1VPL9iqLYczJEmNQnz14OA2urRDQ7lxYloAQC1HaQIAnNGOo5ma9t1W/bK9aEa8QB8P3dm/BdOHAwDqFEoTAKCElMw8vbJop+asOiC7UTQj3vW9mujuAS1Vz9/L7HgAANQoShMAwCG3wKbpy/bonSX/m+RhULtIPTi4jWLDmBEPAFA3UZoAADIMQws2Hta077Yp6WSuJCk+JkSPXdJW3ZuGmpwOAABzUZoAoI7bnJyuKd9s0cq9aZKKJnl4YFBrDe0YxSQPAACI0gQAddbxrHy9tGiHZq8sOm7Jx9NNt/dpoVsvbCZfLyZ5AADgFEoTANQxVptdH/2+X6/+tEOZeYWSpKHxUXpocBs1CvE1OR0AAM6H0gQAdcjSHcc09dst2pWSJUlqFxWkSUPbqUcsxy0BAHAmlCYAqAP2Hc/Wcz/u1E9bUyRJ9f299O+LW+vqbjFy57glAADKRGkCgFosp6BQ3+x30/0rV8hqM+ThZtGNvZvqzgEtFezraXY8AABcAqUJAGohwzD04+ajmvrNZiWnu0ky1KdVAz1+aZxahAeYHQ8AAJdCaQKAWmZfarYmf7NZv24/JkkK9Tb0zJWdNahjI5OTAQDgmihNAFBL5FlteuvX3XpnyW4VFNrl5e6mW85vqtjcHRrQNtzseAAAuCxKEwDUAj9vO6rJ87foQFqOJOmClmGaMqydYkK89d13O0xOBwCAa6M0AYALO3QiR1O+2aJFW45KkiKDfPTE0DgNbh8pi8Uiq9VqckIAAFwfpQkAXFB+oU3Tl+3V//28U3lWuzzcLLr5/FjdNaClArz51Q4AQFXif1YAcDF/7DmuR+Zt1J5j2ZKknrGhenJ4e7WKCDQ5GQAAtROlCQBcxMmcAj3z3VZ9/tchSVJYgLceu6StLusUJYuFE9QCAFBdKE0A4OQMw9D89cl68tstSs0qkCRd17OxHhzUhhPUAgBQAyhNAODEDqbl6NGvNmnpjqJzLrUMD9AzV3RQ96ahJicDAKDuoDQBgBOy2uz6YPlevfLTDuVZ7fLycNOd/Vrotj7N5eXhZnY8AADqFEoTADiZdQdP6uG5G7X1cIYkqVezUD1zeQc1axBgcjIAAOomShMAOIns/EK9uHC7Zq7YJ8OQQvw89eiQtrqqazQTPQAAYCJKEwA4geU7U/XQ3A06dCJXknR550Z67JK2qh/gbXIyAABAaQIAE2XkWfXMgq2aveqgJKlRiK+euaKD+rRqYHIyAABwCqUJAEyyeOtRPTJvo45m5EuSRic00QOD2ijAm1/NAAA4E/5nBoAalpZdoCnfbNbX65IlSU3r++m5KzuqZ7P6JicDAACloTQBQA0xDEMLNh7WpK8363h2gdws0r8uaKZ7E1vJx9Pd7HgAAOAMKE0AUANSMvL0+Neb9OPmo5KkVhEBeuGqeMXHhJgbDAAAnBWlCQCqkWEYmr8+WU98vVnpuVZ5uFk0vl8L3dGvubw92LoEAIAroDQBQDU5npWvx7/epO82HpEktYsK0gtXxSsuKsjkZAAAoCIoTQBQDX7cfESPztuo1KwCebhZNKF/C93Rr4U83d3MjgYAACqI0gQAVSg9x6op32zW3LVJkoqOXXr5mk5q3yjY5GQAAKCyKE0AUEWW7DimB/+7QUcy8uRmkW69sLnuTWzJsUsAALg4ShMAnKOs/EI9vWCrPlt5QJIUG+avF6+OV9cm9UxOBgAAqgKlCQDOwZ97jmviF+t16ESuJOnG3k314KA28vVi6xIAALUFpQkAKqGg0K6XF+3Qu0t3yzCkRiG+euHqjurdPMzsaAAAoIpRmgCggnalZOru2eu0OTlDknRNt2g9MbSdArz5lQoAQG3E//AAUE6GYejjP/br6QVblV9oVz0/T027oqMGtY80OxoAAKhGlCYAKIeUzDz9+4sNWrLjmCTpgpZhevHqeEUE+ZicDAAAVLcKl6b09HTNmzdPy5Yt0759+5STk6MGDRqoc+fOuvjii9W7d+/qyAkAplm4+YgemrtRadkF8vJw0yOD22h0QlO5uVnMjgYAAGpAuU9Nf/jwYf3rX/9Sw4YNNXXqVGVnZ6tTp04aMGCAoqOj9csvvygxMVFxcXGaM2dOdWYGgBqRnV+oh77coFs/Xq207AK1bRikb+88XzeeF0thAgCgDin3lqb4+HiNHj1aK1euVPv27UtdJzc3V1999ZVefvllHTx4UPfff3+VBQWAmrT+4EndPXut9h3PkcUi3XpBM903sBUnqgUAoA4qd2navHmzGjRoUOY6vr6+GjlypEaOHKljx46dczgAqGl2u6H3lu3Riz9uV6HdUMNgH710TTxTiQMAUIeVuzSdrTCdYrVa5enpWe71AcBZHMvM132fr9OynamSpCEdIjXt8o4K9vM0ORkAADBTuY9pkqTRo0crIyPjjNf/9ddf6ty58zmHAoCatnTHMQ1+bamW7UyVj6ebpl3RQW9e14XCBAAAKlaaNm3apLi4OP3444/FllutVj3yyCPq3bu3zj///CoNCADVqaDQrmnfbdXoD1YqNatArSMC9c2E8zWyR2NZLEz2AAAAKjjl+MqVKzV16lQNHTpUN910k1566SVt27ZNY8aMUXZ2thYsWKDExMTqygoAVerA8Rzd+dkarT+ULkm6vldjPXZJnHw8mewBAAD8T4W2NHl4eGjq1Kn6/fff9dtvv6lVq1bq3bu3zjvvPG3cuJHCBMBlfL0uSUNeX6b1h9IV5OOhd67voqeGd6AwAQCAEip8cltJ8vb2lqenp9LT0+Xl5aXzzjtPgYGBVZ0NAKpcTkGhJn29WV+sPiRJ6taknl4b2VmNQnxNTgYAAJxVhbY0GYahadOmqVu3burUqZOSk5P1/PPPa8KECbrsssuUkpJSXTkB4JztPJqpy974TV+sPiSLRbqrfwvNvrUXhQkAAJSpQqUpISFB//d//6cvvvhCM2bMUHBwsMaPH6/169fr5MmTiouL05w5c6orKwBU2ry1hzTsjd+0MyVL4YHe+vSWXrpvYGt5uFfo1yAAAKiDKrR7XtOmTfXdd98pNDS02PJmzZrp119/1auvvqqxY8fq2muvrdKQAFBZeVabpnyzWZ+tPChJOq9Ffb16bWc1CPQ2ORkAAHAVFSpNs2fPPuN1FotF9957ry699NJzDgUAVWFvarbGz1qjrYcz/t4dr6XuGtBS7m5MJQ4AAMqvUhNBlKVly5ZVfZcAUGELNhzWg19uUFZ+oer7e+nVEZ10QcsGZscCAAAuqMpLEwCYKb/QpmcWbNWHv++XJPVoGqr/u66zIoJ8TE4GAABcFaUJQK1xMC1Hd3y6Rhv+Plnt7X2ba2JiKyZ7AAAA54TSBKBW+HnbUd0ze50y8goV4uepV67ppH5tws2OBQAAagFKEwCXZrcbem3xTr22eKckqVNMiN4c1YVzLwEAgCpDaQLgstJzrLpnzlr9sv2YJGl0QhM9dkmcvDzYHQ8AAFSdcn+ycHNzk7u7e4W/pk6dWp35AdRRW5IzNPSN5fpl+zF5e7jppavjNfWy9hQmAABQ5cq9pWnv3r2VeoCQkJBK3Q4AzmTe2kN6eO5G5Vntign11TvXd1W7qGCzYwEAgFqq3KWpSZMm1ZkDAM6qoNCupxdscUwn3qdVA702opNC/LxMTgYAAGozjmkC4BKOZuRp/Kw1Wr3/hCTprv4tdPdFreTuZjE5GQAAqO0oTQCc3qp9aRo/a42OZeYr0MdDr1zTSRfFRZgdCwAA1BGUJgBO7eM/9mvK/M0qtBtqHRGod2/oqqZh/mbHAgAAdQilCYBTKii0a/I3m/XpnwckSUPjo/TclR3k58WvLQAAULP49AHA6aRm5ev2T1Zr1b4TslikBwe10W0XNpPFwvFLAACg5lWoNKWlpendd9/VokWLtGfPHuXk5MjPz0/NmjVTYmKibrvtNoWGhlZXVgB1wKakdN360V9KTs9ToLeHXh/ZWf3ahJsdCwAA1GHlPgvk2rVr1aZNG82aNUsdO3bU/fffr8zMTI0dO1b9+/fXDz/8oLZt22rdunXVGBdAbfbN+mRd9c4KJafnqVmYv76acB6FCQAAmK7cW5puv/12jRkzRi+88IJj2cMPP6xRo0apWbNmeuyxxzRt2jTddttt+vPPP6slLIDayWY39NLC7Xrr192SpL6tG+i1EZ0V7OtpcjIAAIAKbGnasGGDJkyYUOY6o0aN0saNG885VGn27dunsWPHKjY2Vr6+vmrevLkmTZqkgoKCank8ADUjI8+qf330l6Mw3danmd4f053CBAAAnEa5tzS1bt1as2fP1oMPPnjGdb7//nu1bt26SoL907Zt22S32/Xuu++qRYsW2rRpk/71r38pOztbL774YrU8JoDqtTc1W7d8uEq7j2XL28NNz1/VUZd1amR2LAAAgGLKXZpef/11XXrppfr66681ZMgQtWzZUna7XQsXLpSHh4eWLl2qBQsWaP78+dUSdNCgQRo0aJDjcrNmzbR9+3a9/fbblCbABf22K1W3f7JaGXmFahjso/du6KYO0cFmxwIAACih3KXpggsu0KZNm/Taa6/pv//9r/bs2aP8/Hw98MADjtnz1q1bp5iYmOrMW0x6enqZs/Xl5+crPz/fcTkjI0OSZLVaZbVaqz3f2ZzK4AxZ4Bpqy5iZ89chTf5mqwrthjrHBOvNkZ3UINDb5Z+XM6otYwY1hzGDimLMoKKcZcxU5PEthmEY1Zil2uzevVtdunTRSy+9pFtuuaXUdSZPnqwpU6aUWP7pp5/Kz8+vuiMC+Ae7IX29302/Hi46nLJbmF0jmtvlWe6jKwEAAKpGTk6OrrvuOqWnpysoKKjMdU0vTWcqNqdbtWqVunXr5ricnJysPn36qE+fPpo+ffoZb1falqaYmBilpqae9YWpCVarVYsWLVJiYqI8PTnoHWfnymMmO79Q932xUT9vPyZJumdAC43vE8sJa6uZK48ZmIMxg4pizKCinGXMZGRkKCwsrFylqUInt60OEyZM0IgRI8pcp2nTpo7vk5OT1a9fPyUkJOi9994r83be3t7y9vYusdzT09Op3tTOlgfOz9XGTPLJXI398C9tPZwhbw83vXRNvC7tGGV2rDrF1cYMzMeYQUUxZlBRZo+Zijy26aUpLCxMYWFh5Vo3KSlJ/fr1U9euXTVjxgy5ubFPD+Ds1h08qX999JeOZeYrLMBb08d0U6eYELNjAQAAlJvppam8kpOT1bdvXzVu3Fgvvviijh075rguMjLSxGQAzmTBhsO67/N1yi+0q01koN6/sbsahfiaHQsAAKBCXKY0LVy4ULt27dKuXbsUHR1d7DoXncsCqLUMw9Cbv+zSiwt3SJL6twnX6yM7K8DbZX7lAAAAOFT5/m1Tp07V0qVLq/pudeONN8owjFK/ADiPgkK7Jn6+3lGYxp4fq/+M7kZhAgAALqvKS9OMGTM0aNAgDR06tKrvGoCTS8+1aswHKzV3bZLc3Sx6anh7PX5pnNzdmCEPAAC4rir/0+/evXuVl5enJUuWVPVdA3BiSSdzddOMldpxNEv+Xu566/qu6tOqgdmxAAAAzlm17C/j4+Ojiy++uDruGoAT2pycrptmrFJKZr4igrz1wY3d1S4q2OxYAAAAVaJSu+c1bdpUU6dO1YEDB6o6DwAXs2THMV3zzu9KycxX64hAzRt/HoUJAADUKpUqTRMnTtTXX3+tZs2aKTExUbNnz1Z+fn5VZwPg5OasOqCbZ65SdoFNvZvX1+fjEhTFlOIAAKCWqVRpuvPOO7V69WqtXr1acXFxuuuuu9SwYUNNmDBBa9asqeqMAJyMYRh6eeF2PfjlRtnshq7o3Egzb+qhYF/OBA8AAGqfc5o9Lz4+Xq+99pqSkpI0adIkTZ8+Xd27d1d8fLw++OADpgMHaqGCQrsmfrFer/+8S5J0Z/8WeumaeHl5VPlknAAAAE7hnCaCsFqtmjdvnmbMmKFFixapV69eGjt2rJKTk/Xoo4/qp59+0qefflpVWQGYLCPPqvGfrNHyXamOKcVH9mhsdiwAAIBqVanStGbNGs2YMUOfffaZ3N3ddcMNN+iVV15RmzZtHOsMHDhQF154YZUFBWCulIw8jf5gpbYdyZSfl7veHNVF/VqHmx0LAACg2lWqNHXv3l2JiYl6++23NXz4cHl6ljyOIS4uTiNGjDjngADMtzc1Wze8/6cOnchVWIC3Zt7UXe0bMUMeAACoGypVmvbs2aMmTZqUuY6/v79mzJhRqVAAnMfGQ+m6ccZKHc8uUNP6fvro5p5qXN/P7FgAAAA1plKl6WyFSSqaXctisVTm7gE4ieU7U3Xbx38pu8Cm9o2CNOPGHmoQ6G12LAAAgBpV7umu2rZtq08//VQFBQVlrrdz507dfvvteu655845HADzfLshWTfNXOk4B9Nn/+pFYQIAAHVSubc0vfnmm3rwwQd1xx13aODAgerWrZuioqLk4+OjEydOaMuWLVq+fLm2bNmiCRMmaPz48dWZG0A1+uj3fZo0f7MMQ7qkQ0O9fG28vD3czY4FAABginKXpv79+2vVqlVasWKF5syZo08//VT79u1Tbm6uwsLC1LlzZ40ePVrXX3+9QkJCqjEygOpiGIZeWbTDcQ6mG3o10eRh7eTuxq62AACg7qrwMU29e/dW7969qyMLABPZ7IYe+2qTPlt5QJJ070WtdNeAFhybCAAA6rwKlybDMLRr1y5ZrVa1atVKHh7ndH5cAE4gz2rTPbPX6YfNR2SxSE9e1l7X9zr7hC8AAAB1QbkngpCkffv2qVOnTmrTpo06dOigFi1aaPXq1dWVDUANyMov1E0zVumHzUfk5e6mt67rQmECAAA4TYVK04MPPqi8vDx9/PHH+uKLL9SwYUPdfvvt1ZUNQDU7mVOg66f/qd/3HJe/l7tm3txdgzs0NDsWAACAU6nQvnXLli3TZ599pj59+kiSevTooSZNmig3N1e+vr7VEhBA9TiWma8b3v9T245kKsTPUx/e1EPxMSFmxwIAAHA6FdrSdOTIEbVp08ZxOTo6Wr6+vjp69GiVBwNQfZJO5uqad3/XtiOZahDorTm3JlCYAAAAzqBCW5osFovc3Ir3LDc3NxmGUaWhAFSfvanZun76n0o6matGIb6adUtPNQ3zNzsWAACA06pQaTIMQ61atSo2BXFWVpY6d+5crEylpaVVXUIAVWbbkQxdP32lUrPy1SzMX5/c0lNRIexaCwAAUJYKlaYZM2ZUVw4A1WzdwZMa88FKpeda1bZhkD4e20NhAd5mxwIAAHB6FSpNY8aMqa4cAKrR77uP65YPVym7wKYujUM048YeCvbzNDsWAACAS+DMtEAt98u2FI37ZLXyC+06r0V9vXdDN/l789YHAAAoLz45AbXYdxsP667P1qrQbuiituF647ou8vF0NzsWAACAS6E0AbXU1+uSdO+cdbIb0rD4KL10Tbw83St0lgEAAACI0gTUSl+uPqR//3e97IZ0dddoPXtlR7m7Wc5+QwAAAJRQqT87T506VTk5OSWW5+bmaurUqeccCkDlzVl1QPf/XZhG9mis5yhMAAAA56RSpWnKlCnKysoqsTwnJ0dTpkw551AAKueTP/brwS83yjCk0QlN9Mzl7eVGYQIAADgnldo9zzCMYie4PWX9+vUKDQ0951AAKm7mb3s1+ZstkqSbz4vV45e2LfV9CgAAgIqpUGmqV6+eLBaLLBaLWrVqVewDmc1mU1ZWlsaNG1flIQGUbfqyPXpqwVZJ0m19mumhQW0oTAAAAFWkQqXp1VdflWEYuvnmmzVlyhQFBwc7rvPy8lLTpk2VkJBQ5SEBnNnbv+7Wcz9skyRN6NdCEwe2ojABAABUoQqVpjFjxkiSYmNj1bt3b3l6elZLKADl83+Ld+qlRTskSfde1Ep3X9TS5EQAAAC1T6WOaerTp4/sdrt27NihlJQU2e32YtdfeOGFVRIOwJmdXpj+fXFr3dGvhcmJAAAAaqdKlaY//vhD1113nfbv3y/DMIpdZ7FYZLPZqiQcgNK9s2SPXvpplyTpocFtNK5Pc5MTAQAA1F6VKk3jxo1Tt27dtGDBAjVs2JDjJ4AatDjJovkHigrTA4NaU5gAAACqWaVK086dO/Xf//5XLVqwOxBQk97/bZ/mH3CXJN0/sJXG9+U9CAAAUN0qdXLbnj17ateuXVWdBUAZpi/bo2d/KDqG6a7+zTWhP5M+AAAA1IRKbWm68847NXHiRB05ckQdOnQoMYtex44dqyQcgCIzftvrOA/TxdF23dmPXfIAAABqSqVK05VXXilJuvnmmx3LLBaLDMNgIgigin30+z5N+WaLJOn2PrFqnb/T5EQAAAB1S6VK0969e6s6B4BSfPzHfj3x9WZJ0u19m+ve/s30/feUJgAAgJpUqdLUpEmTqs4B4B9mrzygx7/aJEm67cJmeuDi1iosLDQ5FQAAQN1T7tI0f/58DR48WJ6enpo/f36Z6w4bNuycgwF12by1h/TwvI2SpFvOj9VDg9swtT8AAIBJyl2ahg8friNHjig8PFzDhw8/43oc0wScmx82Hdb9X2yQYUijE5ro0UvaUpgAAABMVO7SZLfbS/0eQNX5ZXuK7vxsrWx2Q1d1jdbkoe0oTAAAACar1HmaAFS933cf17iPV8tqM3RJx4Z67sqOcnOjMAEAAJit0qVpyZIlGjp0qFq0aKGWLVtq2LBhWrZsWVVmA+qM1ftPaOyHq5RfaNdFbcP16rWd5E5hAgAAcAqVKk2ffPKJLrroIvn5+emuu+7ShAkT5OvrqwEDBujTTz+t6oxArbYpKV03zlipnAKbLmgZpjeu6yJPdzYCAwAAOItKTTn+9NNP6/nnn9e9997rWHb33Xfr5Zdf1pNPPqnrrruuygICtdmOo5m64f0/lZlXqO5N6+ndG7rKx9Pd7FgAAAA4TaX+nL1nzx4NHTq0xPJhw4Zx4lugnPalZmvU9D91IseqjtHB+uDG7vLzqtTfMQAAAFCNKlWaYmJitHjx4hLLFy9erJiYmHMOBdR2ySdzNWr6nzqWma82kYH66OYeCvTxNDsWAAAASlGpP2tPnDhRd911l9atW6fevXvLYrFo+fLlmjlzpl577bWqzgjUKmnZBbrh/T+VdDJXzcL89fHYngrx8zI7FgAAAM6gUqXp9ttvV2RkpF566SV9/vnnkqS2bdtqzpw5uuyyy6o0IFCbZOUX6sYZK7X7WLYaBvvo41t6qkGgt9mxAAAAUIZKH0Bx+eWX6/LLL6/KLECtlme16daP/tKGQ+mq5+epj8f2UKMQX7NjAQAA4CyY1xioAYU2u+6evVYrdh+Xv5e7Pry5h1qEB5odCwAAAOVQoS1N9erVk8VS9gk3PTw8FBkZqcTERD3++OMKCQk5l3yAyzMMQ4/M26gfNx+Vl7ub/jO6mzpGh5gdCwAAAOVUodL06quvnnUdu92ulJQUzZgxQ8nJyfrss88qmw2oFZ79fps+/+uQ3CzS6yM7q3eLMLMjAQAAoAIqVJrGjBlT7nUTExOVmJhY4UBAbfL2r7v17tI9kqRnr+ioQe0jTU4EAACAiqq2Y5ratm2rJ554orruHnB6n608oOd+2CZJemRIG13TnXOYAQAAuKJqK02+vr66++67q+vuAaf2w6bDenTeRknS7X2b69YLm5ucCAAAAJXF7HlAFVu5N013zV4nuyGN7BGjBy5ubXYkAAAAnANKE1CFdhzN1C0frlJBoV2JcRF6aniHs844CQAAAOdGaQKqSPLJXI35YKUy8grVtUk9/d/IznJ3ozABAAC4ugrNnnfKfffdV+pyi8UiHx8ftWjRQpdddplCQ0PPKRzgKtJzrbpxxkodTs9T8wb+en9MN/l4upsdCwAAAFWgUqVp7dq1WrNmjWw2m1q3bi3DMLRz5065u7urTZs2euuttzRx4kQtX75ccXFxVZ0ZcCp5Vpv+9dFf2nE0SxFB3vrw5h4K8fMyOxYAAACqSKV2z7vssst00UUXKTk5WatXr9aaNWuUlJSkxMREjRw5UklJSbrwwgt17733VnVewKnY7Ibu+3ydVu5NU6C3h2be1EPR9fzMjgUAAIAqVKnS9MILL+jJJ59UUFCQY1lQUJAmT56s559/Xn5+fnriiSe0evXqKgsKOBvDMPTkt1v03cYj8nJ307uju6ptw6Cz3xAAAAAupVKlKT09XSkpKSWWHzt2TBkZGZKkkJAQFRQUnFs6wIm9s2SPZq7YJ0l66Zp49W4eZm4gAAAAVItK75538803a968eTp06JCSkpI0b948jR07VsOHD5ckrVy5Uq1atarKrIDTmLvmkJ77YZsk6fFL4zQ0PsrkRAAAAKgulZoI4t1339W9996rESNGqLCwsOiOPDw0ZswYvfLKK5KkNm3aaPr06VWXFHASK3al6oH/bpAk3XphM409P9bkRAAAAKhOlSpNAQEB+s9//qNXXnlFe/bskWEYat68uQICAhzrdOrUqaoyAk5j59FM3fbJahXaDQ2Nj9JDg9qYHQkAAADVrFKl6ZSAgACFhobKYrEUK0xAbXQsM183zVylzLxCdWtSTy9c1VFunLwWAACg1qvUMU12u11Tp05VcHCwmjRposaNGyskJERPPvmk7HZ7VWcETJdbYNMtH67SoRO5alrfT++N5uS1AAAAdUWltjQ9+uijev/99/Xss8/qvPPOk2EY+u233zR58mTl5eXp6aefruqcgGlsdkP3zFmr9YfSVc/PUzNu6qFQf05eCwAAUFdUqjR9+OGHmj59uoYNG+ZYFh8fr0aNGmn8+PGUJtQq077bqh83H5WXu5veG91NsWH+ZkcCAABADarU7nlpaWlq06bkAfBt2rRRWlraOYcCnMVHv+/T9OV7JUkvXhOv7k1DTU4EAACAmlap0hQfH6833nijxPI33nhD8fHx5xwKcAaLtx7V5PmbJUn/vri1hnEuJgAAgDqpUrvnPf/887rkkkv0008/KSEhQRaLRStWrNDBgwf13XffVXVGoMZtSkrXnZ+tld2Qru0Wo/F9m5sdCQAAACap1JamPn36aMeOHbr88st18uRJpaWl6YorrtD27dt1wQUXVHVGoEYdTs/VzTNXKafApgtahumpy9vLYmFqcQAAgLqq0udpioqKKjHhw8GDB3XzzTfrgw8+OOdggBlyCgo1duZfSsnMV+uIQL05qos83Sv1twUAAADUElX6aTAtLU0ffvhhVd4lUGPsdkP3zlmnLYczFBbgpfdv7KYgH0+zYwEAAMBk/Akd+NvLi3Y4phZ/94auiq7nZ3YkAAAAOAFKEyDpq7VJeuOXXZKkZ6/soK5NmFocAAAARShNqPPWHDihB77cIEm6vW9zXdEl2uREAAAAcCYVmgjiiiuuKPP6kydPnksWoMYlnczVrR+tVkGhXQPjIvTvga3NjgQAAAAnU6HSFBwcfNbrR48efU6BgJqSnV+oWz78S6lZ+WrbMEivXNtJbm5MLQ4AAIDiKlSaZsyYUV05ymXYsGFat26dUlJSVK9ePV100UV67rnnFBUVZWouuJ5TM+Vt/XumvOljusnfu9Iz8AMAAKAWc6ljmvr166fPP/9c27dv15dffqndu3frqquuMjsWXNCLC7dr4ZZTM+V1U6MQX7MjAQAAwEm51J/W7733Xsf3TZo00UMPPaThw4fLarXK05Pz6aB85q45pLd+3S1Jeu6qDurapJ7JiQAAAODMXKo0nS4tLU2zZs1S7969z1iY8vPzlZ+f77ickZEhSbJarbJarTWSsyynMjhDlrpi/aF0PTR3oyRp3IWxurR9hEu9/owZVBRjBhXFmEFFMWZQUc4yZiry+BbDMIxqzFLlHnzwQb3xxhvKyclRr1699O2336p+/fqlrjt58mRNmTKlxPJPP/1Ufn6cuLSuySiQXtzorvQCi9rXs2tsa7uY9wEAAKBuysnJ0XXXXaf09HQFBQWVua7ppelMxeZ0q1atUrdu3SRJqampSktL0/79+zVlyhQFBwfr22+/lcVS8tNvaVuaYmJilJqaetYXpiZYrVYtWrRIiYmJ7F5YzQoK7bphxl9ac+Ckmjfw1xe39lSgj+ttaGXMoKIYM6goxgwqijGDinKWMZORkaGwsLBylSbTPzVOmDBBI0aMKHOdpk2bOr4PCwtTWFiYWrVqpbZt2yomJkZ//PGHEhISStzO29tb3t7eJZZ7eno61Zva2fLURpO+3ag1B04q0MdD/xndTaGBrj3xA2MGFcWYQUUxZlBRjBlUlNljpiKPbXppOlWCKuPURrLTtyYB/zTrz/369M8Dslik10d0VrMGAWZHAgAAgAsxvTSV18qVK7Vy5Uqdf/75qlevnvbs2aMnnnhCzZs3L3UrEyBJf+1L0+T5myVJ9w9srX5twk1OBAAAAFfjMudp8vX11dy5czVgwAC1bt1aN998s9q3b68lS5aUugsecDg9V+M+WSOrzdAlHRpqfN/mZkcCAACAC3KZLU0dOnTQzz//bHYMuIg8q03jPl6t1Kx8tYkM1AtXdyx1shAAAADgbFxmSxNQXoZh6NF5m7T+ULpC/Dz13g3d5OflMn8fAAAAgJOhNKHW+XDFPn255pDcLNIbI7uocX3OyQUAAIDKozShVvljz3E9uWCrJOmRIW11fsvKzcwIAAAAnEJpQq1xJD1PEz5dI5vd0PBOURp7fqzZkQAAAFALUJpQKxQU2nXHp2uUmlWgNpGBmnYFEz8AAACgalCaUCs8891Wrd5/QoE+Hnr3hq7y9XI3OxIAAABqCUoTXN7X65I0c8U+SdIr13RSk/r+5gYCAABArUJpgkvbfiRTD325UZJ0R7/muiguwuREAAAAqG0oTXBZmXlW3f7JauVabTq/RZjuS2xtdiQAAADUQpQmuCTDMPTvLzZoT2q2ooJ99PrIznJ3Y+IHAAAAVD1KE1zSe0v36IfNR+Tl7qa3ru+qUH8vsyMBAACglqI0weWs2J2q537YJkl6YmicOsWEmBsIAAAAtRqlCS7lSHqe7vpsreyGdGWXaI3q2djsSAAAAKjlKE1wGVbb/05g27ZhkJ4a3p4T2AIAAKDaUZrgMl78cbvjBLbvXN+FE9gCAACgRlCa4BIWbz2qd5fukSS9cFU8J7AFAABAjaE0weklnczVxC/WS5JuOq+pBrWPNDkRAAAA6hJKE5ya1WbXhE/X6GSOVfHRwXp4cFuzIwEAAKCOoTTBqb3w43atPXBSQT4eeuO6LvLyYMgCAACgZvEJFE7rpy1H9d6p45iujldMqJ/JiQAAAFAXUZrglA6dyHEcx3TzebG6uB3HMQEAAMAclCY4nYJCuyZ8ulbpuVbFx4ToocFtzI4EAACAOozSBKfzwo/btO7g38cxjezMcUwAAAAwFZ9G4VQWbTmq/yzbK0l6keOYAAAA4AQoTXAah07kaOLn6yRJY8+P1UCOYwIAAIAToDTBKRTa7Lp79jpl5BWqU0yIHhzEcUwAAABwDpQmOIXXf96l1ftPKNDbQ//HcUwAAABwInwyhen+2HNcb/y8U5L0zBUdOI4JAAAAToXSBFOdzCnQvXPWyW5IV3eN1tD4KLMjAQAAAMVQmmAawzD04JcbdDg9T83C/DV5WDuzIwEAAAAlUJpgmll/HtCPm4/Ky91Nr4/sLH9vD7MjAQAAACVQmmCK7Ucy9eS3WyRJDwxqrfaNgk1OBAAAAJSO0oQal2e16c7P1ii/0K6+rRvo5vNizY4EAAAAnBGlCTXu6QVbteNolsICvPXi1fFyc7OYHQkAAAA4I0oTatSPm4/o4z/2S5JeviZeYQHeJicCAAAAykZpQo05nJ6rB7/cIEm67cJmurBVA5MTAQAAAGdHaUKNsNkN3TtnnU7mWNUxOlgTB7Y2OxIAAABQLpQm1Ijpy/bojz1p8vNy1+sjOsvLg6EHAAAA18AnV1S7LckZenHhdknSpKFxahrmb3IiAAAAoPwoTahWeVab7pmzVlaboYFxEbqmW4zZkQAAAIAKoTShWj3/w3bH9OLTruggi4XpxQEAAOBaKE2oNst2HtMHv+2VJL1wVUfVZ3pxAAAAuCBKE6rFyZwC3f/FeknS9b0aq1+bcJMTAQAAAJVDaUKVMwxDj87bpKMZ+WrWwF+PDokzOxIAAABQaZQmVLl5a5O0YONhebhZ9Oq1neTr5W52JAAAAKDSKE2oUgfTcvTE15slSXcPaKmO0SHmBgIAAADOEaUJVcZmNzTx8/XKyi9U1yb1dHvf5mZHAgAAAM4ZpQlV5t2lu7VyX5r8vdz1yjWd5OHO8AIAAIDr41MtqsTm5HS9smiHJGnS0HZqXN/P5EQAAABA1aA04ZzlF9o08fP1stoMDYyL0NXdos2OBAAAAFQZShPO2euLd2rbkUyF+nvpmSs6yGKxmB0JAAAAqDKUJpyTtQdO6O1fd0uSnh7eXmEB3iYnAgAAAKoWpQmVlme1aeIX62U3pGHxURrcoaHZkQAAAIAqR2lCpb3443btOZatBoHemnpZO7PjAAAAANWC0oRKWbk3Te//tleS9NyVHRTi52VyIgAAAKB6UJpQYdn5hbr/i/UyDOmabtHq3ybC7EgAAABAtaE0ocKmfb9VB9JyFBXso8cujTM7DgAAAFCtKE2okOU7U/XJHwckSc9fFa8gH0+TEwEAAADVi9KEcsvIs+qB/66XJN3Qq4nObxlmciIAAACg+lGaUG5PfrNFyel5ahzqp4cGtzE7DgAAAFAjKE0ol8Vbj+qL1YdksUgvXh0vf28PsyMBAAAANYLShLNKz7Xq4bkbJUljz4tVj9hQkxMBAAAANYfShLN6esEWpWTmKzbMX/df3NrsOAAAAECNojShTMt2HtPnfxXtlvf8VR3l4+ludiQAAACgRlGacEZZ+YV66Mui3fLGJDRV96bslgcAAIC6h9KEM3r+h21KOpmr6Hq++je75QEAAKCOojShVCv3pumj3/dLkp69oiOz5QEAAKDOojShhNwCm+MktiO6x3ASWwAAANRplCaU8MpPO7TveI4igrz1yCVtzY4DAAAAmIrShGLWHTyp6cv2SJKeubyDgnw8TU4EAAAAmIvSBIf8wqLd8uyGdFmnKA1oG2F2JAAAAMB0lCY4vPnzLu04mqX6/l6aNLSd2XEAAAAAp0BpgiRpS3KG3vp1tyRp6mXtFervZXIiAAAAwDlQmqBCm10PfLlehXZDF7eL0JAOkWZHAgAAAJwGpQl6f/lebUrKULCvp568rL0sFovZkQAAAACnQWmq4w4cz9ErP+2QJD16SVuFB/mYnAgAAABwLpSmOswwDD361UblWe1KaFZfV3eNNjsSAAAA4HQoTXXYV+uStGxnqrw83PTMFR3YLQ8AAAAoBaWpjkrLLtCT326VJN09oKViw/xNTgQAAAA4J0pTHfXUgi1Kyy5Q64hA3XphM7PjAAAAAE6L0lQHLd+ZqrlrkmSxSM9e2UGe7gwDAAAA4Ez4tFzH5BbY9Mi8jZKkMQlN1blxPZMTAQAAAM6N0lTHvLZ4pw6k5ahhsI/uv7i12XEAAAAAp0dpqkO2JGfoP8v2SJKevKy9Arw9TE4EAAAAOD9KUx1hsxt6eO4G2eyGhnSI1EVxEWZHAgAAAFwCpamO+HDFPq0/lK5AHw9NHtrO7DgAAACAy6A01QFJJ3P14sLtkqSHB7dVeJCPyYkAAAAA1+GSpSk/P1+dOnWSxWLRunXrzI7j1AzD0KSvNymnwKYeTUM1onuM2ZEAAAAAl+KSpemBBx5QVFSU2TFcwsItR/XT1hR5ulv0zBXt5eZmMTsSAAAA4FJcrjR9//33WrhwoV588UWzozi97PxCTZ6/WZJ024XN1SI80OREAAAAgOtxqTmnjx49qn/961/66quv5Ofnd9b18/PzlZ+f77ickZEhSbJarbJardWWs7xOZaiuLC8v3K7D6XmKruer2y5o4hTPGeemuscMah/GDCqKMYOKYsygopxlzFTk8S2GYRjVmKXKGIahIUOG6LzzztNjjz2mffv2KTY2VmvXrlWnTp1Kvc3kyZM1ZcqUEss//fTTcpUuV5aULb24wV12WXRrG5va1XOJHzMAAABQI3JycnTdddcpPT1dQUFBZa5remk6U7E53apVq7RixQrNmTNHS5culbu7e7lKU2lbmmJiYpSamnrWF6YmWK1WLVq0SImJifL09Kyy+7XbDY2YvlJrD6br4rhwvTGyU5XdN8xVXWMGtRdjBhXFmEFFMWZQUc4yZjIyMhQWFlau0mT67nkTJkzQiBEjylynadOmeuqpp/THH3/I29u72HXdunXTqFGj9OGHH5a4nbe3d4n1JcnT09Op3tRVnWf2ygNaezBd/l7umnxZe6d6rqgazjaG4fwYM6goxgwqijGDijJ7zFTksU0vTWFhYQoLCzvreq+//rqeeuopx+Xk5GRdfPHFmjNnjnr27FmdEV3K8ax8Tft+myTpvoGt1TDY1+REAAAAgGszvTSVV+PGjYtdDggIkCQ1b95c0dHRZkRyStO+36b0XKviGgZpTEITs+MAAAAALs/lphzHmf2557j+u/qQLBbp6cvby8OdHy8AAABwrlxmS9M/NW3aVC4y8V+NKCi067GvNkmSRvZorM6N65mcCAAAAKgd2BRRS0xfvkc7U7JU399LD17cxuw4AAAAQK1BaaoFDqbl6PXFOyVJj17SVsF+zFwDAAAAVBVKk4szDEOT5m9WntWuXs1CdXnnRmZHAgAAAGoVSpOLW7TlqH7eliJPd4ueGt5BFovF7EgAAABArUJpcmF5VpumfrtFkvSvC5qpRXiAyYkAAACA2ofS5MLe+nW3Dp3IVVSwjyb0b2F2HAAAAKBWojS5qP3Hs/XOkt2SpMcujZOfl8vOHg8AAAA4NUqTi5ryzRYVFNp1QcswDW4faXYcAAAAoNaiNLmgn06b/GHysHZM/gAAAABUI0qTi8mz2jTl282SpLHnN1PzBkz+AAAAAFQnSpOLeWfJbh1My1XDYB/dyeQPAAAAQLWjNLmQg2k5evvXoskfHr2krfy9mfwBAAAAqG6UJhcy5Zstyi+0q3fz+rqkQ0Oz4wAAAAB1AqXJRfy87ah+2npUHm4WTb2MyR8AAACAmkJpcgF5Vpsmz98iSRp7fqxahAeanAgAAACoOyhNLuC9pXt0IC1HEUHeunNAS7PjAAAAAHUKpcnJHUzL0Zu/7JIkPXpJnAKY/AEAAACoUZQmJ/fUgqLJHxKa1dfQjkz+AAAAANQ0SpMT+21Xqn7cfFTubhZNHsbkDwAAAIAZKE1OqtBm15RvNkuSbujVRK0jmfwBAAAAMAOlyUl98sd+7TiapXp+nrr3olZmxwEAAADqLEqTE0rLLtDLi3ZIkiYObK1gP0+TEwEAAAB1F6XJCb20cLsy8grVtmGQRvZobHYcAAAAoE6jNDmZLckZ+mzlAUnS5KFxcndj8gcAAADATJQmJ2IYhiZ/s1l2Q7qkY0P1bFbf7EgAAABAnUdpciILNh7Wyr1p8vF00yND2podBwAAAIAoTU4jt8CmZxZslSSN69NcjUJ8TU4EAAAAQKI0OY13luxWcnqeGoX46rYLm5sdBwAAAMDfKE1OIOlkrt5ZsluS9MiQtvL1cjc5EQAAAIBTKE1O4Lkfdii/0K6esaEa0iHS7DgAAAAATuNhdoC6bme6Rd9vOSo3izR5WDtZLEwxDgAAADgTtjSZqNBm19x9RT+C63o2VtuGQSYnAgAAAPBPlCYTzVmdpOQci4J9PTQxsbXZcQAAAACUgtJkkpM5BXr1p12SpLv7t1A9fy+TEwEAAAAoDaXJJK8s2qGTuVZF+hoa2T3a7DgAAAAAzoDSZJLusaGKDPLWFbF2ebjzYwAAAACcFZ/WTXJpxyj9dO8Fah1smB0FAAAAQBkoTSby9uDlBwAAAJwdn9oBAAAAoAyUJgAAAAAoA6UJAAAAAMpAaQIAAACAMlCaAAAAAKAMlCYAAAAAKAOlCQAAAADKQGkCAAAAgDJQmgAAAACgDJQmAAAAACgDpQkAAAAAykBpAgAAAIAyUJoAAAAAoAyUJgAAAAAoA6UJAAAAAMpAaQIAAACAMlCaAAAAAKAMHmYHqEmGYUiSMjIyTE5SxGq1KicnRxkZGfL09DQ7DlwAYwYVxZhBRTFmUFGMGVSUs4yZU53gVEcoS50qTZmZmZKkmJgYk5MAAAAAcAaZmZkKDg4ucx2LUZ5qVUvY7XYlJycrMDBQFovF7DjKyMhQTEyMDh48qKCgILPjwAUwZlBRjBlUFGMGFcWYQUU5y5gxDEOZmZmKioqSm1vZRy3VqS1Nbm5uio6ONjtGCUFBQfySQYUwZlBRjBlUFGMGFcWYQUU5w5g52xamU5gIAgAAAADKQGkCAAAAgDJQmkzk7e2tSZMmydvb2+wocBGMGVQUYwYVxZhBRTFmUFGuOGbq1EQQAAAAAFBRbGkCAAAAgDJQmgAAAACgDJQmAAAAACgDpQkAAAAAykBpMslbb72l2NhY+fj4qGvXrlq2bJnZkWCSpUuXaujQoYqKipLFYtFXX31V7HrDMDR58mRFRUXJ19dXffv21ebNm4utk5+frzvvvFNhYWHy9/fXsGHDdOjQoRp8FqhJ06ZNU/fu3RUYGKjw8HANHz5c27dvL7YO4wane/vtt9WxY0fHiSQTEhL0/fffO65nvOBspk2bJovFonvuucexjHGD002ePFkWi6XYV2RkpON6Vx8vlCYTzJkzR/fcc48effRRrV27VhdccIEGDx6sAwcOmB0NJsjOzlZ8fLzeeOONUq9//vnn9fLLL+uNN97QqlWrFBkZqcTERGVmZjrWueeeezRv3jzNnj1by5cvV1ZWli699FLZbLaaehqoQUuWLNEdd9yhP/74Q4sWLVJhYaEGDhyo7OxsxzqMG5wuOjpazz77rP766y/99ddf6t+/vy677DLHBxbGC8qyatUqvffee+rYsWOx5Ywb/FO7du10+PBhx9fGjRsd17n8eDFQ43r06GGMGzeu2LI2bdoYDz30kEmJ4CwkGfPmzXNcttvtRmRkpPHss886luXl5RnBwcHGO++8YxiGYZw8edLw9PQ0Zs+e7VgnKSnJcHNzM3744Ycayw7zpKSkGJKMJUuWGIbBuEH51KtXz5g+fTrjBWXKzMw0WrZsaSxatMjo06ePcffddxuGwe8ZlDRp0iQjPj6+1Otqw3hhS1MNKygo0OrVqzVw4MBiywcOHKgVK1aYlArOau/evTpy5Eix8eLt7a0+ffo4xsvq1atltVqLrRMVFaX27dszpuqI9PR0SVJoaKgkxg3KZrPZNHv2bGVnZyshIYHxgjLdcccduuSSS3TRRRcVW864QWl27typqKgoxcbGasSIEdqzZ4+k2jFePMwOUNekpqbKZrMpIiKi2PKIiAgdOXLEpFRwVqfGRGnjZf/+/Y51vLy8VK9evRLrMKZqP8MwdN999+n8889X+/btJTFuULqNGzcqISFBeXl5CggI0Lx58xQXF+f4MMJ4wT/Nnj1ba9as0apVq0pcx+8Z/FPPnj310UcfqVWrVjp69Kieeuop9e7dW5s3b64V44XSZBKLxVLssmEYJZYBp1RmvDCm6oYJEyZow4YNWr58eYnrGDc4XevWrbVu3TqdPHlSX375pcaMGaMlS5Y4rme84HQHDx7U3XffrYULF8rHx+eM6zFucMrgwYMd33fo0EEJCQlq3ry5PvzwQ/Xq1UuSa48Xds+rYWFhYXJ3dy/RmFNSUkq0b+DUrDNljZfIyEgVFBToxIkTZ1wHtdOdd96p+fPn65dfflF0dLRjOeMGpfHy8lKLFi3UrVs3TZs2TfHx8XrttdcYLyjV6tWrlZKSoq5du8rDw0MeHh5asmSJXn/9dXl4eDh+7owbnIm/v786dOignTt31orfM5SmGubl5aWuXbtq0aJFxZYvWrRIvXv3NikVnFVsbKwiIyOLjZeCggItWbLEMV66du0qT0/PYuscPnxYmzZtYkzVUoZhaMKECZo7d65+/vlnxcbGFruecYPyMAxD+fn5jBeUasCAAdq4caPWrVvn+OrWrZtGjRqldevWqVmzZowblCk/P19bt25Vw4YNa8fvGTNmn6jrZs+ebXh6ehrvv/++sWXLFuOee+4x/P39jX379pkdDSbIzMw01q5da6xdu9aQZLz88svG2rVrjf379xuGYRjPPvusERwcbMydO9fYuHGjMXLkSKNhw4ZGRkaG4z7GjRtnREdHGz/99JOxZs0ao3///kZ8fLxRWFho1tNCNbr99tuN4OBg49dffzUOHz7s+MrJyXGsw7jB6R5++GFj6dKlxt69e40NGzYYjzzyiOHm5mYsXLjQMAzGC8rn9NnzDINxg+ImTpxo/Prrr8aePXuMP/74w7j00kuNwMBAx+dbVx8vlCaTvPnmm0aTJk0MLy8vo0uXLo6pglH3/PLLL4akEl9jxowxDKNoms5JkyYZkZGRhre3t3HhhRcaGzduLHYfubm5xoQJE4zQ0FDD19fXuPTSS40DBw6Y8GxQE0obL5KMGTNmONZh3OB0N998s+P/nAYNGhgDBgxwFCbDYLygfP5Zmhg3ON21115rNGzY0PD09DSioqKMK664wti8ebPjelcfLxbDMAxztnEBAAAAgPPjmCYAAAAAKAOlCQAAAADKQGkCAAAAgDJQmgAAAACgDJQmAAAAACgDpQkAAAAAykBpAgAAAIAyUJoAAAAAoAyUJgAAysliseirr74yOwYAoIZRmgAALuHGG2+UxWIp8TVo0CCzowEAajkPswMAAFBegwYN0owZM4ot8/b2NikNAKCuYEsTAMBleHt7KzIysthXvXr1JBXtOvf2229r8ODB8vX1VWxsrL744otit9+4caP69+8vX19f1a9fX7feequysrKKrfPBBx+oXbt28vb2VsOGDTVhwoRi16empuryyy+Xn5+fWrZsqfnz51fvkwYAmI7SBACoNR5//HFdeeWVWr9+va6//nqNHDlSW7dulSTl5ORo0KBBqlevnlatWqUvvvhCP/30U7FS9Pbbb+uOO+7Qrbfeqo0bN2r+/Plq0aJFsceYMmWKrrnmGm3YsEFDhgzRqFGjlJaWVqPPEwBQsyyGYRhmhwAA4GxuvPFGffLJJ/Lx8Sm2/MEHH9Tjjz8ui8WicePG6e2333Zc16tXL3Xp0kVvvfWW/vOf/+jBBx/UwYMH5e/vL0n67rvvNHToUCUnJysiIkKNGjXSTTfdpKeeeqrUDBaLRY899piefPJJSVJ2drYCAwP13XffcWwVANRiHNMEAHAZ/fr1K1aKJCk0NNTxfUJCQrHrEhIStG7dOknS1q1bFR8f7yhMknTeeefJbrdr+/btslgsSk5O1oABA8rM0LFjR8f3/v7+CgwMVEpKSmWfEgDABVCaAAAuw9/fv8TucmdjsVgkSYZhOL4vbR1fX99y3Z+np2eJ29rt9gplAgC4Fo5pAgDUGn/88UeJy23atJEkxcXFad26dcrOznZc/9tvv8nNzU2tWrVSYGCgmjZtqsWLF9doZgCA82NLEwDAZeTn5+vIkSPFlnl4eCgsLEyS9MUXX6hbt246//zzNWvWLK1cuVLvv/++JGnUqFGaNGmSxowZo8mTJ+vYsWO68847dcMNNygiIkKSNHnyZI0bN07h4eEaPHiwMjMz9dtvv+nOO++s2ScKAHAqlCYAgMv44Ycf1LBhw2LLWrdurW3btkkqmtlu9uzZGj9+vCIjIzVr1izFxcVJkvz8/PTjjz/q7rvvVvfu3eXn56crr7xSL7/8suO+xowZo7y8PL3yyiu6//77FRYWpquuuqrmniAAwCkxex4AoFawWCyaN2+ehg8fbnYUAEAtwzFNAAAAAFAGShMAAAAAlIFjmgAAtQJ7mwMAqgtbmgAAAACgDJQmAAAAACgDpQkAAAAAykBpAgAAAIAyUJoAAAAAoAyUJgAAAAAoA6UJAAAAAMpAaQIAAACAMvw/UhNNaMPoewoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Using a logger is good practice for research code instead of print statements.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Model & Probability Functions ---\n",
    "\n",
    "def log_gaussian_likelihood(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the log-likelihood using the stable torch.distributions module.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): The observed data (ground truth).\n",
    "        pred_mean (torch.Tensor): The model's predictions (the mean of the Gaussian).\n",
    "        sigma (float): The standard deviation of the Gaussian.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The total log-likelihood of the data.\n",
    "    \"\"\"\n",
    "    # Create a tensor for sigma with the correct dtype and device\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    # Create a Normal distribution object. `loc` is the mean, `scale` is the standard deviation.\n",
    "    normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    \n",
    "    # Calculate the log probability of the data 'x' under this distribution.\n",
    "    # This is done element-wise, so we sum them to get the total log probability.\n",
    "    log_prob = normal_dist.log_prob(x).sum()\n",
    "    \n",
    "    return log_prob\n",
    "\n",
    "def log_gaussian_prior(theta: torch.Tensor, sigma_prior: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the log-prior probability of the weights `theta`.\n",
    "    Assumes a Gaussian prior centered at 0.\n",
    "    \n",
    "    Args:\n",
    "        theta (torch.Tensor): The model parameters (weights).\n",
    "        sigma_prior (float): The standard deviation of the Gaussian prior.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The log-prior probability of the parameters.\n",
    "    \"\"\"\n",
    "    # Using the stable distributions module is best practice.\n",
    "    prior_dist = Normal(loc=0.0, scale=sigma_prior)\n",
    "    return prior_dist.log_prob(theta).sum()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the experiment.\"\"\"\n",
    "    # --- 1. Generate Synthetic Data ---\n",
    "    # We will model a simple linear relationship: y = X @ theta_true + noise\n",
    "    n_samples = 200\n",
    "    n_features = 5\n",
    "    noise_std_dev = 0.1  # True standard deviation of the noise in data generation\n",
    "\n",
    "    # Create a \"true\" set of weights we want our model to discover\n",
    "    theta_true = torch.tensor([2.5, -1.0, 3.3, 0.0, -4.1]).unsqueeze(1)\n",
    "    \n",
    "    # Generate random input data X\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Generate the output y using the linear model and add Gaussian noise\n",
    "    noise = torch.randn(n_samples, 1) * noise_std_dev\n",
    "    y = X @ theta_true + noise\n",
    "\n",
    "    logging.info(f\"Generated data with {n_samples} samples and {n_features} features.\")\n",
    "    logging.info(f\"True theta:\\n{theta_true.T}\")\n",
    "\n",
    "    # --- 2. Setup the Model and Optimizer ---\n",
    "    # Initialize model parameters `theta` randomly. This is what we will learn.\n",
    "    theta_model = torch.randn(n_features, 1, requires_grad=True)\n",
    "\n",
    "    # Setup the optimizer. We use Adam, a standard choice.\n",
    "    # We pass the parameter we want to optimize: `theta_model`.\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.Adam([theta_model], lr=learning_rate)\n",
    "    \n",
    "    # --- 3. Perform Gradient Ascent ---\n",
    "    # We want to *maximize* the log joint. PyTorch optimizers *minimize* a loss.\n",
    "    # So, our \"loss\" will be the *negative* log joint probability.\n",
    "    epochs = 500\n",
    "    log_joint_history = []\n",
    "    \n",
    "    logging.info(\"Starting gradient ascent to maximize log joint probability...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Zero out gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Forward Pass ---\n",
    "        # 1. Get the model's prediction for y\n",
    "        y_pred = X @ theta_model\n",
    "        \n",
    "        # 2. Calculate the log likelihood using the provided function\n",
    "        # We pass the true noise std dev for this test.\n",
    "        log_likelihood = log_gaussian_likelihood(y, y_pred, sigma=noise_std_dev)\n",
    "        #log_likelihood_manual = log_gaussian_likelihood_manual(y, y_pred, sigma=noise_std_dev)\n",
    "        # print the difference between the two\n",
    "        #logging.info(f\"Difference between the two log likelihoods: {log_likelihood - log_likelihood_manual}\")\n",
    "        \n",
    "        # 3. Calculate the log prior on the weights\n",
    "        log_prior = log_gaussian_prior(theta_model)\n",
    "        \n",
    "        # 4. Calculate the log joint probability (the objective we want to maximize)\n",
    "        log_joint = log_likelihood + log_prior\n",
    "        \n",
    "        # --- Backward Pass ---\n",
    "        # 5. Define the loss as the negative log joint\n",
    "        loss = -log_joint\n",
    "        \n",
    "        # 6. Backpropagate to compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 7. Update the parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Logging ---\n",
    "        log_joint_history.append(log_joint.item())\n",
    "        if epoch % 50 == 0:\n",
    "            logging.info(f\"Epoch {epoch:03d} | Log Joint: {log_joint.item():.4f} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    logging.info(\"Training finished.\")\n",
    "    logging.info(f\"Learned theta:\\n{theta_model.T.detach()}\")\n",
    "    logging.info(f\"True theta:\\n{theta_true.T}\")\n",
    "    \n",
    "    # --- 4. Plot the results ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_joint_history)\n",
    "    plt.title(\"Log Joint Probability during Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Log Joint P(y, θ | X)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dibs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
