{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/dibs.py\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    try:\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except RuntimeError:\n",
    "        trace, p = torch.tensor(0.0, device=g.device, dtype=g.dtype), g.clone()\n",
    "        for k in range(1, min(d + 1, 20)):\n",
    "            trace += (alpha ** k) * torch.trace(p) / k\n",
    "            if k < 19:\n",
    "                p = p @ g\n",
    "        return trace\n",
    "\n",
    "\n",
    "def log_gaussian_likelihood(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    residuals = x - pred_mean\n",
    "    #old incorrect log_prob = -0.5 * (np.log(2 * np.pi) -  (1/2)* torch.log(sigma_tensor**2) -  0.5*(residuals / sigma_tensor) ** 2) old\n",
    "    log_prob = -0.5 * (torch.log(2 * torch.pi * sigma_tensor**2)) - 0.5 * ((residuals / sigma_tensor)**2)\n",
    "    #normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    #log_prob = normal_dist.log_prob(x)\n",
    "\n",
    "    return torch.sum(log_prob)\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    raw_scores = alpha * torch.einsum('...ik,...jk->...ij', u, v)\n",
    "    *batch_dims, d, _ = z.shape[:-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    if batch_dims:\n",
    "        diag_mask = diag_mask.expand(*batch_dims, d, d)\n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    if probs.ndim == 3:\n",
    "        diag_mask = diag_mask.expand(probs.shape[0], d, d)\n",
    "    return probs * diag_mask\n",
    "\n",
    "def gumbel_soft_gmat(z: torch.Tensor,\n",
    "                     hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Soft Gumbel–Softmax adjacency  (Eq. B.6)\n",
    "\n",
    "        g_ij  = σ_τ( L_ij + α⟨u_i , v_j⟩ )\n",
    "\n",
    "    where  L_ij ~ Logistic(0,1)  and  τ = hparams['tau']. appendix b2\n",
    "    \"\"\"\n",
    "    raw = scores(z, hparams[\"alpha\"])\n",
    "\n",
    "    # Logistic(0,1) noise   L = log U - log(1-U)\n",
    "    u = torch.rand_like(raw)\n",
    "    L = torch.log(u) - torch.log1p(-u)\n",
    "\n",
    "    logits = (raw + L) / hparams[\"tau\"]\n",
    "    g_soft = torch.sigmoid(logits)\n",
    "\n",
    "    d = g_soft.size(-1)\n",
    "    mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    return g_soft * mask\n",
    "\n",
    "def log_full_likelihood(data: Dict[str, Any], soft_gmat: torch.Tensor, theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    ## TODO: Expert belief: update this to use interventions, change the full likelihood \n",
    "    # and also add log bernoulli likelihood calculatior\n",
    "    x_data = data['x']\n",
    "    effective_W = theta * soft_gmat\n",
    "    pred_mean = torch.matmul(x_data, effective_W)\n",
    "    sigma_obs = hparams.get('sigma_obs_noise', 0.1)\n",
    "    return log_gaussian_likelihood(x_data, pred_mean, sigma=sigma_obs)\n",
    "\n",
    "def log_theta_prior(theta_effective: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    return log_gaussian_likelihood(theta_effective, torch.zeros_like(theta_effective), sigma=sigma)\n",
    "\n",
    "def gumbel_acyclic_constr_mc(z: torch.Tensor, d: int, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    h_samples = []\n",
    "    for _ in range(hparams['n_nongrad_mc_samples']):\n",
    "        # FOR NOW, JUST GIVE THE SOFT MATRIX, AND BY ANNEALING IT TO HARD MATRIX\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "        h_samples.append(acyclic_constr(g_soft, d))\n",
    "        \n",
    "        # should gumbel soft gmat to hard gmat be done with >0.5 or with a sigmoid?  \n",
    "        #print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #g_hard = torch.bernoulli(g_soft)\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #h_samples.append(acyclic_constr(g_hard, d))\n",
    "        #g_hard = (g_soft > 0.5).float()\n",
    "        #how about this  mentioned in dibs       g_ST   = g_hard + (g_soft - g_soft.detach())   # straight-through\n",
    "        \n",
    "        #TODO fix above\n",
    "        # for now use g_soft\n",
    "        \n",
    "    h_samples = torch.stack(h_samples)\n",
    "\n",
    "\n",
    "    return torch.mean(h_samples, dim=0)\n",
    "\n",
    "def grad_z_log_joint_gumbel(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    d = z.shape[0]\n",
    "    theta_const = theta\n",
    "    \n",
    "    #z.requires_grad_(True)\n",
    "    # --- Part 1: Prior Gradient ---\n",
    "    # MC estimate of gradient of acyclicity constraint using Gumbel soft graphs\n",
    "\n",
    "    h_mean = gumbel_acyclic_constr_mc(z, d, hparams)\n",
    "    grad_h_mc = torch.autograd.grad(h_mean, z)[0]\n",
    "    grad_log_z_prior_total = -hparams['beta'] * grad_h_mc - (z / hparams['sigma_z']**2)\n",
    "\n",
    "    # --- Part 2: Likelihood Gradient ---\n",
    "    \n",
    "    # 1. We need to collect the log-probability AND the gradient for each sample.\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "\n",
    "    for _ in range( hparams['n_grad_mc_samples']):\n",
    "        # 2. Generate a single soft graph sample.\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "        # 3. Calculate the log-joint for this single sample.\n",
    "        log_density_one_sample = log_full_likelihood(data, g_soft, theta_const, hparams) + \\\n",
    "                                 log_theta_prior(theta_const * g_soft, hparams.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "        # 4. Calculate the gradient for this single sample.\n",
    "        # We must use retain_graph=True because we are doing a backward pass\n",
    "        # inside a loop, and PyTorch would otherwise free the graph memory.\n",
    "        grad, = torch.autograd.grad(log_density_one_sample, z, retain_graph=True)\n",
    "        \n",
    "        log_density_samples.append(log_density_one_sample)\n",
    "        grad_samples.append(grad)\n",
    "\n",
    "    # 5. After the loop, we can safely detach z_ from any further graph history.\n",
    "\n",
    "    # 6. Compute the final likelihood gradient using the stable weighted average.\n",
    "    # This correctly computes E[p*∇log(p)] / E[p]\n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grad_p = torch.stack(grad_samples)\n",
    "    grad_lik = weighted_grad(log_p, grad_p)\n",
    "\n",
    "\n",
    "    #if z.grad is not None:\n",
    "    #    z.grad.zero_()\n",
    "    #z.requires_grad_(False)\n",
    "    \n",
    "    # Final combined gradient\n",
    "    \n",
    "\n",
    "\n",
    "    total = grad_log_z_prior_total + grad_lik\n",
    "    # 3) Combine\n",
    "    # ------------------------------------------------\n",
    "    return total.detach()\n",
    "\n",
    "\n",
    "## SCORE BASED ESTIMATOR FOR GRADIENT Z \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Score-function estimator for ∇_Z log p(Z,Θ | D)\n",
    "#  (Section B.2 of the paper, b = 0)\n",
    "# ------------------------------------------------------------\n",
    "def analytic_score_g_given_z(z, g, hparams):\n",
    "    # 1. logits and probabilities\n",
    "    probs = bernoulli_soft_gmat(z, hparams)\n",
    "    diff   = g - probs                 # (g_ij − σ(s_ij))\n",
    "    u, v   = z[..., 0], z[..., 1]      # (d,k)\n",
    "\n",
    "    # 2. gradients wrt u and v\n",
    "    grad_u = hparams['alpha'] * torch.einsum('ij,jk->ik', diff, v)   # (d,k)\n",
    "    grad_v = hparams['alpha'] * torch.einsum('ij,ik->jk', diff, u)   # (d,k)\n",
    "\n",
    "    return torch.stack([grad_u, grad_v], dim=-1)          # (d,k,2)\n",
    "\n",
    "\n",
    "def grad_z_log_joint_score(z: torch.Tensor,\n",
    "                           theta: torch.Tensor,\n",
    "                           data: Dict[str, Any],\n",
    "                           hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ∇_Z log p(Z,Θ | D)  using the score-function (REINFORCE) estimator.\n",
    "\n",
    "    This replaces the Gumbel-soft estimator.\n",
    "    \"\"\"\n",
    "    sigma_z2 = hparams['sigma_z'] ** 2\n",
    "    beta     = hparams['beta']\n",
    "\n",
    "    M = hparams['n_grad_mc_samples'] # M = 50\n",
    "    d = z.shape[0]                   # d = 4\n",
    "    theta_const = theta\n",
    "\n",
    "\n",
    "    # 1. sample hard graphs \n",
    "    with torch.no_grad():\n",
    "        g_hard_samples = [torch.bernoulli(bernoulli_soft_gmat(z, hparams)) for _ in range(M)]\n",
    "\n",
    "    ll = []\n",
    "    scores = []\n",
    "    for g in g_hard_samples:\n",
    "        log_lik = log_full_likelihood(data, g, theta_const, hparams)\n",
    "        theta_eff = theta_const * g\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        \n",
    "        # log likelihood \n",
    "        ll.append(log_lik + log_theta_prior_val)\n",
    "        \n",
    "        # score \n",
    "        scores.append(analytic_score_g_given_z(z, g, hparams))\n",
    "    \n",
    "    log_p = torch.stack(ll)\n",
    "    grad_p = torch.stack(scores)\n",
    "\n",
    "    log_p_max = log_p.max()\n",
    "    log_p_shifted = log_p - log_p_max\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    unnormalized_w = torch.exp(log_p_shifted/10)\n",
    "    #print(f'unnormalized_w shape: {unnormalized_w.shape}, values: \\n {unnormalized_w}')\n",
    "    w = unnormalized_w / unnormalized_w.sum()\n",
    "                     # (M,1,1,...)\n",
    "    #print(f'w shape: {w.shape}, values: \\n {w}')\n",
    "\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                    \n",
    "    \n",
    "    ## compute the weighted avg \n",
    "    grad_lik = (w * grad_p).sum(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Z-prior: Gaussian + acyclicity penalty --------------\n",
    "    # gumbel is possible cuz no differentiable function in expectation \n",
    "    z_ = z.detach().clone().requires_grad_(True)\n",
    "    h_mean = gumbel_acyclic_constr_mc(z_, d, hparams)       # differentiable w.r.t z_\n",
    "    grad_h, = torch.autograd.grad(h_mean, z_, retain_graph=False)\n",
    "    grad_prior = -beta * grad_h - z_ / sigma_z2\n",
    "\n",
    "    return (grad_lik + grad_prior).detach()\n",
    "\n",
    "\n",
    "\n",
    "def weighted_grad(log_p: torch.Tensor,\n",
    "                  grad_p: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return   Σ softmax(log_p)_m * grad_p[m]\n",
    "    Shapes\n",
    "        log_p   : (M,)\n",
    "        grad_p  : (M, …)   (any extra dims)\n",
    "    \"\"\"\n",
    "    # 1. numerically stable soft-max weights\n",
    "    #print(f'log_p shape: {log_p.shape}, values:\\n {log_p}')\n",
    "    #print(f'grad_p shape: {grad_p.shape}, values: \\n{grad_p}')\n",
    "    log_p_shifted = log_p - log_p.max()          # (M,)\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    w = torch.exp(log_p_shifted)\n",
    "    #print(f'w shape: {w.shape}, values:\\n {w}')\n",
    "    w = w / w.sum()\n",
    "    #print(f'w after normalization shape: {w.shape}, values:\\n {w}')\n",
    "\n",
    "    # 2. broadcast weights onto grad tensor\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                      # (M,1,1,...)\n",
    "\n",
    "    return (w * grad_p).sum(dim=0)               # same shape as grad slice\n",
    "\n",
    "\n",
    "\n",
    "def grad_theta_log_joint(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    #theta.requires_grad_(True)\n",
    "    n_samples = hparams.get('n_grad_mc_samples', 1)\n",
    "    theta_ = theta.clone().detach().requires_grad_(True)\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        g_soft = bernoulli_soft_gmat(z, hparams)\n",
    "        #print(f\"g_soft values: {g_soft}\")\n",
    "        g_hard = torch.bernoulli(g_soft)\n",
    "        #print(f\"g_hard values: {g_hard}\")\n",
    "\n",
    "        # tryign with gumbel to be consistent with grad z and gumbel mc acylci impelmentation\n",
    "        #g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        log_lik_val = log_full_likelihood(data, g_hard, theta_, hparams)\n",
    "        theta_eff = theta_ * g_hard\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        #ll_grad, = torch.autograd.grad(log_lik_val, theta_, retain_graph=True)\n",
    "        #log_theta_prior_grad, = torch.autograd.grad(log_theta_prior_val, theta_ , retain_graph=True)\n",
    "        #print(f\"ll_grad shape: {ll_grad.shape}, values: {ll_grad}\")\n",
    "        #print(f\"log_theta_prior_grad shape: {log_theta_prior_grad.shape}, values: {log_theta_prior_grad}\")\n",
    "\n",
    "        current_log_density = log_lik_val + log_theta_prior_val\n",
    "        current_grad ,= torch.autograd.grad(current_log_density, theta_)\n",
    "        log_density_samples.append(current_log_density) \n",
    "\n",
    "        grad_samples.append(current_grad)\n",
    "    #print(f\" END OF Grad_theta mc_samples, iter number: {hparams.get('current_iteration',1)}  \\n\")\n",
    "\n",
    "    log_p_tensor = torch.stack(log_density_samples)\n",
    "    grad_p_tensor = torch.stack(grad_samples)\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    #if theta.grad is not None:\n",
    "    #    theta.grad.zero_()\n",
    "    #theta.requires_grad_(False)\n",
    "\n",
    "    grad =weighted_grad(log_p_tensor, grad_p_tensor)\n",
    "    #grad = stable_gradient_estimator(log_p_tensor, grad_p_tensor)\n",
    "    #print(f\"Grad_theta shape: {grad.shape}, values: \\n {grad}\")\n",
    "\n",
    "    return  grad.detach()\n",
    "\n",
    "\n",
    "def grad_log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "    grad_z = grad_z_log_joint_gumbel(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    #grad_z = grad_z_log_joint_score(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    grad_theta = grad_theta_log_joint(params[\"z\"].detach(), params[\"theta\"], data, hparams)\n",
    "    \n",
    "    return {\"z\": grad_z, \"theta\": grad_theta}\n",
    "\n",
    "def log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    hparams_updated = update_dibs_hparams(hparams, params[\"t\"].item())\n",
    "    z, theta = params['z'], params['theta']\n",
    "    d = z.shape[0]\n",
    "\n",
    "    g_soft = bernoulli_soft_gmat(z, hparams_updated)\n",
    "    log_lik = log_full_likelihood(data, g_soft, theta, hparams_updated)\n",
    "\n",
    "    log_prior_z_gaussian = torch.sum(Normal(0.0, hparams_updated['sigma_z']).log_prob(z))\n",
    "    expected_h_val = gumbel_acyclic_constr_mc(z, d, hparams_updated)\n",
    "    log_prior_z_acyclic = -hparams_updated['beta'] * expected_h_val\n",
    "    log_prior_z = log_prior_z_gaussian + log_prior_z_acyclic\n",
    "    \n",
    "    theta_eff = theta * g_soft\n",
    "    log_prior_theta = log_theta_prior(theta_eff, hparams_updated.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "    if (hparams_updated['current_iteration'] > 850 and hparams_updated['current_iteration'] < 1200):\n",
    "        with torch.no_grad():\n",
    "            log_terms = {\n",
    "                \"log_lik\":      log_lik.item(),\n",
    "                \"z_prior_gauss\":log_prior_z_gaussian.item(),\n",
    "                \"z_prior_acyc\": log_prior_z_acyclic.item(),   # usually ≤ 0\n",
    "                \"theta_prior\":  log_prior_theta.item(),\n",
    "                \"log_joint\": log_lik + log_prior_theta + log_prior_z + log_prior_z_acyclic,\n",
    "                \"penalty\": -hparams_updated['beta'] * expected_h_val.item()\n",
    "            }\n",
    "        print(f\"[dbg] {log_terms}\")\n",
    "\n",
    "    \n",
    "    return log_lik + log_prior_z + log_prior_theta\n",
    "\n",
    "def update_dibs_hparams(hparams: Dict[str, Any], t_step: float) -> Dict[str, Any]:\n",
    "\n",
    "    hparams['beta'] = hparams['beta_base'] * t_step # linear \n",
    "\n",
    "    hparams['alpha'] = hparams['alpha_base'] * t_step  # linear slope 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hparams['current_iteration'] = t_step # Store current iteration\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def hard_gmat_from_z(z: torch.Tensor, alpha: float = 1.0) -> torch.Tensor:\n",
    "    s = scores(z, alpha)\n",
    "    return (s > 0).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing acyclic_constr function\n",
      "============================================================\n",
      "\n",
      "1. Testing small acyclic graph (d=3):\n",
      "   Acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0 (should be <= 0 for acyclic)\n",
      "\n",
      "2. Testing small cyclic graph (d=3):\n",
      "   Cyclic graph constraint: 0.011667\n",
      "   Expected: > 0 (penalizes cycles)\n",
      "\n",
      "3. Testing identity/no edges (d=4):\n",
      "   Identity matrix constraint: 5.765625\n",
      "   Expected: close to 0\n",
      "\n",
      "4. Testing large graph (d=12, eigenvalue path):\n",
      "   Large acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0\n",
      "\n",
      "5. Testing large graph with potential eigenvalue issues (d=15):\n",
      "   Problematic graph constraint: 306.006714\n",
      "   Expected: large positive value (many cycles)\n",
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.042117\n",
      "   Gradient computed successfully: False\n",
      "   ERROR in gradient computation: 'NoneType' object has no attribute 'norm'\n",
      "\n",
      "7. Testing edge cases:\n",
      "   Very small values constraint: 0.0000000000\n",
      "   Large values constraint: 422.222290\n",
      "\n",
      "============================================================\n",
      "acyclic_constr testing complete\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111888/216125626.py:91: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
      "/tmp/ipykernel_111888/216125626.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "def test_acyclic_constr():\n",
    "    \"\"\"\n",
    "    Test the acyclic_constr function with various scenarios to debug potential issues\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing acyclic_constr function\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test case 1: Small acyclic graph (d <= 10)\n",
    "    print(\"\\n1. Testing small acyclic graph (d=3):\")\n",
    "    d = 3\n",
    "    g_acyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.3],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.0, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_acyclic = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"   Acyclic graph constraint: {h_acyclic.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0 (should be <= 0 for acyclic)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 2: Small cyclic graph\n",
    "    print(\"\\n2. Testing small cyclic graph (d=3):\")\n",
    "    g_cyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.0],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.3, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_cyclic = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"   Cyclic graph constraint: {h_cyclic.item():.6f}\")\n",
    "        print(f\"   Expected: > 0 (penalizes cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 3: Identity matrix (no edges)\n",
    "    print(\"\\n3. Testing identity/no edges (d=4):\")\n",
    "    d = 4\n",
    "    g_identity = torch.eye(d, dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_identity = acyclic_constr(g_identity, d)\n",
    "        print(f\"   Identity matrix constraint: {h_identity.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 4: Large graph (d > 10, triggers eigenvalue computation)\n",
    "    print(\"\\n4. Testing large graph (d=12, eigenvalue path):\")\n",
    "    d = 12\n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "    g_large = torch.randn(d, d) * 0.1\n",
    "    g_large = torch.triu(g_large, diagonal=1)  # Upper triangular (acyclic)\n",
    "    \n",
    "    try:\n",
    "        h_large = acyclic_constr(g_large, d)\n",
    "        print(f\"   Large acyclic graph constraint: {h_large.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "        \n",
    "    # Test case 5: Large graph that might cause eigenvalue issues\n",
    "    print(\"\\n5. Testing large graph with potential eigenvalue issues (d=15):\")\n",
    "    d = 15\n",
    "    g_problematic = torch.ones(d, d) * 0.5\n",
    "    g_problematic.fill_diagonal_(0)  # No self-loops\n",
    "    \n",
    "    try:\n",
    "        h_problematic = acyclic_constr(g_problematic, d)\n",
    "        print(f\"   Problematic graph constraint: {h_problematic.item():.6f}\")\n",
    "        print(f\"   Expected: large positive value (many cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR (expected - should fallback to series): {e}\")\n",
    "    \n",
    "    # Test case 6: Check gradients work\n",
    "    print(\"\\n6. Testing gradient computation:\")\n",
    "    d = 4\n",
    "    g_test = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "    g_test.data.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_test = acyclic_constr(g_test, d)\n",
    "        h_test.backward()\n",
    "        print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "        print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient computation: {e}\")\n",
    "    \n",
    "    # Test case 7: Edge cases\n",
    "    print(\"\\n7. Testing edge cases:\")\n",
    "    \n",
    "    # Very small values\n",
    "    d = 3\n",
    "    g_small = torch.ones(d, d) * 1e-10\n",
    "    g_small.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_small = acyclic_constr(g_small, d)\n",
    "        print(f\"   Very small values constraint: {h_small.item():.10f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with small values: {e}\")\n",
    "    \n",
    "    # Very large values (might cause overflow)\n",
    "    g_large_vals = torch.ones(d, d) * 10.0\n",
    "    g_large_vals.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_large_vals = acyclic_constr(g_large_vals, d)\n",
    "        print(f\"   Large values constraint: {h_large_vals.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large values: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"acyclic_constr testing complete\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run the test\n",
    "test_acyclic_constr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.000009\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.013367\n",
      "\n",
      "6b. Testing gradient computation (alternative method):\n",
      "   Constraint value: 0.016230\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.014625\n",
      "\n",
      "6c. Testing gradient computation (simple test):\n",
      "   Constraint value: 0.020845\n",
      "   Gradient computed using autograd.grad: True\n",
      "   Gradient norm: 0.276399\n",
      "   Gradient shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test case 6: Check gradients work\n",
    "print(\"\\n6. Testing gradient computation:\")\n",
    "d = 4\n",
    "\n",
    "# Method 1: Create tensor properly to maintain leaf status\n",
    "g_test = torch.randn(d, d) * 0.1\n",
    "# Zero out diagonal elements without modifying .data\n",
    "mask = ~torch.eye(d, dtype=torch.bool)\n",
    "g_test = g_test * mask.float()\n",
    "g_test.requires_grad_(True)\n",
    "\n",
    "try:\n",
    "    h_test = acyclic_constr(g_test, d)\n",
    "    h_test.backward()\n",
    "    print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "    if g_test.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None - tensor may not be leaf\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 2: Alternative approach using retain_grad()\n",
    "print(\"\\n6b. Testing gradient computation (alternative method):\")\n",
    "g_test2 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "g_test2.data.fill_diagonal_(0)\n",
    "g_test2.retain_grad()  # This ensures gradients are kept even for non-leaf tensors\n",
    "\n",
    "try:\n",
    "    h_test2 = acyclic_constr(g_test2, d)\n",
    "    h_test2.backward()\n",
    "    print(f\"   Constraint value: {h_test2.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test2.grad is not None}\")\n",
    "    if g_test2.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test2.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 3: Test with a simple differentiable operation\n",
    "print(\"\\n6c. Testing gradient computation (simple test):\")\n",
    "g_test3 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "# Create off-diagonal matrix using multiplication\n",
    "off_diag_mask = 1.0 - torch.eye(d)\n",
    "g_test3_masked = g_test3 * off_diag_mask\n",
    "\n",
    "try:\n",
    "    h_test3 = acyclic_constr(g_test3_masked, d)\n",
    "    grad_g3 = torch.autograd.grad(h_test3, g_test3, retain_graph=False)[0]\n",
    "    print(f\"   Constraint value: {h_test3.item():.6f}\")\n",
    "    print(f\"   Gradient computed using autograd.grad: {grad_g3 is not None}\")\n",
    "    if grad_g3 is not None:\n",
    "        print(f\"   Gradient norm: {grad_g3.norm().item():.6f}\")\n",
    "        print(f\"   Gradient shape: {grad_g3.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores z to prob and Bernouilli soft_gmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing scores and bernoulli_soft_gmat functions\n",
      "======================================================================\n",
      "\n",
      "1. Testing simple 2D case with known values:\n",
      "   Z shape: torch.Size([2, 3, 2])\n",
      "   Z:\n",
      "tensor([[[1.0000, 0.5000],\n",
      "         [0.0000, 1.0000],\n",
      "         [1.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 1.0000],\n",
      "         [1.0000, 0.5000],\n",
      "         [1.0000, 0.0000]]])\n",
      "   Scores shape: torch.Size([2, 2])\n",
      "   Scores:\n",
      "tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "   Manual score (0->1): 1.000000\n",
      "   Computed score (0->1): 1.000000\n",
      "   Manual score (1->0): 1.000000\n",
      "   Computed score (1->0): 1.000000\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "\n",
      "2. Testing bernoulli_soft_gmat:\n",
      "   Probabilities shape: torch.Size([2, 2])\n",
      "   Probabilities:\n",
      "tensor([[0.0000, 0.7311],\n",
      "        [0.7311, 0.0000]])\n",
      "   Manual prob (0->1): 0.731059\n",
      "   Computed prob (0->1): 0.731059\n",
      "   Manual prob (1->0): 0.731059\n",
      "   Computed prob (1->0): 0.731059\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "   All probs in [0,1]: True\n",
      "\n",
      "3. Testing with different alpha values:\n",
      "   Alpha = 0.1:\n",
      "     Max score: 0.100000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.524979\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 1.0:\n",
      "     Max score: 1.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.731059\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 5.0:\n",
      "     Max score: 5.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.993307\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 10.0:\n",
      "     Max score: 10.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.999955\n",
      "     Min prob: 0.000000\n",
      "\n",
      "4. Testing gradient flow:\n",
      "   ERROR in gradient flow: 'NoneType' object has no attribute 'shape'\n",
      "\n",
      "5. Testing consistency between scores and probabilities:\n",
      "   Max difference between manual and direct computation: 0.0000000000\n",
      "   Are they approximately equal: True\n",
      "\n",
      "6. Testing edge cases:\n",
      "   Zero embeddings - scores: tensor([0.])\n",
      "   Zero embeddings - probs: tensor([0.0000, 0.5000])\n",
      "   Zero embeddings - all probs should be 0.5: False\n",
      "   Large embeddings - max score: 200.000000\n",
      "   Large embeddings - max prob: 1.000000\n",
      "   Large embeddings - are probs valid: True\n",
      "\n",
      "7. Testing batched operation:\n",
      "   Batch scores shape: torch.Size([2, 3, 3])\n",
      "   Expected shape: (2, 3, 3)\n",
      "   Shapes match: True\n",
      "\n",
      "======================================================================\n",
      "scores and bernoulli_soft_gmat testing complete\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111888/3757974893.py:113: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n"
     ]
    }
   ],
   "source": [
    "def test_scores_and_bernoulli_soft_gmat():\n",
    "    \"\"\"\n",
    "    Test the scores and bernoulli_soft_gmat functions based on the mathematical formulation\n",
    "    from section 4.2 of the paper.\n",
    "    \n",
    "    Mathematical background:\n",
    "    - Z = [U, V] where U, V ∈ R^(k×d)\n",
    "    - scores should compute α * u_i^T v_j for all i,j\n",
    "    - bernoulli_soft_gmat should compute σ_α(u_i^T v_j) = 1/(1 + exp(-α * u_i^T v_j))\n",
    "    - Diagonal elements should be 0 (no self-loops)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Testing scores and bernoulli_soft_gmat functions\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test case 1: Simple 2D case with known values\n",
    "    print(\"\\n1. Testing simple 2D case with known values:\")\n",
    "    d, k = 2, 3\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Create simple Z = [U, V] with known values\n",
    "    U = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]], dtype=torch.float32)  # (k, d)\n",
    "    V = torch.tensor([[0.5, 1.0], [1.0, 0.5], [0.0, 0.0]], dtype=torch.float32)  # (k, d)\n",
    "    Z = torch.stack([U.T, V.T], dim=-1)  # (d, k, 2)\n",
    "    \n",
    "    print(f\"   Z shape: {Z.shape}\")\n",
    "    print(f\"   Z:\\n{Z}\")\n",
    "    \n",
    "    # Test scores function\n",
    "    try:\n",
    "        scores_result = scores(Z, alpha)\n",
    "        print(f\"   Scores shape: {scores_result.shape}\")\n",
    "        print(f\"   Scores:\\n{scores_result}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        u1, v1 = Z[0, :, 0], Z[0, :, 1]  # u1, v1 for node 0\n",
    "        u2, v2 = Z[1, :, 0], Z[1, :, 1]  # u2, v2 for node 1\n",
    "        \n",
    "        manual_score_01 = alpha * torch.dot(u1, v2)\n",
    "        manual_score_10 = alpha * torch.dot(u2, v1)\n",
    "        \n",
    "        print(f\"   Manual score (0->1): {manual_score_01.item():.6f}\")\n",
    "        print(f\"   Computed score (0->1): {scores_result[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual score (1->0): {manual_score_10.item():.6f}\")\n",
    "        print(f\"   Computed score (1->0): {scores_result[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(scores_result)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in scores: {e}\")\n",
    "    \n",
    "    # Test bernoulli_soft_gmat function\n",
    "    print(\"\\n2. Testing bernoulli_soft_gmat:\")\n",
    "    hparams = {\"alpha\": alpha}\n",
    "    \n",
    "    try:\n",
    "        probs = bernoulli_soft_gmat(Z, hparams)\n",
    "        print(f\"   Probabilities shape: {probs.shape}\")\n",
    "        print(f\"   Probabilities:\\n{probs}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        manual_prob_01 = torch.sigmoid(manual_score_01)\n",
    "        manual_prob_10 = torch.sigmoid(manual_score_10)\n",
    "        \n",
    "        print(f\"   Manual prob (0->1): {manual_prob_01.item():.6f}\")\n",
    "        print(f\"   Computed prob (0->1): {probs[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual prob (1->0): {manual_prob_10.item():.6f}\")\n",
    "        print(f\"   Computed prob (1->0): {probs[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(probs)}\")\n",
    "        \n",
    "        # Check probabilities are in [0, 1]\n",
    "        print(f\"   All probs in [0,1]: {torch.all((probs >= 0) & (probs <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in bernoulli_soft_gmat: {e}\")\n",
    "    \n",
    "    # Test case 3: Test with different alpha values\n",
    "    print(\"\\n3. Testing with different alpha values:\")\n",
    "    alphas = [0.1, 1.0, 5.0, 10.0]\n",
    "    \n",
    "    for alpha_test in alphas:\n",
    "        hparams_test = {\"alpha\": alpha_test}\n",
    "        try:\n",
    "            scores_test = scores(Z, alpha_test)\n",
    "            probs_test = bernoulli_soft_gmat(Z, hparams_test)\n",
    "            \n",
    "            print(f\"   Alpha = {alpha_test}:\")\n",
    "            print(f\"     Max score: {scores_test.max().item():.6f}\")\n",
    "            print(f\"     Min score: {scores_test.min().item():.6f}\")\n",
    "            print(f\"     Max prob: {probs_test.max().item():.6f}\")\n",
    "            print(f\"     Min prob: {probs_test.min().item():.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR with alpha {alpha_test}: {e}\")\n",
    "    \n",
    "    # Test case 4: Test gradient flow\n",
    "    print(\"\\n4. Testing gradient flow:\")\n",
    "    d, k = 3, 4\n",
    "    Z_grad = torch.randn(d, k, 2, requires_grad=True) * 0.5\n",
    "    hparams_grad = {\"alpha\": 2.0}\n",
    "    \n",
    "    try:\n",
    "        scores_grad = scores(Z_grad, hparams_grad[\"alpha\"])\n",
    "        probs_grad = bernoulli_soft_gmat(Z_grad, hparams_grad)\n",
    "        \n",
    "        # Compute some loss and backpropagate\n",
    "        loss = torch.sum(probs_grad ** 2)\n",
    "        loss.backward()\n",
    "        \n",
    "        print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n",
    "        print(f\"   Z gradient norm: {Z_grad.grad.norm().item():.6f}\")\n",
    "        print(f\"   Loss value: {loss.item():.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient flow: {e}\")\n",
    "    \n",
    "    # Test case 5: Test consistency between scores and probabilities\n",
    "    print(\"\\n5. Testing consistency between scores and probabilities:\")\n",
    "    d, k = 4, 3\n",
    "    Z_test = torch.randn(d, k, 2) * 0.3\n",
    "    alpha_test = 1.5\n",
    "    hparams_test = {\"alpha\": alpha_test}\n",
    "    \n",
    "    try:\n",
    "        scores_manual = scores(Z_test, alpha_test)\n",
    "        probs_from_scores = torch.sigmoid(scores_manual)\n",
    "        \n",
    "        # Zero out diagonal\n",
    "        diag_mask = 1.0 - torch.eye(d)\n",
    "        probs_from_scores = probs_from_scores * diag_mask\n",
    "        \n",
    "        probs_direct = bernoulli_soft_gmat(Z_test, hparams_test)\n",
    "        \n",
    "        # Check if they match\n",
    "        max_diff = torch.max(torch.abs(probs_from_scores - probs_direct))\n",
    "        print(f\"   Max difference between manual and direct computation: {max_diff.item():.10f}\")\n",
    "        print(f\"   Are they approximately equal: {torch.allclose(probs_from_scores, probs_direct, atol=1e-6)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in consistency test: {e}\")\n",
    "    \n",
    "    # Test case 6: Test edge cases\n",
    "    print(\"\\n6. Testing edge cases:\")\n",
    "    \n",
    "    # Zero embeddings\n",
    "    Z_zero = torch.zeros(3, 2, 2)\n",
    "    hparams_zero = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_zero = scores(Z_zero, 1.0)\n",
    "        probs_zero = bernoulli_soft_gmat(Z_zero, hparams_zero)\n",
    "        \n",
    "        print(f\"   Zero embeddings - scores: {scores_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - probs: {probs_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - all probs should be 0.5: {torch.allclose(probs_zero, torch.ones_like(probs_zero) * 0.5)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with zero embeddings: {e}\")\n",
    "    \n",
    "    # Very large embeddings (test numerical stability)\n",
    "    Z_large = torch.ones(2, 2, 2) * 10.0\n",
    "    hparams_large = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_large = scores(Z_large, 1.0)\n",
    "        probs_large = bernoulli_soft_gmat(Z_large, hparams_large)\n",
    "        \n",
    "        print(f\"   Large embeddings - max score: {scores_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - max prob: {probs_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - are probs valid: {torch.all((probs_large >= 0) & (probs_large <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large embeddings: {e}\")\n",
    "    \n",
    "    # Test case 7: Test batched operation\n",
    "    print(\"\\n7. Testing batched operation:\")\n",
    "    batch_size = 2\n",
    "    d, k = 3, 2\n",
    "    Z_batch = torch.randn(batch_size, d, k, 2) * 0.5\n",
    "    hparams_batch = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_batch = scores(Z_batch, 1.0)\n",
    "        # Note: bernoulli_soft_gmat might not support batching directly\n",
    "        print(f\"   Batch scores shape: {scores_batch.shape}\")\n",
    "        print(f\"   Expected shape: ({batch_size}, {d}, {d})\")\n",
    "        print(f\"   Shapes match: {scores_batch.shape == (batch_size, d, d)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in batch operation: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"scores and bernoulli_soft_gmat testing complete\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run the test\n",
    "test_scores_and_bernoulli_soft_gmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running tests for acyclic_constr ---\n",
      "\n",
      "--- Running tests for Graph Generation ---\n",
      "Large acyclic graph (d=12) H(G): 0.000000\n",
      "Graph with self-loop H(G): 1.370371\n",
      "Acyclic graph H(G): 0.000000\n",
      "Graph with 2-cycle H(G): 0.666667\n",
      "\n",
      "Calculated soft G-matrix:\n",
      "tensor([[0.0000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.0000]])\n",
      "Expected soft G-matrix:\n",
      "tensor([[0.5000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.5000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.5000]])\n",
      "\n",
      "Calculated scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n",
      "Expected scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standalone test cell for debug_notebook.ipynb\n",
    "#\n",
    "# To use this, you would typically have the functions available \n",
    "# in the same notebook or imported from your 'models/dibs.py' script.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import unittest\n",
    "import logging\n",
    "\n",
    "# --- Setup basic logger ---\n",
    "# This is to prevent errors if log.warning is called in acyclic_constr\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# --- Functions to be tested ---\n",
    "# Pasted here for stand-alone execution.\n",
    "# In a real scenario, you would import these from your scripts.\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    # Ensure g is a floating point tensor for matrix operations\n",
    "    g = g.float()\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    # Using matrix_power for d <= 10 as it's generally stable for smaller matrices\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    # For larger d, eigenvalues are more efficient but can be numerically unstable\n",
    "    try:\n",
    "        # Eigenvalue decomposition is faster for large d\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        # The constraint is based on the sum of the d-th power of eigenvalues\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except torch.linalg.LinAlgError:\n",
    "        # Fallback to series expansion if eigenvalue computation fails\n",
    "        # This is a less precise but more stable approximation\n",
    "        log.warning(f\"Eigenvalue computation failed for d={d}. Falling back to series expansion.\")\n",
    "        trace = torch.tensor(0.0, device=g.device, dtype=g.dtype)\n",
    "        p = eye.clone() # Start with identity matrix for power calculation\n",
    "        for k in range(1, min(d + 1, 20)): # Limit to 20 terms for practical purposes\n",
    "            p = p @ m\n",
    "            trace += torch.trace(p) / k\n",
    "        return trace\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    \"\"\"Calculates the raw edge scores from latent embeddings.\"\"\"\n",
    "    # z has shape [d, k, 2]\n",
    "    # u and v have shape [d, k]\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    \n",
    "    # einsum performs batch matrix multiplication of u and v.T\n",
    "    # 'ik,jk->ij' means: sum over k for each i and j\n",
    "    raw_scores = alpha * torch.einsum('ik,jk->ij', u, v)\n",
    "    \n",
    "    # Ensure no self-loops by masking the diagonal\n",
    "    d = z.shape[0]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    \n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: dict) -> torch.Tensor:\n",
    "    \"\"\"Generates a soft adjacency matrix using a Bernoulli parameterization.\"\"\"\n",
    "    # Get probabilities by applying a sigmoid to the raw scores\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    \n",
    "    # The scores function already handles the diagonal masking, but as a safeguard:\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    \n",
    "    return probs * diag_mask\n",
    "\n",
    "\n",
    "# --- Test Cases ---\n",
    "\n",
    "class TestAcyclicConstraint(unittest.TestCase):\n",
    "\n",
    "    def test_strictly_acyclic_graph(self):\n",
    "        \"\"\"Tests a graph with no cycles (a Directed Acyclic Graph).\"\"\"\n",
    "        g_acyclic = torch.tensor([[0., 1., 1.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_acyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"Acyclic graph H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=5, msg=\"Acyclic graph should have H(G) = 0\")\n",
    "\n",
    "    def test_self_loop_cycle(self):\n",
    "        \"\"\"Tests a graph with a self-loop (the simplest cycle).\"\"\"\n",
    "        g_cyclic = torch.tensor([[1., 1., 0.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with self-loop H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "\n",
    "    def test_two_node_cycle(self):\n",
    "        \"\"\"Tests a graph with a 2-cycle (A -> B, B -> A).\"\"\"\n",
    "        g_cyclic = torch.tensor([[0., 1., 0.], [1., 0., 0.], [0., 1., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with 2-cycle H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "        \n",
    "    def test_large_graph_eigenvalue_path(self):\n",
    "        \"\"\"Tests the eigenvalue code path with a larger (d=12) acyclic graph.\"\"\"\n",
    "        d = 12\n",
    "        g_large_acyclic = torch.triu(torch.ones(d, d), diagonal=1)\n",
    "        h_val = acyclic_constr(g_large_acyclic, d)\n",
    "        print(f\"Large acyclic graph (d=12) H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=4, msg=\"Large acyclic graph should have H(G) near 0\")\n",
    "\n",
    "\n",
    "class TestGraphGeneration(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up common variables for the tests.\"\"\"\n",
    "        self.d = 3  # Number of nodes\n",
    "        self.k = 2  # Latent dimension\n",
    "        # Latent variable Z = [U, V]\n",
    "        self.z = torch.arange(self.d * self.k * 2, dtype=torch.float32).view(self.d, self.k, 2)\n",
    "        # z will be:\n",
    "        # [[[ 0,  1], [ 2,  3]],\n",
    "        #  [[ 4,  5], [ 6,  7]],\n",
    "        #  [[ 8,  9], [10, 11]]]\n",
    "        self.alpha = 0.5\n",
    "        self.hparams = {\"alpha\": self.alpha}\n",
    "\n",
    "    def test_scores_calculation(self):\n",
    "        \"\"\"Tests the bilinear score calculation G_ij = alpha * u_i^T v_j.\"\"\"\n",
    "        u = self.z[..., 0] # [[[0, 2], [4, 6], [8, 10]]]\n",
    "        v = self.z[..., 1] # [[[1, 3], [5, 7], [9, 11]]]\n",
    "        \n",
    "        # Manually calculate expected scores\n",
    "        expected_scores = self.alpha * torch.matmul(u, v.T)\n",
    "        # Set diagonal to zero\n",
    "        expected_scores.fill_diagonal_(0)\n",
    "        \n",
    "        # Get scores from function\n",
    "        s = scores(self.z, self.alpha)\n",
    "        print(f\"\\nCalculated scores:\\n{s}\")\n",
    "        print(f\"Expected scores:\\n{expected_scores}\")\n",
    "        \n",
    "        self.assertTrue(torch.allclose(s, expected_scores), \"Scores do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(s) == 0), \"Diagonal of scores matrix should be zero.\")\n",
    "\n",
    "    def test_bernoulli_soft_gmat(self):\n",
    "        \"\"\"Tests the sigmoid transformation of scores to get probabilities.\"\"\"\n",
    "        # Calculate scores first\n",
    "        s = scores(self.z, self.alpha)\n",
    "        \n",
    "        # Manually calculate expected probabilities\n",
    "        expected_probs = torch.sigmoid(s)\n",
    "        \n",
    "        # Get probabilities from function\n",
    "        g_soft = bernoulli_soft_gmat(self.z, self.hparams)\n",
    "        print(f\"\\nCalculated soft G-matrix:\\n{g_soft}\")\n",
    "        print(f\"Expected soft G-matrix:\\n{expected_probs}\")\n",
    "        expected_probs.fill_diagonal_(0)\n",
    "        self.assertTrue(torch.allclose(g_soft, expected_probs), \"Soft G-matrix probabilities do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(g_soft) == 0), \"Diagonal of soft G-matrix should be zero.\")\n",
    "\n",
    "\n",
    "# --- Running the tests ---\n",
    "# This allows running the tests directly from the cell.\n",
    "suite = unittest.TestSuite()\n",
    "print(\"--- Running tests for acyclic_constr ---\")\n",
    "suite.addTest(unittest.makeSuite(TestAcyclicConstraint))\n",
    "print(\"\\n--- Running tests for Graph Generation ---\")\n",
    "suite.addTest(unittest.makeSuite(TestGraphGeneration))\n",
    "\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing Simple Linear Model Log Joint\n",
      "======================================================================\n",
      "Data: 100 samples, 3 features\n",
      "True theta: tensor([ 2.0000, -1.5000,  0.8000])\n",
      "\n",
      "1. Testing log joint evaluation:\n",
      "   Log joint at true theta: 0.826130\n",
      "   Log likelihood: 0.883105\n",
      "   Log prior: -0.056975\n",
      "   Sum: 0.826130\n",
      "\n",
      "2. Testing gradient computation:\n",
      "   Gradient at zero: tensor([ 235.3849, -119.4812,   69.8788])\n",
      "   Gradient norm: 273.065704\n",
      "   Gradient direction vs true theta direction:\n",
      "   Normalized gradient: tensor([ 0.8620, -0.4376,  0.2559])\n",
      "   Normalized true theta: tensor([ 0.7619, -0.5715,  0.3048])\n",
      "\n",
      "3. Testing gradient ascent optimization:\n",
      "   Iteration 0: log_joint = -350.740051, theta = tensor([ 0.2354, -0.1195,  0.0699])\n",
      "   Gradient: tensor([ 235.3849, -119.4812,   69.8788])\n",
      "   Iteration 1: log_joint = -280.228943, theta = tensor([ 0.4431, -0.2297,  0.1329])\n",
      "   Gradient: tensor([ 207.6687, -110.2470,   63.0065])\n",
      "   Iteration 2: log_joint = -224.182434, theta = tensor([ 0.6263, -0.3314,  0.1898])\n",
      "   Gradient: tensor([ 183.2248, -101.6599,   56.8947])\n",
      "   Iteration 3: log_joint = -179.574722, theta = tensor([ 0.7879, -0.4251,  0.2412])\n",
      "   Gradient: tensor([161.6650, -93.6850,  51.4517])\n",
      "   Iteration 4: log_joint = -144.023575, theta = tensor([ 0.9306, -0.5114,  0.2878])\n",
      "   Gradient: tensor([142.6479, -86.2874,  46.5973])\n",
      "   Iteration 5: log_joint = -115.651581, theta = tensor([ 1.0565, -0.5908,  0.3301])\n",
      "   Gradient: tensor([125.8724, -79.4327,  42.2617])\n",
      "   Iteration 6: log_joint = -92.977478, theta = tensor([ 1.1675, -0.6639,  0.3685])\n",
      "   Gradient: tensor([111.0732, -73.0876,  38.3836])\n",
      "   Iteration 7: log_joint = -74.831360, theta = tensor([ 1.2656, -0.7311,  0.4034])\n",
      "   Gradient: tensor([ 98.0166, -67.2194,  34.9095])\n",
      "   Iteration 8: log_joint = -60.288193, theta = tensor([ 1.3521, -0.7929,  0.4352])\n",
      "   Gradient: tensor([ 86.4968, -61.7971,  31.7926])\n",
      "   Iteration 9: log_joint = -48.615669, theta = tensor([ 1.4284, -0.8497,  0.4642])\n",
      "   Gradient: tensor([ 76.3322, -56.7905,  28.9918])\n",
      "   Iteration 10: log_joint = -39.233410, theta = tensor([ 1.4957, -0.9019,  0.4906])\n",
      "   Gradient: tensor([ 67.3628, -52.1713,  26.4711])\n",
      "   Iteration 11: log_joint = -31.680845, theta = tensor([ 1.5552, -0.9498,  0.5148])\n",
      "   Gradient: tensor([ 59.4478, -47.9123,  24.1990])\n",
      "   Iteration 12: log_joint = -25.592068, theta = tensor([ 1.6077, -0.9938,  0.5370])\n",
      "   Gradient: tensor([ 52.4628, -43.9878,  22.1479])\n",
      "   Iteration 13: log_joint = -20.676001, theta = tensor([ 1.6540, -1.0341,  0.5573])\n",
      "   Gradient: tensor([ 46.2982, -40.3737,  20.2934])\n",
      "   Iteration 14: log_joint = -16.700768, theta = tensor([ 1.6948, -1.0712,  0.5759])\n",
      "   Gradient: tensor([ 40.8574, -37.0471,  18.6141])\n",
      "   Iteration 15: log_joint = -13.481428, theta = tensor([ 1.7309, -1.1052,  0.5930])\n",
      "   Gradient: tensor([ 36.0552, -33.9868,  17.0911])\n",
      "   Iteration 16: log_joint = -10.870297, theta = tensor([ 1.7627, -1.1363,  0.6087])\n",
      "   Gradient: tensor([ 31.8166, -31.1726,  15.7080])\n",
      "   Iteration 17: log_joint = -8.749246, theta = tensor([ 1.7908, -1.1649,  0.6231])\n",
      "   Gradient: tensor([ 28.0751, -28.5859,  14.4500])\n",
      "   Iteration 18: log_joint = -7.023686, theta = tensor([ 1.8155, -1.1911,  0.6364])\n",
      "   Gradient: tensor([ 24.7726, -26.2093,  13.3042])\n",
      "   Iteration 19: log_joint = -5.617748, theta = tensor([ 1.8374, -1.2152,  0.6487])\n",
      "   Gradient: tensor([ 21.8573, -24.0264,  12.2592])\n",
      "   Iteration 20: log_joint = -4.470505, theta = tensor([ 1.8567, -1.2372,  0.6600])\n",
      "   Gradient: tensor([ 19.2838, -22.0221,  11.3049])\n",
      "   Iteration 21: log_joint = -3.532960, theta = tensor([ 1.8737, -1.2574,  0.6704])\n",
      "   Gradient: tensor([ 17.0121, -20.1824,  10.4323])\n",
      "   Iteration 22: log_joint = -2.765649, theta = tensor([ 1.8887, -1.2759,  0.6801])\n",
      "   Gradient: tensor([ 15.0067, -18.4942,   9.6334])\n",
      "   Iteration 23: log_joint = -2.136741, theta = tensor([ 1.9019, -1.2928,  0.6890])\n",
      "   Gradient: tensor([ 13.2363, -16.9455,   8.9012])\n",
      "   Iteration 24: log_joint = -1.620524, theta = tensor([ 1.9136, -1.3083,  0.6972])\n",
      "   Gradient: tensor([ 11.6736, -15.5251,   8.2293])\n",
      "   Iteration 25: log_joint = -1.196202, theta = tensor([ 1.9239, -1.3226,  0.7048])\n",
      "   Gradient: tensor([ 10.2940, -14.2227,   7.6122])\n",
      "   Iteration 26: log_joint = -0.846923, theta = tensor([ 1.9330, -1.3356,  0.7119])\n",
      "   Gradient: tensor([  9.0763, -13.0286,   7.0447])\n",
      "   Iteration 27: log_joint = -0.559018, theta = tensor([ 1.9410, -1.3475,  0.7184])\n",
      "   Gradient: tensor([  8.0014, -11.9341,   6.5225])\n",
      "   Iteration 28: log_joint = -0.321379, theta = tensor([ 1.9480, -1.3584,  0.7244])\n",
      "   Gradient: tensor([  7.0526, -10.9311,   6.0415])\n",
      "   Iteration 29: log_joint = -0.124968, theta = tensor([ 1.9542, -1.3685,  0.7300])\n",
      "   Gradient: tensor([  6.2151, -10.0120,   5.5980])\n",
      "   Iteration 30: log_joint = 0.037579, theta = tensor([ 1.9597, -1.3776,  0.7352])\n",
      "   Gradient: tensor([ 5.4760, -9.1699,  5.1888])\n",
      "   Iteration 31: log_joint = 0.172274, theta = tensor([ 1.9645, -1.3860,  0.7400])\n",
      "   Gradient: tensor([ 4.8237, -8.3985,  4.8110])\n",
      "   Iteration 32: log_joint = 0.284028, theta = tensor([ 1.9688, -1.3937,  0.7445])\n",
      "   Gradient: tensor([ 4.2481, -7.6919,  4.4620])\n",
      "   Iteration 33: log_joint = 0.376861, theta = tensor([ 1.9725, -1.4008,  0.7486])\n",
      "   Gradient: tensor([ 3.7401, -7.0447,  4.1393])\n",
      "   Iteration 34: log_joint = 0.454068, theta = tensor([ 1.9758, -1.4072,  0.7525])\n",
      "   Gradient: tensor([ 3.2920, -6.4520,  3.8408])\n",
      "   Iteration 35: log_joint = 0.518352, theta = tensor([ 1.9787, -1.4131,  0.7560])\n",
      "   Gradient: tensor([ 2.8966, -5.9093,  3.5645])\n",
      "   Iteration 36: log_joint = 0.571938, theta = tensor([ 1.9813, -1.4185,  0.7593])\n",
      "   Gradient: tensor([ 2.5479, -5.4123,  3.3087])\n",
      "   Iteration 37: log_joint = 0.616655, theta = tensor([ 1.9835, -1.4235,  0.7624])\n",
      "   Gradient: tensor([ 2.2403, -4.9572,  3.0717])\n",
      "   Iteration 38: log_joint = 0.654009, theta = tensor([ 1.9855, -1.4280,  0.7653])\n",
      "   Gradient: tensor([ 1.9691, -4.5405,  2.8521])\n",
      "   Iteration 39: log_joint = 0.685245, theta = tensor([ 1.9872, -1.4322,  0.7679])\n",
      "   Gradient: tensor([ 1.7299, -4.1590,  2.6484])\n",
      "   Iteration 40: log_joint = 0.711390, theta = tensor([ 1.9887, -1.4360,  0.7704])\n",
      "   Gradient: tensor([ 1.5191, -3.8097,  2.4596])\n",
      "   Iteration 41: log_joint = 0.733295, theta = tensor([ 1.9901, -1.4395,  0.7727])\n",
      "   Gradient: tensor([ 1.3333, -3.4899,  2.2844])\n",
      "   Iteration 42: log_joint = 0.751665, theta = tensor([ 1.9912, -1.4427,  0.7748])\n",
      "   Gradient: tensor([ 1.1696, -3.1972,  2.1218])\n",
      "   Iteration 43: log_joint = 0.767084, theta = tensor([ 1.9923, -1.4456,  0.7768])\n",
      "   Gradient: tensor([ 1.0254, -2.9291,  1.9709])\n",
      "   Iteration 44: log_joint = 0.780037, theta = tensor([ 1.9931, -1.4483,  0.7786])\n",
      "   Gradient: tensor([ 0.8983, -2.6837,  1.8308])\n",
      "   Iteration 45: log_joint = 0.790927, theta = tensor([ 1.9939, -1.4508,  0.7803])\n",
      "   Gradient: tensor([ 0.7865, -2.4590,  1.7007])\n",
      "   Iteration 46: log_joint = 0.800091, theta = tensor([ 1.9946, -1.4530,  0.7819])\n",
      "   Gradient: tensor([ 0.6881, -2.2533,  1.5799])\n",
      "   Iteration 47: log_joint = 0.807807, theta = tensor([ 1.9952, -1.4551,  0.7833])\n",
      "   Gradient: tensor([ 0.6014, -2.0649,  1.4677])\n",
      "   Iteration 48: log_joint = 0.814309, theta = tensor([ 1.9958, -1.4570,  0.7847])\n",
      "   Gradient: tensor([ 0.5253, -1.8924,  1.3634])\n",
      "   Iteration 49: log_joint = 0.819792, theta = tensor([ 1.9962, -1.4587,  0.7860])\n",
      "   Gradient: tensor([ 0.4583, -1.7345,  1.2666])\n",
      "   Iteration 50: log_joint = 0.824418, theta = tensor([ 1.9966, -1.4603,  0.7871])\n",
      "   Gradient: tensor([ 0.3994, -1.5898,  1.1766])\n",
      "   Iteration 51: log_joint = 0.828325, theta = tensor([ 1.9970, -1.4618,  0.7882])\n",
      "   Gradient: tensor([ 0.3477, -1.4574,  1.0930])\n",
      "   Iteration 52: log_joint = 0.831626, theta = tensor([ 1.9973, -1.4631,  0.7892])\n",
      "   Gradient: tensor([ 0.3023, -1.3361,  1.0153])\n",
      "   Iteration 53: log_joint = 0.834417, theta = tensor([ 1.9975, -1.4643,  0.7902])\n",
      "   Gradient: tensor([ 0.2625, -1.2250,  0.9430])\n",
      "   Iteration 54: log_joint = 0.836777, theta = tensor([ 1.9977, -1.4654,  0.7911])\n",
      "   Gradient: tensor([ 0.2276, -1.1233,  0.8759])\n",
      "   Iteration 55: log_joint = 0.838774, theta = tensor([ 1.9979, -1.4665,  0.7919])\n",
      "   Gradient: tensor([ 0.1970, -1.0301,  0.8136])\n",
      "   Iteration 56: log_joint = 0.840466, theta = tensor([ 1.9981, -1.4674,  0.7926])\n",
      "   Gradient: tensor([ 0.1702, -0.9447,  0.7556])\n",
      "   Iteration 57: log_joint = 0.841899, theta = tensor([ 1.9983, -1.4683,  0.7933])\n",
      "   Gradient: tensor([ 0.1467, -0.8665,  0.7018])\n",
      "   Iteration 58: log_joint = 0.843114, theta = tensor([ 1.9984, -1.4691,  0.7940])\n",
      "   Gradient: tensor([ 0.1262, -0.7948,  0.6517])\n",
      "   Iteration 59: log_joint = 0.844144, theta = tensor([ 1.9985, -1.4698,  0.7946])\n",
      "   Gradient: tensor([ 0.1083, -0.7292,  0.6052])\n",
      "   Iteration 60: log_joint = 0.845017, theta = tensor([ 1.9986, -1.4705,  0.7952])\n",
      "   Gradient: tensor([ 0.0926, -0.6690,  0.5619])\n",
      "   Iteration 61: log_joint = 0.845759, theta = tensor([ 1.9987, -1.4711,  0.7957])\n",
      "   Gradient: tensor([ 0.0790, -0.6138,  0.5218])\n",
      "   Iteration 62: log_joint = 0.846389, theta = tensor([ 1.9987, -1.4717,  0.7962])\n",
      "   Gradient: tensor([ 0.0671, -0.5633,  0.4844])\n",
      "   Iteration 63: log_joint = 0.846923, theta = tensor([ 1.9988, -1.4722,  0.7966])\n",
      "   Gradient: tensor([ 0.0568, -0.5170,  0.4498])\n",
      "   Iteration 64: log_joint = 0.847378, theta = tensor([ 1.9988, -1.4726,  0.7970])\n",
      "   Gradient: tensor([ 0.0478, -0.4745,  0.4175])\n",
      "   Iteration 65: log_joint = 0.847764, theta = tensor([ 1.9989, -1.4731,  0.7974])\n",
      "   Gradient: tensor([ 0.0400, -0.4356,  0.3876])\n",
      "   Iteration 66: log_joint = 0.848092, theta = tensor([ 1.9989, -1.4735,  0.7978])\n",
      "   Gradient: tensor([ 0.0333, -0.3999,  0.3598])\n",
      "   Iteration 67: log_joint = 0.848371, theta = tensor([ 1.9989, -1.4738,  0.7981])\n",
      "   Gradient: tensor([ 0.0274, -0.3672,  0.3339])\n",
      "   Iteration 68: log_joint = 0.848609, theta = tensor([ 1.9990, -1.4742,  0.7984])\n",
      "   Gradient: tensor([ 0.0224, -0.3372,  0.3099])\n",
      "   Iteration 69: log_joint = 0.848811, theta = tensor([ 1.9990, -1.4745,  0.7987])\n",
      "   Gradient: tensor([ 0.0181, -0.3096,  0.2876])\n",
      "   Iteration 70: log_joint = 0.848983, theta = tensor([ 1.9990, -1.4748,  0.7990])\n",
      "   Gradient: tensor([ 0.0144, -0.2844,  0.2669])\n",
      "   Iteration 71: log_joint = 0.849129, theta = tensor([ 1.9990, -1.4750,  0.7992])\n",
      "   Gradient: tensor([ 0.0112, -0.2612,  0.2477])\n",
      "   Iteration 72: log_joint = 0.849254, theta = tensor([ 1.9990, -1.4753,  0.7994])\n",
      "   Gradient: tensor([ 0.0085, -0.2400,  0.2298])\n",
      "   Iteration 73: log_joint = 0.849360, theta = tensor([ 1.9990, -1.4755,  0.7997])\n",
      "   Gradient: tensor([ 0.0062, -0.2205,  0.2132])\n",
      "   Iteration 74: log_joint = 0.849451, theta = tensor([ 1.9990, -1.4757,  0.7999])\n",
      "   Gradient: tensor([ 0.0042, -0.2026,  0.1978])\n",
      "   Iteration 75: log_joint = 0.849528, theta = tensor([ 1.9990, -1.4759,  0.8000])\n",
      "   Gradient: tensor([ 0.0026, -0.1862,  0.1835])\n",
      "   Iteration 76: log_joint = 0.849593, theta = tensor([ 1.9990, -1.4761,  0.8002])\n",
      "   Gradient: tensor([ 0.0012, -0.1711,  0.1702])\n",
      "   Iteration 77: log_joint = 0.849649, theta = tensor([ 1.9990, -1.4762,  0.8004])\n",
      "   Gradient: tensor([ 7.4031e-05, -1.5728e-01,  1.5788e-01])\n",
      "   Iteration 78: log_joint = 0.849697, theta = tensor([ 1.9990, -1.4764,  0.8005])\n",
      "   Gradient: tensor([-0.0009, -0.1446,  0.1464])\n",
      "   Iteration 79: log_joint = 0.849738, theta = tensor([ 1.9990, -1.4765,  0.8007])\n",
      "   Gradient: tensor([-0.0016, -0.1329,  0.1358])\n",
      "   Iteration 80: log_joint = 0.849773, theta = tensor([ 1.9990, -1.4766,  0.8008])\n",
      "   Gradient: tensor([-0.0023, -0.1222,  0.1259])\n",
      "   Iteration 81: log_joint = 0.849803, theta = tensor([ 1.9990, -1.4767,  0.8009])\n",
      "   Gradient: tensor([-0.0028, -0.1124,  0.1168])\n",
      "   Iteration 82: log_joint = 0.849828, theta = tensor([ 1.9990, -1.4768,  0.8010])\n",
      "   Gradient: tensor([-0.0032, -0.1033,  0.1083])\n",
      "   Iteration 83: log_joint = 0.849849, theta = tensor([ 1.9990, -1.4769,  0.8011])\n",
      "   Gradient: tensor([-0.0035, -0.0950,  0.1004])\n",
      "   Iteration 84: log_joint = 0.849868, theta = tensor([ 1.9990, -1.4770,  0.8012])\n",
      "   Gradient: tensor([-0.0037, -0.0874,  0.0931])\n",
      "   Iteration 85: log_joint = 0.849883, theta = tensor([ 1.9990, -1.4771,  0.8013])\n",
      "   Gradient: tensor([-0.0038, -0.0804,  0.0863])\n",
      "   Iteration 86: log_joint = 0.849897, theta = tensor([ 1.9990, -1.4772,  0.8014])\n",
      "   Gradient: tensor([-0.0039, -0.0740,  0.0800])\n",
      "   Iteration 87: log_joint = 0.849908, theta = tensor([ 1.9990, -1.4772,  0.8014])\n",
      "   Gradient: tensor([-0.0039, -0.0681,  0.0742])\n",
      "   Iteration 88: log_joint = 0.849918, theta = tensor([ 1.9990, -1.4773,  0.8015])\n",
      "   Gradient: tensor([-0.0040, -0.0626,  0.0688])\n",
      "   Iteration 89: log_joint = 0.849926, theta = tensor([ 1.9990, -1.4774,  0.8016])\n",
      "   Gradient: tensor([-0.0039, -0.0577,  0.0637])\n",
      "   Iteration 90: log_joint = 0.849933, theta = tensor([ 1.9990, -1.4774,  0.8016])\n",
      "   Gradient: tensor([-0.0039, -0.0531,  0.0591])\n",
      "   Iteration 91: log_joint = 0.849939, theta = tensor([ 1.9990, -1.4775,  0.8017])\n",
      "   Gradient: tensor([-0.0038, -0.0488,  0.0548])\n",
      "   Iteration 92: log_joint = 0.849944, theta = tensor([ 1.9990, -1.4775,  0.8017])\n",
      "   Gradient: tensor([-0.0037, -0.0450,  0.0507])\n",
      "   Iteration 93: log_joint = 0.849949, theta = tensor([ 1.9990, -1.4775,  0.8018])\n",
      "   Gradient: tensor([-0.0036, -0.0414,  0.0470])\n",
      "   Iteration 94: log_joint = 0.849953, theta = tensor([ 1.9990, -1.4776,  0.8018])\n",
      "   Gradient: tensor([-0.0035, -0.0381,  0.0436])\n",
      "   Iteration 95: log_joint = 0.849956, theta = tensor([ 1.9990, -1.4776,  0.8019])\n",
      "   Gradient: tensor([-0.0034, -0.0351,  0.0404])\n",
      "   Iteration 96: log_joint = 0.849959, theta = tensor([ 1.9990, -1.4776,  0.8019])\n",
      "   Gradient: tensor([-0.0032, -0.0323,  0.0374])\n",
      "   Iteration 97: log_joint = 0.849961, theta = tensor([ 1.9990, -1.4777,  0.8019])\n",
      "   Gradient: tensor([-0.0031, -0.0298,  0.0347])\n",
      "   Iteration 98: log_joint = 0.849963, theta = tensor([ 1.9990, -1.4777,  0.8020])\n",
      "   Gradient: tensor([-0.0030, -0.0274,  0.0321])\n",
      "   Iteration 99: log_joint = 0.849965, theta = tensor([ 1.9990, -1.4777,  0.8020])\n",
      "   Gradient: tensor([-0.0028, -0.0252,  0.0298])\n",
      "   Iteration 100: log_joint = 0.849966, theta = tensor([ 1.9990, -1.4778,  0.8020])\n",
      "   Gradient: tensor([-0.0027, -0.0233,  0.0276])\n",
      "   Iteration 101: log_joint = 0.849968, theta = tensor([ 1.9990, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.0025, -0.0214,  0.0255])\n",
      "   Iteration 102: log_joint = 0.849969, theta = tensor([ 1.9990, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.0024, -0.0197,  0.0237])\n",
      "   Iteration 103: log_joint = 0.849970, theta = tensor([ 1.9989, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.0023, -0.0182,  0.0219])\n",
      "   Iteration 104: log_joint = 0.849970, theta = tensor([ 1.9989, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.0022, -0.0167,  0.0203])\n",
      "   Iteration 105: log_joint = 0.849971, theta = tensor([ 1.9989, -1.4778,  0.8021])\n",
      "   Gradient: tensor([-0.0021, -0.0154,  0.0188])\n",
      "   Iteration 106: log_joint = 0.849972, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0020, -0.0142,  0.0174])\n",
      "   Iteration 107: log_joint = 0.849972, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0019, -0.0131,  0.0161])\n",
      "   Iteration 108: log_joint = 0.849973, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0017, -0.0121,  0.0149])\n",
      "   Iteration 109: log_joint = 0.849973, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0016, -0.0112,  0.0138])\n",
      "   Iteration 110: log_joint = 0.849973, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0015, -0.0103,  0.0128])\n",
      "   Iteration 111: log_joint = 0.849973, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0014, -0.0095,  0.0119])\n",
      "   Iteration 112: log_joint = 0.849974, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0014, -0.0087,  0.0110])\n",
      "   Iteration 113: log_joint = 0.849974, theta = tensor([ 1.9989, -1.4779,  0.8022])\n",
      "   Gradient: tensor([-0.0013, -0.0081,  0.0102])\n",
      "   Iteration 114: log_joint = 0.849974, theta = tensor([ 1.9989, -1.4779,  0.8023])\n",
      "   Gradient: tensor([-0.0012, -0.0074,  0.0094])\n",
      "   Iteration 115: log_joint = 0.849974, theta = tensor([ 1.9989, -1.4779,  0.8023])\n",
      "   Gradient: tensor([-0.0011, -0.0069,  0.0087])\n",
      "   Iteration 116: log_joint = 0.849974, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0011, -0.0063,  0.0081])\n",
      "   Iteration 117: log_joint = 0.849974, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0010, -0.0058,  0.0075])\n",
      "   Iteration 118: log_joint = 0.849974, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0009, -0.0054,  0.0069])\n",
      "   Iteration 119: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0009, -0.0050,  0.0064])\n",
      "   Iteration 120: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0008, -0.0046,  0.0059])\n",
      "   Iteration 121: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0008, -0.0042,  0.0055])\n",
      "   Iteration 122: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0007, -0.0039,  0.0051])\n",
      "   Iteration 123: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0007, -0.0036,  0.0047])\n",
      "   Iteration 124: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0006, -0.0033,  0.0044])\n",
      "   Iteration 125: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0006, -0.0031,  0.0040])\n",
      "   Iteration 126: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0005, -0.0028,  0.0037])\n",
      "   Iteration 127: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0005, -0.0026,  0.0035])\n",
      "   Iteration 128: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0005, -0.0024,  0.0032])\n",
      "   Iteration 129: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0004, -0.0022,  0.0030])\n",
      "   Iteration 130: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0004, -0.0020,  0.0027])\n",
      "   Iteration 131: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0004, -0.0019,  0.0025])\n",
      "   Iteration 132: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0004, -0.0017,  0.0023])\n",
      "   Iteration 133: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0003, -0.0016,  0.0022])\n",
      "   Iteration 134: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8023])\n",
      "   Gradient: tensor([-0.0003, -0.0015,  0.0020])\n",
      "   Iteration 135: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0003, -0.0014,  0.0019])\n",
      "   Iteration 136: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0003, -0.0013,  0.0017])\n",
      "   Iteration 137: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0003, -0.0012,  0.0016])\n",
      "   Iteration 138: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0011,  0.0015])\n",
      "   Iteration 139: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0010,  0.0014])\n",
      "   Iteration 140: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0009,  0.0013])\n",
      "   Iteration 141: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0008,  0.0012])\n",
      "   Iteration 142: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0008,  0.0011])\n",
      "   Iteration 143: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0007,  0.0010])\n",
      "   Iteration 144: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0007,  0.0009])\n",
      "   Iteration 145: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0002, -0.0006,  0.0009])\n",
      "   Iteration 146: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0001, -0.0006,  0.0008])\n",
      "   Iteration 147: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0001, -0.0005,  0.0007])\n",
      "   Iteration 148: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0001, -0.0005,  0.0007])\n",
      "   Iteration 149: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0001, -0.0004,  0.0006])\n",
      "   Iteration 150: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-0.0001, -0.0004,  0.0006])\n",
      "   Iteration 151: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-9.5763e-05, -3.8735e-04,  5.3962e-04])\n",
      "   Iteration 152: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-9.0832e-05, -3.5887e-04,  4.9995e-04])\n",
      "   Iteration 153: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-8.1310e-05, -3.3254e-04,  4.6286e-04])\n",
      "   Iteration 154: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-6.9598e-05, -3.0418e-04,  4.2718e-04])\n",
      "   Iteration 155: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-6.0865e-05, -2.7333e-04,  3.9779e-04])\n",
      "   Iteration 156: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.6365e-05, -2.5759e-04,  3.6780e-04])\n",
      "   Iteration 157: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.7423e-05, -2.4035e-04,  3.3972e-04])\n",
      "   Iteration 158: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3191e-05, -2.2401e-04,  3.1432e-04])\n",
      "   Iteration 159: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.2581e-05, -2.0424e-04,  2.9043e-04])\n",
      "   Iteration 160: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.6485e-05, -1.8602e-04,  2.6805e-04])\n",
      "   Iteration 161: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.7350e-05, -1.6657e-04,  2.5247e-04])\n",
      "   Iteration 162: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.7991e-05, -1.5746e-04,  2.3188e-04])\n",
      "   Iteration 163: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.8454e-05, -1.5147e-04,  2.1326e-04])\n",
      "   Iteration 164: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.9824e-05, -1.4228e-04,  1.9334e-04])\n",
      "   Iteration 165: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3105e-05, -1.3309e-04,  1.8031e-04])\n",
      "   Iteration 166: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3254e-05, -1.2393e-04,  1.6557e-04])\n",
      "   Iteration 167: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.4640e-05, -1.1746e-04,  1.5177e-04])\n",
      "   Iteration 168: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.6279e-05, -1.0813e-04,  1.3752e-04])\n",
      "   Iteration 169: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.7993e-05, -9.8488e-05,  1.3060e-04])\n",
      "   Iteration 170: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.7069e-05, -8.7867e-05,  1.2105e-04])\n",
      "   Iteration 171: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.8873e-05, -7.7363e-05,  1.1119e-04])\n",
      "   Iteration 172: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.9946e-05, -6.7576e-05,  1.0393e-04])\n",
      "   Iteration 173: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.5297e-05, -5.7928e-05,  9.6413e-05])\n",
      "   Iteration 174: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.6191e-05, -5.8591e-05,  8.7524e-05])\n",
      "   Iteration 175: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.7801e-05, -6.0711e-05,  8.1037e-05])\n",
      "   Iteration 176: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.3480e-05, -5.1631e-05,  7.7744e-05])\n",
      "   Iteration 177: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.3331e-05, -5.3278e-05,  7.3259e-05])\n",
      "   Iteration 178: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.4448e-05, -5.4321e-05,  6.7894e-05])\n",
      "   Iteration 179: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.6147e-05, -5.4395e-05,  6.2597e-05])\n",
      "   Iteration 180: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.7473e-05, -5.4155e-05,  5.8253e-05])\n",
      "   Iteration 181: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.8413e-05, -5.4890e-05,  5.3547e-05])\n",
      "   Iteration 182: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-4.9456e-05, -5.6380e-05,  4.7176e-05])\n",
      "   Iteration 183: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.0112e-05, -5.6641e-05,  4.1812e-05])\n",
      "   Iteration 184: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.0991e-05, -5.7430e-05,  3.6142e-05])\n",
      "   Iteration 185: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.1796e-05, -5.8607e-05,  3.2565e-05])\n",
      "   Iteration 186: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 187: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 188: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 189: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 190: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 191: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 192: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 193: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 194: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 195: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 196: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 197: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 198: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 199: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 200: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 201: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 202: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 203: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 204: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 205: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 206: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 207: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 208: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 209: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 210: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 211: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 212: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 213: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 214: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 215: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 216: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 217: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 218: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 219: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 220: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 221: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 222: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 223: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 224: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 225: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 226: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 227: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 228: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 229: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 230: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 231: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 232: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 233: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 234: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 235: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 236: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 237: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 238: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 239: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 240: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 241: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 242: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 243: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 244: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 245: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 246: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 247: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 248: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 249: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 250: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 251: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 252: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 253: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 254: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 255: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 256: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 257: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 258: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 259: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 260: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 261: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 262: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 263: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 264: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 265: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 266: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 267: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 268: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 269: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 270: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 271: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 272: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 273: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 274: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 275: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 276: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 277: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 278: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 279: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 280: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 281: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 282: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 283: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 284: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 285: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 286: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 287: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 288: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 289: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 290: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 291: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 292: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 293: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 294: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 295: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 296: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 297: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 298: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 299: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 300: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 301: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 302: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 303: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 304: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 305: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 306: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 307: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 308: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 309: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 310: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 311: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 312: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 313: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 314: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 315: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 316: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 317: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 318: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 319: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 320: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 321: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 322: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 323: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 324: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 325: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 326: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 327: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 328: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 329: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 330: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 331: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 332: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 333: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 334: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 335: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 336: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 337: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 338: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 339: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 340: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 341: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 342: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 343: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 344: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 345: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 346: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 347: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 348: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 349: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 350: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 351: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 352: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 353: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 354: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 355: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 356: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 357: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 358: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 359: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 360: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 361: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 362: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 363: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 364: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 365: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 366: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 367: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 368: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 369: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 370: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 371: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 372: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 373: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 374: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 375: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 376: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 377: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 378: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 379: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 380: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 381: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 382: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 383: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 384: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 385: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 386: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 387: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 388: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 389: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 390: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 391: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 392: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 393: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 394: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 395: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 396: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 397: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 398: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 399: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 400: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 401: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 402: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 403: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 404: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 405: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 406: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 407: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 408: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 409: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 410: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 411: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 412: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 413: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 414: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 415: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 416: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 417: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 418: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 419: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 420: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 421: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 422: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 423: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 424: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 425: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 426: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 427: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 428: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 429: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 430: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 431: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 432: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 433: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 434: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 435: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 436: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 437: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 438: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 439: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 440: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 441: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 442: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 443: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 444: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 445: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 446: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 447: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 448: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 449: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 450: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 451: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 452: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 453: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 454: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 455: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 456: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 457: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 458: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 459: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 460: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 461: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 462: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 463: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 464: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 465: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 466: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 467: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 468: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 469: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 470: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 471: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 472: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 473: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 474: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 475: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 476: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 477: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 478: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 479: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 480: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 481: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 482: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 483: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 484: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 485: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 486: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 487: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 488: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 489: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 490: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 491: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 492: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 493: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 494: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 495: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 496: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 497: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 498: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "   Iteration 499: log_joint = 0.849975, theta = tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Gradient: tensor([-5.3375e-05, -5.8823e-05,  2.7767e-05])\n",
      "\n",
      "   Final theta: tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   True theta:  tensor([ 2.0000, -1.5000,  0.8000])\n",
      "   Error: 0.022125\n",
      "   Initial log joint: -350.740051\n",
      "   Final log joint: 0.849975\n",
      "   Improvement: 351.590027\n",
      "\n",
      "4. Comparing with analytical solution:\n",
      "   Analytical MAP: tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Optimized theta: tensor([ 1.9989, -1.4780,  0.8024])\n",
      "   Difference: 0.000001\n",
      "   Log joint (analytical): 0.849975\n",
      "   Log joint (optimized): 0.849975\n",
      "\n",
      "5. Testing robustness with different starting points:\n",
      "   Start 1: theta=tensor([1., 1., 1.]), log_joint=-343.985046, grad_norm=258.558350\n",
      "   Start 2: theta=tensor([-2.0000,  0.5000, -1.0000]), log_joint=-1249.823730, grad_norm=527.130554\n",
      "   Start 3: theta=tensor([10., -5.,  2.]), log_joint=-4303.426758, grad_norm=982.983704\n",
      "\n",
      "======================================================================\n",
      "Simple Linear Model Log Joint Testing Complete\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def test_simple_linear_model_log_joint():\n",
    "    \"\"\"\n",
    "    Test log joint computation with a simple linear model:\n",
    "    - Linear model: y = X @ theta + noise\n",
    "    - Gaussian likelihood: p(y | X, theta) = N(X @ theta, sigma^2)\n",
    "    - Gaussian prior: p(theta) = N(0, sigma_prior^2)\n",
    "    - Use gradient ascent to maximize log p(theta | X, y)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Testing Simple Linear Model Log Joint\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Generate synthetic linear data\n",
    "    n_samples, n_features = 100, 3\n",
    "    true_theta = torch.tensor([2.0, -1.5, 0.8])\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    noise = torch.randn(n_samples) * 0.1\n",
    "    y = X @ true_theta + noise\n",
    "    \n",
    "    print(f\"Data: {n_samples} samples, {n_features} features\")\n",
    "    print(f\"True theta: {true_theta}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    sigma_likelihood = torch.tensor(0.1)  # Known noise level\n",
    "    sigma_prior = torch.tensor(2.0)       # Prior variance for theta\n",
    "    \n",
    "    def log_gaussian_likelihood_simple(y, X, theta, sigma):\n",
    "        \"\"\"Simple Gaussian likelihood for linear model\"\"\"\n",
    "        pred = X @ theta\n",
    "        residuals = y - pred\n",
    "        log_prob = -0.5 * torch.log(2 * torch.pi * sigma**2) - 0.5 * (residuals**2 / sigma**2)\n",
    "        return torch.sum(log_prob) \n",
    "    \n",
    "    def log_gaussian_prior_simple(theta, sigma):\n",
    "        \"\"\"Simple Gaussian prior for theta\"\"\"\n",
    "        log_prob = -0.5 * torch.log(2 * torch.pi * sigma**2) - 0.5 * (theta**2 / sigma**2)\n",
    "        return torch.sum(log_prob) \n",
    "    \n",
    "    def log_joint_simple(theta, X, y, sigma_lik, sigma_prior):\n",
    "        \"\"\"Log joint probability\"\"\"\n",
    "        log_lik = log_gaussian_likelihood_simple(y, X, theta, sigma_lik)\n",
    "        log_prior = log_gaussian_prior_simple(theta, sigma_prior)\n",
    "        return log_lik + log_prior\n",
    "    \n",
    "    # Test 1: Evaluate log joint at true parameters\n",
    "    print(\"\\n1. Testing log joint evaluation:\")\n",
    "    try:\n",
    "        log_joint_true = log_joint_simple(true_theta, X, y, sigma_likelihood, sigma_prior)\n",
    "        print(f\"   Log joint at true theta: {log_joint_true.item():.6f}\")\n",
    "        \n",
    "        # Test individual components\n",
    "        log_lik_true = log_gaussian_likelihood_simple(y, X, true_theta, sigma_likelihood)\n",
    "        log_prior_true = log_gaussian_prior_simple(true_theta, sigma_prior)\n",
    "        print(f\"   Log likelihood: {log_lik_true.item():.6f}\")\n",
    "        print(f\"   Log prior: {log_prior_true.item():.6f}\")\n",
    "        print(f\"   Sum: {(log_lik_true + log_prior_true).item():.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in log joint evaluation: {e}\")\n",
    "    \n",
    "    # Test 2: Gradient computation\n",
    "    print(\"\\n2. Testing gradient computation:\")\n",
    "    try:\n",
    "        theta_test = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
    "        log_joint_val = log_joint_simple(theta_test, X, y, sigma_likelihood, sigma_prior)\n",
    "        \n",
    "        # Compute gradient using autograd\n",
    "        grad = torch.autograd.grad(log_joint_val, theta_test)[0]\n",
    "        print(f\"   Gradient at zero: {grad}\")\n",
    "        print(f\"   Gradient norm: {grad.norm().item():.6f}\")\n",
    "        \n",
    "        # The gradient should point towards the true parameters\n",
    "        print(f\"   Gradient direction vs true theta direction:\")\n",
    "        print(f\"   Normalized gradient: {grad / grad.norm()}\")\n",
    "        print(f\"   Normalized true theta: {true_theta / true_theta.norm()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient computation: {e}\")\n",
    "    \n",
    "    # Test 3: Gradient ascent optimization\n",
    "    print(\"\\n3. Testing gradient ascent optimization:\")\n",
    "    try:\n",
    "        # Initialize parameters\n",
    "        theta_opt = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
    "        learning_rate = 1e-3\n",
    "        n_iterations = 500\n",
    "        \n",
    "        log_joints = []\n",
    "        thetas = []\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            # Zero gradients\n",
    "            if theta_opt.grad is not None:\n",
    "                theta_opt.grad.zero_()\n",
    "            \n",
    "            # Forward pass\n",
    "            log_joint_val = log_joint_simple(theta_opt, X, y, sigma_likelihood, sigma_prior)\n",
    "            \n",
    "            # Backward pass\n",
    "            log_joint_val.backward()\n",
    "            \n",
    "            # Store values\n",
    "            log_joints.append(log_joint_val.item())\n",
    "            thetas.append(theta_opt.detach().clone())\n",
    "            \n",
    "            # Gradient ascent step (maximize log joint)\n",
    "            with torch.no_grad():\n",
    "                theta_opt += learning_rate * theta_opt.grad\n",
    "            \n",
    "            if i % 1 == 0:\n",
    "                print(f\"   Iteration {i}: log_joint = {log_joint_val.item():.6f}, theta = {theta_opt.detach()}\")\n",
    "                print(f\"   Gradient: {theta_opt.grad}\")\n",
    "\n",
    "        print(f\"\\n   Final theta: {theta_opt.detach()}\")\n",
    "        print(f\"   True theta:  {true_theta}\")\n",
    "        print(f\"   Error: {(theta_opt.detach() - true_theta).norm().item():.6f}\")\n",
    "        \n",
    "        # Check if optimization improved\n",
    "        print(f\"   Initial log joint: {log_joints[0]:.6f}\")\n",
    "        print(f\"   Final log joint: {log_joints[-1]:.6f}\")\n",
    "        print(f\"   Improvement: {log_joints[-1] - log_joints[0]:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient ascent: {e}\")\n",
    "    \n",
    "    # Test 4: Compare with analytical solution\n",
    "    print(\"\\n4. Comparing with analytical solution:\")\n",
    "    try:\n",
    "        # For linear regression with Gaussian prior, the MAP estimate is:\n",
    "        # theta_MAP = (X^T X + (sigma_lik^2 / sigma_prior^2) * I)^{-1} X^T y\n",
    "        lambda_reg = (sigma_likelihood / sigma_prior) ** 2\n",
    "        XtX = X.T @ X\n",
    "        Xty = X.T @ y\n",
    "        \n",
    "        theta_analytical = torch.linalg.solve(XtX + lambda_reg * torch.eye(n_features), Xty)\n",
    "        \n",
    "        print(f\"   Analytical MAP: {theta_analytical}\")\n",
    "        print(f\"   Optimized theta: {theta_opt.detach()}\")\n",
    "        print(f\"   Difference: {(theta_analytical - theta_opt.detach()).norm().item():.6f}\")\n",
    "        \n",
    "        # Evaluate log joint at analytical solution\n",
    "        log_joint_analytical = log_joint_simple(theta_analytical, X, y, sigma_likelihood, sigma_prior)\n",
    "        print(f\"   Log joint (analytical): {log_joint_analytical.item():.6f}\")\n",
    "        print(f\"   Log joint (optimized): {log_joints[-1]:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in analytical comparison: {e}\")\n",
    "    \n",
    "    # Test 5: Test gradient with different starting points\n",
    "    print(\"\\n5. Testing robustness with different starting points:\")\n",
    "    starting_points = [\n",
    "        torch.tensor([1.0, 1.0, 1.0]),\n",
    "        torch.tensor([-2.0, 0.5, -1.0]),\n",
    "        torch.tensor([10.0, -5.0, 2.0])\n",
    "    ]\n",
    "    \n",
    "    for i, start_point in enumerate(starting_points):\n",
    "        try:\n",
    "            theta_test = start_point.clone().detach().requires_grad_(True)\n",
    "            log_joint_val = log_joint_simple(theta_test, X, y, sigma_likelihood, sigma_prior)\n",
    "            grad = torch.autograd.grad(log_joint_val, theta_test)[0]\n",
    "            \n",
    "            print(f\"   Start {i+1}: theta={start_point}, log_joint={log_joint_val.item():.6f}, grad_norm={grad.norm().item():.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR with starting point {i+1}: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Simple Linear Model Log Joint Testing Complete\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run the test\n",
    "test_simple_linear_model_log_joint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dibs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
