{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/dibs.py\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    try:\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except RuntimeError:\n",
    "        trace, p = torch.tensor(0.0, device=g.device, dtype=g.dtype), g.clone()\n",
    "        for k in range(1, min(d + 1, 20)):\n",
    "            trace += (alpha ** k) * torch.trace(p) / k\n",
    "            if k < 19:\n",
    "                p = p @ g\n",
    "        return trace\n",
    "\n",
    "\n",
    "def log_gaussian_likelihood(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    residuals = x - pred_mean\n",
    "    #old incorrect log_prob = -0.5 * (np.log(2 * np.pi) -  (1/2)* torch.log(sigma_tensor**2) -  0.5*(residuals / sigma_tensor) ** 2) old\n",
    "    log_prob = -0.5 * (torch.log(2 * torch.pi * sigma_tensor**2)) - 0.5 * ((residuals / sigma_tensor)**2)\n",
    "    #normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    #log_prob = normal_dist.log_prob(x)\n",
    "\n",
    "    return torch.sum(log_prob)\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    raw_scores = alpha * torch.einsum('...ik,...jk->...ij', u, v)\n",
    "    *batch_dims, d, _ = z.shape[:-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    if batch_dims:\n",
    "        diag_mask = diag_mask.expand(*batch_dims, d, d)\n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    if probs.ndim == 3:\n",
    "        diag_mask = diag_mask.expand(probs.shape[0], d, d)\n",
    "    return probs * diag_mask\n",
    "\n",
    "def gumbel_soft_gmat(z: torch.Tensor,\n",
    "                     hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Soft Gumbel–Softmax adjacency  (Eq. B.6)\n",
    "\n",
    "        g_ij  = σ_τ( L_ij + α⟨u_i , v_j⟩ )\n",
    "\n",
    "    where  L_ij ~ Logistic(0,1)  and  τ = hparams['tau']. appendix b2\n",
    "    \"\"\"\n",
    "    raw = scores(z, hparams[\"alpha\"])\n",
    "\n",
    "    # Logistic(0,1) noise   L = log U - log(1-U)\n",
    "    u = torch.rand_like(raw)\n",
    "    L = torch.log(u) - torch.log1p(-u)\n",
    "\n",
    "    logits = (raw + L) / hparams[\"tau\"]\n",
    "    g_soft = torch.sigmoid(logits)\n",
    "\n",
    "    d = g_soft.size(-1)\n",
    "    mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    return g_soft * mask\n",
    "\n",
    "def log_full_likelihood(data: Dict[str, Any], soft_gmat: torch.Tensor, theta: torch.Tensor, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    ## TODO: Expert belief: update this to use interventions, change the full likelihood \n",
    "    # and also add log bernoulli likelihood calculatior\n",
    "    x_data = data['x']\n",
    "    effective_W = theta * soft_gmat\n",
    "    pred_mean = torch.matmul(x_data, effective_W)\n",
    "    sigma_obs = hparams.get('sigma_obs_noise', 0.1)\n",
    "    return log_gaussian_likelihood(x_data, pred_mean, sigma=sigma_obs)\n",
    "\n",
    "def log_theta_prior(theta_effective: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    return log_gaussian_likelihood(theta_effective, torch.zeros_like(theta_effective), sigma=sigma)\n",
    "\n",
    "def gumbel_acyclic_constr_mc(z: torch.Tensor, d: int, hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    h_samples = []\n",
    "    for _ in range(hparams['n_nongrad_mc_samples']):\n",
    "        # FOR NOW, JUST GIVE THE SOFT MATRIX, AND BY ANNEALING IT TO HARD MATRIX\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "        h_samples.append(acyclic_constr(g_soft, d))\n",
    "        \n",
    "        # should gumbel soft gmat to hard gmat be done with >0.5 or with a sigmoid?  \n",
    "        #print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_soft shape: {g_soft.shape}, values: \\n {g_soft}')\n",
    "        #g_hard = torch.bernoulli(g_soft)\n",
    "        #if hparams['current_iteration'] % 1 == 0:\n",
    "        #    print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #print(f'g_hard shape: {g_hard.shape}, values: \\n {g_hard}')\n",
    "        #h_samples.append(acyclic_constr(g_hard, d))\n",
    "        #g_hard = (g_soft > 0.5).float()\n",
    "        #how about this  mentioned in dibs       g_ST   = g_hard + (g_soft - g_soft.detach())   # straight-through\n",
    "        \n",
    "        #TODO fix above\n",
    "        # for now use g_soft\n",
    "        \n",
    "    h_samples = torch.stack(h_samples)\n",
    "\n",
    "\n",
    "    return torch.mean(h_samples, dim=0)\n",
    "\n",
    "def grad_z_log_joint_gumbel(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    d = z.shape[0]\n",
    "    theta_const = theta\n",
    "    \n",
    "    #z.requires_grad_(True)\n",
    "    # --- Part 1: Prior Gradient ---\n",
    "    # MC estimate of gradient of acyclicity constraint using Gumbel soft graphs\n",
    "\n",
    "    h_mean = gumbel_acyclic_constr_mc(z, d, hparams)\n",
    "    grad_h_mc = torch.autograd.grad(h_mean, z)[0]\n",
    "    grad_log_z_prior_total = -hparams['beta'] * grad_h_mc - (z / hparams['sigma_z']**2)\n",
    "\n",
    "    # --- Part 2: Likelihood Gradient ---\n",
    "    \n",
    "    # 1. We need to collect the log-probability AND the gradient for each sample.\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "\n",
    "    for _ in range( hparams['n_grad_mc_samples']):\n",
    "        # 2. Generate a single soft graph sample.\n",
    "        g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "        # 3. Calculate the log-joint for this single sample.\n",
    "        log_density_one_sample = log_full_likelihood(data, g_soft, theta_const, hparams) + \\\n",
    "                                 log_theta_prior(theta_const * g_soft, hparams.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "        # 4. Calculate the gradient for this single sample.\n",
    "        # We must use retain_graph=True because we are doing a backward pass\n",
    "        # inside a loop, and PyTorch would otherwise free the graph memory.\n",
    "        grad, = torch.autograd.grad(log_density_one_sample, z, retain_graph=True)\n",
    "        \n",
    "        log_density_samples.append(log_density_one_sample)\n",
    "        grad_samples.append(grad)\n",
    "\n",
    "    # 5. After the loop, we can safely detach z_ from any further graph history.\n",
    "\n",
    "    # 6. Compute the final likelihood gradient using the stable weighted average.\n",
    "    # This correctly computes E[p*∇log(p)] / E[p]\n",
    "    log_p = torch.stack(log_density_samples)\n",
    "    grad_p = torch.stack(grad_samples)\n",
    "    grad_lik = weighted_grad(log_p, grad_p)\n",
    "\n",
    "\n",
    "    #if z.grad is not None:\n",
    "    #    z.grad.zero_()\n",
    "    #z.requires_grad_(False)\n",
    "    \n",
    "    # Final combined gradient\n",
    "    \n",
    "\n",
    "\n",
    "    total = grad_log_z_prior_total + grad_lik\n",
    "    # 3) Combine\n",
    "    # ------------------------------------------------\n",
    "    return total.detach()\n",
    "\n",
    "\n",
    "## SCORE BASED ESTIMATOR FOR GRADIENT Z \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Score-function estimator for ∇_Z log p(Z,Θ | D)\n",
    "#  (Section B.2 of the paper, b = 0)\n",
    "# ------------------------------------------------------------\n",
    "def analytic_score_g_given_z(z, g, hparams):\n",
    "    # 1. logits and probabilities\n",
    "    probs = bernoulli_soft_gmat(z, hparams)\n",
    "    diff   = g - probs                 # (g_ij − σ(s_ij))\n",
    "    u, v   = z[..., 0], z[..., 1]      # (d,k)\n",
    "\n",
    "    # 2. gradients wrt u and v\n",
    "    grad_u = hparams['alpha'] * torch.einsum('ij,jk->ik', diff, v)   # (d,k)\n",
    "    grad_v = hparams['alpha'] * torch.einsum('ij,ik->jk', diff, u)   # (d,k)\n",
    "\n",
    "    return torch.stack([grad_u, grad_v], dim=-1)          # (d,k,2)\n",
    "\n",
    "\n",
    "def grad_z_log_joint_score(z: torch.Tensor,\n",
    "                           theta: torch.Tensor,\n",
    "                           data: Dict[str, Any],\n",
    "                           hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ∇_Z log p(Z,Θ | D)  using the score-function (REINFORCE) estimator.\n",
    "\n",
    "    This replaces the Gumbel-soft estimator.\n",
    "    \"\"\"\n",
    "    sigma_z2 = hparams['sigma_z'] ** 2\n",
    "    beta     = hparams['beta']\n",
    "\n",
    "    M = hparams['n_grad_mc_samples'] # M = 50\n",
    "    d = z.shape[0]                   # d = 4\n",
    "    theta_const = theta\n",
    "\n",
    "\n",
    "    # 1. sample hard graphs \n",
    "    with torch.no_grad():\n",
    "        g_hard_samples = [torch.bernoulli(bernoulli_soft_gmat(z, hparams)) for _ in range(M)]\n",
    "\n",
    "    ll = []\n",
    "    scores = []\n",
    "    for g in g_hard_samples:\n",
    "        log_lik = log_full_likelihood(data, g, theta_const, hparams)\n",
    "        theta_eff = theta_const * g\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        \n",
    "        # log likelihood \n",
    "        ll.append(log_lik + log_theta_prior_val)\n",
    "        \n",
    "        # score \n",
    "        scores.append(analytic_score_g_given_z(z, g, hparams))\n",
    "    \n",
    "    log_p = torch.stack(ll)\n",
    "    grad_p = torch.stack(scores)\n",
    "\n",
    "    log_p_max = log_p.max()\n",
    "    log_p_shifted = log_p - log_p_max\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    unnormalized_w = torch.exp(log_p_shifted/10)\n",
    "    #print(f'unnormalized_w shape: {unnormalized_w.shape}, values: \\n {unnormalized_w}')\n",
    "    w = unnormalized_w / unnormalized_w.sum()\n",
    "                     # (M,1,1,...)\n",
    "    #print(f'w shape: {w.shape}, values: \\n {w}')\n",
    "\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                    \n",
    "    \n",
    "    ## compute the weighted avg \n",
    "    grad_lik = (w * grad_p).sum(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Z-prior: Gaussian + acyclicity penalty --------------\n",
    "    # gumbel is possible cuz no differentiable function in expectation \n",
    "    z_ = z.detach().clone().requires_grad_(True)\n",
    "    h_mean = gumbel_acyclic_constr_mc(z_, d, hparams)       # differentiable w.r.t z_\n",
    "    grad_h, = torch.autograd.grad(h_mean, z_, retain_graph=False)\n",
    "    grad_prior = -beta * grad_h - z_ / sigma_z2\n",
    "\n",
    "    return (grad_lik + grad_prior).detach()\n",
    "\n",
    "\n",
    "\n",
    "def weighted_grad(log_p: torch.Tensor,\n",
    "                  grad_p: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return   Σ softmax(log_p)_m * grad_p[m]\n",
    "    Shapes\n",
    "        log_p   : (M,)\n",
    "        grad_p  : (M, …)   (any extra dims)\n",
    "    \"\"\"\n",
    "    # 1. numerically stable soft-max weights\n",
    "    #print(f'log_p shape: {log_p.shape}, values:\\n {log_p}')\n",
    "    #print(f'grad_p shape: {grad_p.shape}, values: \\n{grad_p}')\n",
    "    log_p_shifted = log_p - log_p.max()          # (M,)\n",
    "    #print(f'log_p_shifted shape: {log_p_shifted.shape}, values: \\n {log_p_shifted}')\n",
    "    w = torch.exp(log_p_shifted)\n",
    "    #print(f'w shape: {w.shape}, values:\\n {w}')\n",
    "    w = w / w.sum()\n",
    "    #print(f'w after normalization shape: {w.shape}, values:\\n {w}')\n",
    "\n",
    "    # 2. broadcast weights onto grad tensor\n",
    "    while w.dim() < grad_p.dim():\n",
    "        w = w.unsqueeze(-1)                      # (M,1,1,...)\n",
    "\n",
    "    return (w * grad_p).sum(dim=0)               # same shape as grad slice\n",
    "\n",
    "\n",
    "\n",
    "def grad_theta_log_joint(z: torch.Tensor, theta: torch.Tensor, data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    #theta.requires_grad_(True)\n",
    "    n_samples = hparams.get('n_grad_mc_samples', 1)\n",
    "    theta_ = theta.clone().detach().requires_grad_(True)\n",
    "    log_density_samples = []\n",
    "    grad_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        g_soft = bernoulli_soft_gmat(z, hparams)\n",
    "        #print(f\"g_soft values: {g_soft}\")\n",
    "        g_hard = torch.bernoulli(g_soft)\n",
    "        #print(f\"g_hard values: {g_hard}\")\n",
    "\n",
    "        # tryign with gumbel to be consistent with grad z and gumbel mc acylci impelmentation\n",
    "        #g_soft = gumbel_soft_gmat(z, hparams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        log_lik_val = log_full_likelihood(data, g_hard, theta_, hparams)\n",
    "        theta_eff = theta_ * g_hard\n",
    "        log_theta_prior_val = log_theta_prior(theta_eff, hparams.get('theta_prior_sigma', 1.0))\n",
    "        #ll_grad, = torch.autograd.grad(log_lik_val, theta_, retain_graph=True)\n",
    "        #log_theta_prior_grad, = torch.autograd.grad(log_theta_prior_val, theta_ , retain_graph=True)\n",
    "        #print(f\"ll_grad shape: {ll_grad.shape}, values: {ll_grad}\")\n",
    "        #print(f\"log_theta_prior_grad shape: {log_theta_prior_grad.shape}, values: {log_theta_prior_grad}\")\n",
    "\n",
    "        current_log_density = log_lik_val + log_theta_prior_val\n",
    "        current_grad ,= torch.autograd.grad(current_log_density, theta_)\n",
    "        log_density_samples.append(current_log_density) \n",
    "\n",
    "        grad_samples.append(current_grad)\n",
    "    #print(f\" END OF Grad_theta mc_samples, iter number: {hparams.get('current_iteration',1)}  \\n\")\n",
    "\n",
    "    log_p_tensor = torch.stack(log_density_samples)\n",
    "    grad_p_tensor = torch.stack(grad_samples)\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    #if theta.grad is not None:\n",
    "    #    theta.grad.zero_()\n",
    "    #theta.requires_grad_(False)\n",
    "\n",
    "    grad =weighted_grad(log_p_tensor, grad_p_tensor)\n",
    "    #grad = stable_gradient_estimator(log_p_tensor, grad_p_tensor)\n",
    "    #print(f\"Grad_theta shape: {grad.shape}, values: \\n {grad}\")\n",
    "\n",
    "    return  grad.detach()\n",
    "\n",
    "\n",
    "def grad_log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "    grad_z = grad_z_log_joint_gumbel(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    #grad_z = grad_z_log_joint_score(params[\"z\"], params[\"theta\"].detach(), data, hparams)\n",
    "    grad_theta = grad_theta_log_joint(params[\"z\"].detach(), params[\"theta\"], data, hparams)\n",
    "    \n",
    "    return {\"z\": grad_z, \"theta\": grad_theta}\n",
    "\n",
    "def log_joint(params: Dict[str, torch.Tensor], data: Dict[str, Any], hparams: Dict[str, Any]) -> torch.Tensor:\n",
    "    hparams_updated = update_dibs_hparams(hparams, params[\"t\"].item())\n",
    "    z, theta = params['z'], params['theta']\n",
    "    d = z.shape[0]\n",
    "\n",
    "    g_soft = bernoulli_soft_gmat(z, hparams_updated)\n",
    "    log_lik = log_full_likelihood(data, g_soft, theta, hparams_updated)\n",
    "\n",
    "    log_prior_z_gaussian = torch.sum(Normal(0.0, hparams_updated['sigma_z']).log_prob(z))\n",
    "    expected_h_val = gumbel_acyclic_constr_mc(z, d, hparams_updated)\n",
    "    log_prior_z_acyclic = -hparams_updated['beta'] * expected_h_val\n",
    "    log_prior_z = log_prior_z_gaussian + log_prior_z_acyclic\n",
    "    \n",
    "    theta_eff = theta * g_soft\n",
    "    log_prior_theta = log_theta_prior(theta_eff, hparams_updated.get('theta_prior_sigma', 1.0))\n",
    "\n",
    "    if (hparams_updated['current_iteration'] > 850 and hparams_updated['current_iteration'] < 1200):\n",
    "        with torch.no_grad():\n",
    "            log_terms = {\n",
    "                \"log_lik\":      log_lik.item(),\n",
    "                \"z_prior_gauss\":log_prior_z_gaussian.item(),\n",
    "                \"z_prior_acyc\": log_prior_z_acyclic.item(),   # usually ≤ 0\n",
    "                \"theta_prior\":  log_prior_theta.item(),\n",
    "                \"log_joint\": log_lik + log_prior_theta + log_prior_z + log_prior_z_acyclic,\n",
    "                \"penalty\": -hparams_updated['beta'] * expected_h_val.item()\n",
    "            }\n",
    "        print(f\"[dbg] {log_terms}\")\n",
    "\n",
    "    \n",
    "    return log_lik + log_prior_z + log_prior_theta\n",
    "\n",
    "def update_dibs_hparams(hparams: Dict[str, Any], t_step: float) -> Dict[str, Any]:\n",
    "\n",
    "    hparams['beta'] = hparams['beta_base'] * t_step # linear \n",
    "\n",
    "    hparams['alpha'] = hparams['alpha_base'] * t_step  # linear slope 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hparams['current_iteration'] = t_step # Store current iteration\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def hard_gmat_from_z(z: torch.Tensor, alpha: float = 1.0) -> torch.Tensor:\n",
    "    s = scores(z, alpha)\n",
    "    return (s > 0).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing acyclic_constr function\n",
      "============================================================\n",
      "\n",
      "1. Testing small acyclic graph (d=3):\n",
      "   Acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0 (should be <= 0 for acyclic)\n",
      "\n",
      "2. Testing small cyclic graph (d=3):\n",
      "   Cyclic graph constraint: 0.011667\n",
      "   Expected: > 0 (penalizes cycles)\n",
      "\n",
      "3. Testing identity/no edges (d=4):\n",
      "   Identity matrix constraint: 5.765625\n",
      "   Expected: close to 0\n",
      "\n",
      "4. Testing large graph (d=12, eigenvalue path):\n",
      "   Large acyclic graph constraint: 0.000000\n",
      "   Expected: close to 0\n",
      "\n",
      "5. Testing large graph with potential eigenvalue issues (d=15):\n",
      "   Problematic graph constraint: 306.006714\n",
      "   Expected: large positive value (many cycles)\n",
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.042117\n",
      "   Gradient computed successfully: False\n",
      "   ERROR in gradient computation: 'NoneType' object has no attribute 'norm'\n",
      "\n",
      "7. Testing edge cases:\n",
      "   Very small values constraint: 0.0000000000\n",
      "   Large values constraint: 422.222290\n",
      "\n",
      "============================================================\n",
      "acyclic_constr testing complete\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120265/216125626.py:91: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
      "/tmp/ipykernel_120265/216125626.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "def test_acyclic_constr():\n",
    "    \"\"\"\n",
    "    Test the acyclic_constr function with various scenarios to debug potential issues\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing acyclic_constr function\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test case 1: Small acyclic graph (d <= 10)\n",
    "    print(\"\\n1. Testing small acyclic graph (d=3):\")\n",
    "    d = 3\n",
    "    g_acyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.3],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.0, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_acyclic = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"   Acyclic graph constraint: {h_acyclic.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0 (should be <= 0 for acyclic)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 2: Small cyclic graph\n",
    "    print(\"\\n2. Testing small cyclic graph (d=3):\")\n",
    "    g_cyclic = torch.tensor([\n",
    "        [0.0, 0.5, 0.0],\n",
    "        [0.0, 0.0, 0.7], \n",
    "        [0.3, 0.0, 0.0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_cyclic = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"   Cyclic graph constraint: {h_cyclic.item():.6f}\")\n",
    "        print(f\"   Expected: > 0 (penalizes cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 3: Identity matrix (no edges)\n",
    "    print(\"\\n3. Testing identity/no edges (d=4):\")\n",
    "    d = 4\n",
    "    g_identity = torch.eye(d, dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        h_identity = acyclic_constr(g_identity, d)\n",
    "        print(f\"   Identity matrix constraint: {h_identity.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "    \n",
    "    # Test case 4: Large graph (d > 10, triggers eigenvalue computation)\n",
    "    print(\"\\n4. Testing large graph (d=12, eigenvalue path):\")\n",
    "    d = 12\n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "    g_large = torch.randn(d, d) * 0.1\n",
    "    g_large = torch.triu(g_large, diagonal=1)  # Upper triangular (acyclic)\n",
    "    \n",
    "    try:\n",
    "        h_large = acyclic_constr(g_large, d)\n",
    "        print(f\"   Large acyclic graph constraint: {h_large.item():.6f}\")\n",
    "        print(f\"   Expected: close to 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR: {e}\")\n",
    "        \n",
    "    # Test case 5: Large graph that might cause eigenvalue issues\n",
    "    print(\"\\n5. Testing large graph with potential eigenvalue issues (d=15):\")\n",
    "    d = 15\n",
    "    g_problematic = torch.ones(d, d) * 0.5\n",
    "    g_problematic.fill_diagonal_(0)  # No self-loops\n",
    "    \n",
    "    try:\n",
    "        h_problematic = acyclic_constr(g_problematic, d)\n",
    "        print(f\"   Problematic graph constraint: {h_problematic.item():.6f}\")\n",
    "        print(f\"   Expected: large positive value (many cycles)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR (expected - should fallback to series): {e}\")\n",
    "    \n",
    "    # Test case 6: Check gradients work\n",
    "    print(\"\\n6. Testing gradient computation:\")\n",
    "    d = 4\n",
    "    g_test = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "    g_test.data.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_test = acyclic_constr(g_test, d)\n",
    "        h_test.backward()\n",
    "        print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "        print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient computation: {e}\")\n",
    "    \n",
    "    # Test case 7: Edge cases\n",
    "    print(\"\\n7. Testing edge cases:\")\n",
    "    \n",
    "    # Very small values\n",
    "    d = 3\n",
    "    g_small = torch.ones(d, d) * 1e-10\n",
    "    g_small.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_small = acyclic_constr(g_small, d)\n",
    "        print(f\"   Very small values constraint: {h_small.item():.10f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with small values: {e}\")\n",
    "    \n",
    "    # Very large values (might cause overflow)\n",
    "    g_large_vals = torch.ones(d, d) * 10.0\n",
    "    g_large_vals.fill_diagonal_(0)\n",
    "    \n",
    "    try:\n",
    "        h_large_vals = acyclic_constr(g_large_vals, d)\n",
    "        print(f\"   Large values constraint: {h_large_vals.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large values: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"acyclic_constr testing complete\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run the test\n",
    "test_acyclic_constr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Testing gradient computation:\n",
      "   Constraint value: -0.000009\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.013367\n",
      "\n",
      "6b. Testing gradient computation (alternative method):\n",
      "   Constraint value: 0.016230\n",
      "   Gradient computed successfully: True\n",
      "   Gradient norm: 2.014625\n",
      "\n",
      "6c. Testing gradient computation (simple test):\n",
      "   Constraint value: 0.020845\n",
      "   Gradient computed using autograd.grad: True\n",
      "   Gradient norm: 0.276399\n",
      "   Gradient shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test case 6: Check gradients work\n",
    "print(\"\\n6. Testing gradient computation:\")\n",
    "d = 4\n",
    "\n",
    "# Method 1: Create tensor properly to maintain leaf status\n",
    "g_test = torch.randn(d, d) * 0.1\n",
    "# Zero out diagonal elements without modifying .data\n",
    "mask = ~torch.eye(d, dtype=torch.bool)\n",
    "g_test = g_test * mask.float()\n",
    "g_test.requires_grad_(True)\n",
    "\n",
    "try:\n",
    "    h_test = acyclic_constr(g_test, d)\n",
    "    h_test.backward()\n",
    "    print(f\"   Constraint value: {h_test.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test.grad is not None}\")\n",
    "    if g_test.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None - tensor may not be leaf\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 2: Alternative approach using retain_grad()\n",
    "print(\"\\n6b. Testing gradient computation (alternative method):\")\n",
    "g_test2 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "g_test2.data.fill_diagonal_(0)\n",
    "g_test2.retain_grad()  # This ensures gradients are kept even for non-leaf tensors\n",
    "\n",
    "try:\n",
    "    h_test2 = acyclic_constr(g_test2, d)\n",
    "    h_test2.backward()\n",
    "    print(f\"   Constraint value: {h_test2.item():.6f}\")\n",
    "    print(f\"   Gradient computed successfully: {g_test2.grad is not None}\")\n",
    "    if g_test2.grad is not None:\n",
    "        print(f\"   Gradient norm: {g_test2.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"   Gradient is None\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")\n",
    "\n",
    "# Method 3: Test with a simple differentiable operation\n",
    "print(\"\\n6c. Testing gradient computation (simple test):\")\n",
    "g_test3 = torch.randn(d, d, requires_grad=True) * 0.1\n",
    "# Create off-diagonal matrix using multiplication\n",
    "off_diag_mask = 1.0 - torch.eye(d)\n",
    "g_test3_masked = g_test3 * off_diag_mask\n",
    "\n",
    "try:\n",
    "    h_test3 = acyclic_constr(g_test3_masked, d)\n",
    "    grad_g3 = torch.autograd.grad(h_test3, g_test3, retain_graph=False)[0]\n",
    "    print(f\"   Constraint value: {h_test3.item():.6f}\")\n",
    "    print(f\"   Gradient computed using autograd.grad: {grad_g3 is not None}\")\n",
    "    if grad_g3 is not None:\n",
    "        print(f\"   Gradient norm: {grad_g3.norm().item():.6f}\")\n",
    "        print(f\"   Gradient shape: {grad_g3.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ERROR in gradient computation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores z to prob and Bernouilli soft_gmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing scores and bernoulli_soft_gmat functions\n",
      "======================================================================\n",
      "\n",
      "1. Testing simple 2D case with known values:\n",
      "   Z shape: torch.Size([2, 3, 2])\n",
      "   Z:\n",
      "tensor([[[1.0000, 0.5000],\n",
      "         [0.0000, 1.0000],\n",
      "         [1.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 1.0000],\n",
      "         [1.0000, 0.5000],\n",
      "         [1.0000, 0.0000]]])\n",
      "   Scores shape: torch.Size([2, 2])\n",
      "   Scores:\n",
      "tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "   Manual score (0->1): 1.000000\n",
      "   Computed score (0->1): 1.000000\n",
      "   Manual score (1->0): 1.000000\n",
      "   Computed score (1->0): 1.000000\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "\n",
      "2. Testing bernoulli_soft_gmat:\n",
      "   Probabilities shape: torch.Size([2, 2])\n",
      "   Probabilities:\n",
      "tensor([[0.0000, 0.7311],\n",
      "        [0.7311, 0.0000]])\n",
      "   Manual prob (0->1): 0.731059\n",
      "   Computed prob (0->1): 0.731059\n",
      "   Manual prob (1->0): 0.731059\n",
      "   Computed prob (1->0): 0.731059\n",
      "   Diagonal elements (should be 0): tensor([0., 0.])\n",
      "   All probs in [0,1]: True\n",
      "\n",
      "3. Testing with different alpha values:\n",
      "   Alpha = 0.1:\n",
      "     Max score: 0.100000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.524979\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 1.0:\n",
      "     Max score: 1.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.731059\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 5.0:\n",
      "     Max score: 5.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.993307\n",
      "     Min prob: 0.000000\n",
      "   Alpha = 10.0:\n",
      "     Max score: 10.000000\n",
      "     Min score: 0.000000\n",
      "     Max prob: 0.999955\n",
      "     Min prob: 0.000000\n",
      "\n",
      "4. Testing gradient flow:\n",
      "   ERROR in gradient flow: 'NoneType' object has no attribute 'shape'\n",
      "\n",
      "5. Testing consistency between scores and probabilities:\n",
      "   Max difference between manual and direct computation: 0.0000000000\n",
      "   Are they approximately equal: True\n",
      "\n",
      "6. Testing edge cases:\n",
      "   Zero embeddings - scores: tensor([0.])\n",
      "   Zero embeddings - probs: tensor([0.0000, 0.5000])\n",
      "   Zero embeddings - all probs should be 0.5: False\n",
      "   Large embeddings - max score: 200.000000\n",
      "   Large embeddings - max prob: 1.000000\n",
      "   Large embeddings - are probs valid: True\n",
      "\n",
      "7. Testing batched operation:\n",
      "   Batch scores shape: torch.Size([2, 3, 3])\n",
      "   Expected shape: (2, 3, 3)\n",
      "   Shapes match: True\n",
      "\n",
      "======================================================================\n",
      "scores and bernoulli_soft_gmat testing complete\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120265/3757974893.py:113: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647789720/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n"
     ]
    }
   ],
   "source": [
    "def test_scores_and_bernoulli_soft_gmat():\n",
    "    \"\"\"\n",
    "    Test the scores and bernoulli_soft_gmat functions based on the mathematical formulation\n",
    "    from section 4.2 of the paper.\n",
    "    \n",
    "    Mathematical background:\n",
    "    - Z = [U, V] where U, V ∈ R^(k×d)\n",
    "    - scores should compute α * u_i^T v_j for all i,j\n",
    "    - bernoulli_soft_gmat should compute σ_α(u_i^T v_j) = 1/(1 + exp(-α * u_i^T v_j))\n",
    "    - Diagonal elements should be 0 (no self-loops)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Testing scores and bernoulli_soft_gmat functions\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test case 1: Simple 2D case with known values\n",
    "    print(\"\\n1. Testing simple 2D case with known values:\")\n",
    "    d, k = 2, 3\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Create simple Z = [U, V] with known values\n",
    "    U = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]], dtype=torch.float32)  # (k, d)\n",
    "    V = torch.tensor([[0.5, 1.0], [1.0, 0.5], [0.0, 0.0]], dtype=torch.float32)  # (k, d)\n",
    "    Z = torch.stack([U.T, V.T], dim=-1)  # (d, k, 2)\n",
    "    \n",
    "    print(f\"   Z shape: {Z.shape}\")\n",
    "    print(f\"   Z:\\n{Z}\")\n",
    "    \n",
    "    # Test scores function\n",
    "    try:\n",
    "        scores_result = scores(Z, alpha)\n",
    "        print(f\"   Scores shape: {scores_result.shape}\")\n",
    "        print(f\"   Scores:\\n{scores_result}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        u1, v1 = Z[0, :, 0], Z[0, :, 1]  # u1, v1 for node 0\n",
    "        u2, v2 = Z[1, :, 0], Z[1, :, 1]  # u2, v2 for node 1\n",
    "        \n",
    "        manual_score_01 = alpha * torch.dot(u1, v2)\n",
    "        manual_score_10 = alpha * torch.dot(u2, v1)\n",
    "        \n",
    "        print(f\"   Manual score (0->1): {manual_score_01.item():.6f}\")\n",
    "        print(f\"   Computed score (0->1): {scores_result[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual score (1->0): {manual_score_10.item():.6f}\")\n",
    "        print(f\"   Computed score (1->0): {scores_result[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(scores_result)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in scores: {e}\")\n",
    "    \n",
    "    # Test bernoulli_soft_gmat function\n",
    "    print(\"\\n2. Testing bernoulli_soft_gmat:\")\n",
    "    hparams = {\"alpha\": alpha}\n",
    "    \n",
    "    try:\n",
    "        probs = bernoulli_soft_gmat(Z, hparams)\n",
    "        print(f\"   Probabilities shape: {probs.shape}\")\n",
    "        print(f\"   Probabilities:\\n{probs}\")\n",
    "        \n",
    "        # Manual computation for verification\n",
    "        manual_prob_01 = torch.sigmoid(manual_score_01)\n",
    "        manual_prob_10 = torch.sigmoid(manual_score_10)\n",
    "        \n",
    "        print(f\"   Manual prob (0->1): {manual_prob_01.item():.6f}\")\n",
    "        print(f\"   Computed prob (0->1): {probs[0, 1].item():.6f}\")\n",
    "        print(f\"   Manual prob (1->0): {manual_prob_10.item():.6f}\")\n",
    "        print(f\"   Computed prob (1->0): {probs[1, 0].item():.6f}\")\n",
    "        \n",
    "        # Check diagonal is zero\n",
    "        print(f\"   Diagonal elements (should be 0): {torch.diag(probs)}\")\n",
    "        \n",
    "        # Check probabilities are in [0, 1]\n",
    "        print(f\"   All probs in [0,1]: {torch.all((probs >= 0) & (probs <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in bernoulli_soft_gmat: {e}\")\n",
    "    \n",
    "    # Test case 3: Test with different alpha values\n",
    "    print(\"\\n3. Testing with different alpha values:\")\n",
    "    alphas = [0.1, 1.0, 5.0, 10.0]\n",
    "    \n",
    "    for alpha_test in alphas:\n",
    "        hparams_test = {\"alpha\": alpha_test}\n",
    "        try:\n",
    "            scores_test = scores(Z, alpha_test)\n",
    "            probs_test = bernoulli_soft_gmat(Z, hparams_test)\n",
    "            \n",
    "            print(f\"   Alpha = {alpha_test}:\")\n",
    "            print(f\"     Max score: {scores_test.max().item():.6f}\")\n",
    "            print(f\"     Min score: {scores_test.min().item():.6f}\")\n",
    "            print(f\"     Max prob: {probs_test.max().item():.6f}\")\n",
    "            print(f\"     Min prob: {probs_test.min().item():.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR with alpha {alpha_test}: {e}\")\n",
    "    \n",
    "    # Test case 4: Test gradient flow\n",
    "    print(\"\\n4. Testing gradient flow:\")\n",
    "    d, k = 3, 4\n",
    "    Z_grad = torch.randn(d, k, 2, requires_grad=True) * 0.5\n",
    "    hparams_grad = {\"alpha\": 2.0}\n",
    "    \n",
    "    try:\n",
    "        scores_grad = scores(Z_grad, hparams_grad[\"alpha\"])\n",
    "        probs_grad = bernoulli_soft_gmat(Z_grad, hparams_grad)\n",
    "        \n",
    "        # Compute some loss and backpropagate\n",
    "        loss = torch.sum(probs_grad ** 2)\n",
    "        loss.backward()\n",
    "        \n",
    "        print(f\"   Z gradient shape: {Z_grad.grad.shape}\")\n",
    "        print(f\"   Z gradient norm: {Z_grad.grad.norm().item():.6f}\")\n",
    "        print(f\"   Loss value: {loss.item():.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in gradient flow: {e}\")\n",
    "    \n",
    "    # Test case 5: Test consistency between scores and probabilities\n",
    "    print(\"\\n5. Testing consistency between scores and probabilities:\")\n",
    "    d, k = 4, 3\n",
    "    Z_test = torch.randn(d, k, 2) * 0.3\n",
    "    alpha_test = 1.5\n",
    "    hparams_test = {\"alpha\": alpha_test}\n",
    "    \n",
    "    try:\n",
    "        scores_manual = scores(Z_test, alpha_test)\n",
    "        probs_from_scores = torch.sigmoid(scores_manual)\n",
    "        \n",
    "        # Zero out diagonal\n",
    "        diag_mask = 1.0 - torch.eye(d)\n",
    "        probs_from_scores = probs_from_scores * diag_mask\n",
    "        \n",
    "        probs_direct = bernoulli_soft_gmat(Z_test, hparams_test)\n",
    "        \n",
    "        # Check if they match\n",
    "        max_diff = torch.max(torch.abs(probs_from_scores - probs_direct))\n",
    "        print(f\"   Max difference between manual and direct computation: {max_diff.item():.10f}\")\n",
    "        print(f\"   Are they approximately equal: {torch.allclose(probs_from_scores, probs_direct, atol=1e-6)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in consistency test: {e}\")\n",
    "    \n",
    "    # Test case 6: Test edge cases\n",
    "    print(\"\\n6. Testing edge cases:\")\n",
    "    \n",
    "    # Zero embeddings\n",
    "    Z_zero = torch.zeros(3, 2, 2)\n",
    "    hparams_zero = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_zero = scores(Z_zero, 1.0)\n",
    "        probs_zero = bernoulli_soft_gmat(Z_zero, hparams_zero)\n",
    "        \n",
    "        print(f\"   Zero embeddings - scores: {scores_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - probs: {probs_zero.unique()}\")\n",
    "        print(f\"   Zero embeddings - all probs should be 0.5: {torch.allclose(probs_zero, torch.ones_like(probs_zero) * 0.5)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with zero embeddings: {e}\")\n",
    "    \n",
    "    # Very large embeddings (test numerical stability)\n",
    "    Z_large = torch.ones(2, 2, 2) * 10.0\n",
    "    hparams_large = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_large = scores(Z_large, 1.0)\n",
    "        probs_large = bernoulli_soft_gmat(Z_large, hparams_large)\n",
    "        \n",
    "        print(f\"   Large embeddings - max score: {scores_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - max prob: {probs_large.max().item():.6f}\")\n",
    "        print(f\"   Large embeddings - are probs valid: {torch.all((probs_large >= 0) & (probs_large <= 1))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR with large embeddings: {e}\")\n",
    "    \n",
    "    # Test case 7: Test batched operation\n",
    "    print(\"\\n7. Testing batched operation:\")\n",
    "    batch_size = 2\n",
    "    d, k = 3, 2\n",
    "    Z_batch = torch.randn(batch_size, d, k, 2) * 0.5\n",
    "    hparams_batch = {\"alpha\": 1.0}\n",
    "    \n",
    "    try:\n",
    "        scores_batch = scores(Z_batch, 1.0)\n",
    "        # Note: bernoulli_soft_gmat might not support batching directly\n",
    "        print(f\"   Batch scores shape: {scores_batch.shape}\")\n",
    "        print(f\"   Expected shape: ({batch_size}, {d}, {d})\")\n",
    "        print(f\"   Shapes match: {scores_batch.shape == (batch_size, d, d)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR in batch operation: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"scores and bernoulli_soft_gmat testing complete\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run the test\n",
    "test_scores_and_bernoulli_soft_gmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.015s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running tests for acyclic_constr ---\n",
      "\n",
      "--- Running tests for Graph Generation ---\n",
      "Large acyclic graph (d=12) H(G): 0.000000\n",
      "Graph with self-loop H(G): 1.370371\n",
      "Acyclic graph H(G): 0.000000\n",
      "Graph with 2-cycle H(G): 0.666667\n",
      "\n",
      "Calculated soft G-matrix:\n",
      "tensor([[0.0000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.0000]])\n",
      "Expected soft G-matrix:\n",
      "tensor([[0.5000, 0.9991, 1.0000],\n",
      "        [1.0000, 0.5000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.5000]])\n",
      "\n",
      "Calculated scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n",
      "Expected scores:\n",
      "tensor([[ 0.,  7., 11.],\n",
      "        [11.,  0., 51.],\n",
      "        [19., 55.,  0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standalone test cell for debug_notebook.ipynb\n",
    "#\n",
    "# To use this, you would typically have the functions available \n",
    "# in the same notebook or imported from your 'models/dibs.py' script.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import unittest\n",
    "import logging\n",
    "\n",
    "# --- Setup basic logger ---\n",
    "# This is to prevent errors if log.warning is called in acyclic_constr\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# --- Functions to be tested ---\n",
    "# Pasted here for stand-alone execution.\n",
    "# In a real scenario, you would import these from your scripts.\n",
    "\n",
    "def acyclic_constr(g: torch.Tensor, d: int) -> torch.Tensor:\n",
    "    \"\"\"H(G) from NOTEARS (Zheng et al.) with a series fallback for large *d*.\"\"\"\n",
    "    # Ensure g is a floating point tensor for matrix operations\n",
    "    g = g.float()\n",
    "    alpha = 1.0 / d\n",
    "    eye = torch.eye(d, device=g.device, dtype=g.dtype)\n",
    "    m = eye + alpha * g\n",
    "\n",
    "    # Using matrix_power for d <= 10 as it's generally stable for smaller matrices\n",
    "    if d <= 10:\n",
    "        return torch.trace(torch.linalg.matrix_power(m, d)) - d\n",
    "\n",
    "    # For larger d, eigenvalues are more efficient but can be numerically unstable\n",
    "    try:\n",
    "        # Eigenvalue decomposition is faster for large d\n",
    "        eigvals = torch.linalg.eigvals(m)\n",
    "        # The constraint is based on the sum of the d-th power of eigenvalues\n",
    "        return torch.sum(torch.real(eigvals ** d)) - d\n",
    "    except torch.linalg.LinAlgError:\n",
    "        # Fallback to series expansion if eigenvalue computation fails\n",
    "        # This is a less precise but more stable approximation\n",
    "        log.warning(f\"Eigenvalue computation failed for d={d}. Falling back to series expansion.\")\n",
    "        trace = torch.tensor(0.0, device=g.device, dtype=g.dtype)\n",
    "        p = eye.clone() # Start with identity matrix for power calculation\n",
    "        for k in range(1, min(d + 1, 20)): # Limit to 20 terms for practical purposes\n",
    "            p = p @ m\n",
    "            trace += torch.trace(p) / k\n",
    "        return trace\n",
    "\n",
    "def scores(z: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    \"\"\"Calculates the raw edge scores from latent embeddings.\"\"\"\n",
    "    # z has shape [d, k, 2]\n",
    "    # u and v have shape [d, k]\n",
    "    u, v = z[..., 0], z[..., 1]\n",
    "    \n",
    "    # einsum performs batch matrix multiplication of u and v.T\n",
    "    # 'ik,jk->ij' means: sum over k for each i and j\n",
    "    raw_scores = alpha * torch.einsum('ik,jk->ij', u, v)\n",
    "    \n",
    "    # Ensure no self-loops by masking the diagonal\n",
    "    d = z.shape[0]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=z.device, dtype=z.dtype)\n",
    "    \n",
    "    return raw_scores * diag_mask\n",
    "\n",
    "def bernoulli_soft_gmat(z: torch.Tensor, hparams: dict) -> torch.Tensor:\n",
    "    \"\"\"Generates a soft adjacency matrix using a Bernoulli parameterization.\"\"\"\n",
    "    # Get probabilities by applying a sigmoid to the raw scores\n",
    "    probs = torch.sigmoid(scores(z, hparams[\"alpha\"]))\n",
    "    \n",
    "    # The scores function already handles the diagonal masking, but as a safeguard:\n",
    "    d = probs.shape[-1]\n",
    "    diag_mask = 1.0 - torch.eye(d, device=probs.device, dtype=probs.dtype)\n",
    "    \n",
    "    return probs * diag_mask\n",
    "\n",
    "\n",
    "# --- Test Cases ---\n",
    "\n",
    "class TestAcyclicConstraint(unittest.TestCase):\n",
    "\n",
    "    def test_strictly_acyclic_graph(self):\n",
    "        \"\"\"Tests a graph with no cycles (a Directed Acyclic Graph).\"\"\"\n",
    "        g_acyclic = torch.tensor([[0., 1., 1.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_acyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_acyclic, d)\n",
    "        print(f\"Acyclic graph H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=5, msg=\"Acyclic graph should have H(G) = 0\")\n",
    "\n",
    "    def test_self_loop_cycle(self):\n",
    "        \"\"\"Tests a graph with a self-loop (the simplest cycle).\"\"\"\n",
    "        g_cyclic = torch.tensor([[1., 1., 0.], [0., 0., 1.], [0., 0., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with self-loop H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "\n",
    "    def test_two_node_cycle(self):\n",
    "        \"\"\"Tests a graph with a 2-cycle (A -> B, B -> A).\"\"\"\n",
    "        g_cyclic = torch.tensor([[0., 1., 0.], [1., 0., 0.], [0., 1., 0.]])\n",
    "        d = g_cyclic.shape[0]\n",
    "        h_val = acyclic_constr(g_cyclic, d)\n",
    "        print(f\"Graph with 2-cycle H(G): {h_val.item():.6f}\")\n",
    "        self.assertTrue(h_val.item() > 1e-4, msg=\"Cyclic graph should have H(G) > 0\")\n",
    "        \n",
    "    def test_large_graph_eigenvalue_path(self):\n",
    "        \"\"\"Tests the eigenvalue code path with a larger (d=12) acyclic graph.\"\"\"\n",
    "        d = 12\n",
    "        g_large_acyclic = torch.triu(torch.ones(d, d), diagonal=1)\n",
    "        h_val = acyclic_constr(g_large_acyclic, d)\n",
    "        print(f\"Large acyclic graph (d=12) H(G): {h_val.item():.6f}\")\n",
    "        self.assertAlmostEqual(h_val.item(), 0.0, places=4, msg=\"Large acyclic graph should have H(G) near 0\")\n",
    "\n",
    "\n",
    "class TestGraphGeneration(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up common variables for the tests.\"\"\"\n",
    "        self.d = 3  # Number of nodes\n",
    "        self.k = 2  # Latent dimension\n",
    "        # Latent variable Z = [U, V]\n",
    "        self.z = torch.arange(self.d * self.k * 2, dtype=torch.float32).view(self.d, self.k, 2)\n",
    "        # z will be:\n",
    "        # [[[ 0,  1], [ 2,  3]],\n",
    "        #  [[ 4,  5], [ 6,  7]],\n",
    "        #  [[ 8,  9], [10, 11]]]\n",
    "        self.alpha = 0.5\n",
    "        self.hparams = {\"alpha\": self.alpha}\n",
    "\n",
    "    def test_scores_calculation(self):\n",
    "        \"\"\"Tests the bilinear score calculation G_ij = alpha * u_i^T v_j.\"\"\"\n",
    "        u = self.z[..., 0] # [[[0, 2], [4, 6], [8, 10]]]\n",
    "        v = self.z[..., 1] # [[[1, 3], [5, 7], [9, 11]]]\n",
    "        \n",
    "        # Manually calculate expected scores\n",
    "        expected_scores = self.alpha * torch.matmul(u, v.T)\n",
    "        # Set diagonal to zero\n",
    "        expected_scores.fill_diagonal_(0)\n",
    "        \n",
    "        # Get scores from function\n",
    "        s = scores(self.z, self.alpha)\n",
    "        print(f\"\\nCalculated scores:\\n{s}\")\n",
    "        print(f\"Expected scores:\\n{expected_scores}\")\n",
    "        \n",
    "        self.assertTrue(torch.allclose(s, expected_scores), \"Scores do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(s) == 0), \"Diagonal of scores matrix should be zero.\")\n",
    "\n",
    "    def test_bernoulli_soft_gmat(self):\n",
    "        \"\"\"Tests the sigmoid transformation of scores to get probabilities.\"\"\"\n",
    "        # Calculate scores first\n",
    "        s = scores(self.z, self.alpha)\n",
    "        \n",
    "        # Manually calculate expected probabilities\n",
    "        expected_probs = torch.sigmoid(s)\n",
    "        \n",
    "        # Get probabilities from function\n",
    "        g_soft = bernoulli_soft_gmat(self.z, self.hparams)\n",
    "        print(f\"\\nCalculated soft G-matrix:\\n{g_soft}\")\n",
    "        print(f\"Expected soft G-matrix:\\n{expected_probs}\")\n",
    "        expected_probs.fill_diagonal_(0)\n",
    "        self.assertTrue(torch.allclose(g_soft, expected_probs), \"Soft G-matrix probabilities do not match expected values.\")\n",
    "        # Check that diagonal is exactly zero\n",
    "        self.assertTrue(torch.all(torch.diag(g_soft) == 0), \"Diagonal of soft G-matrix should be zero.\")\n",
    "\n",
    "\n",
    "# --- Running the tests ---\n",
    "# This allows running the tests directly from the cell.\n",
    "suite = unittest.TestSuite()\n",
    "print(\"--- Running tests for acyclic_constr ---\")\n",
    "suite.addTest(unittest.makeSuite(TestAcyclicConstraint))\n",
    "print(\"\\n--- Running tests for Graph Generation ---\")\n",
    "suite.addTest(unittest.makeSuite(TestGraphGeneration))\n",
    "\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_gaussian_likelihood_manual(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    residuals = x - pred_mean\n",
    "    #old incorrect log_prob = -0.5 * (np.log(2 * np.pi) -  (1/2)* torch.log(sigma_tensor**2) -  0.5*(residuals / sigma_tensor) ** 2) old\n",
    "    log_prob = -0.5 * (torch.log(2 * torch.pi * sigma_tensor**2)) - 0.5 * ((residuals / sigma_tensor)**2)\n",
    "    #normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    #log_prob = normal_dist.log_prob(x)\n",
    "\n",
    "    return torch.sum(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 09:51:30,528 - INFO - Generated data with 200 samples and 5 features.\n",
      "2025-06-27 09:51:30,529 - INFO - True theta:\n",
      "tensor([[ 2.5000, -1.0000,  3.3000,  0.0000, -4.1000]])\n",
      "2025-06-27 09:51:30,530 - INFO - Starting gradient ascent to maximize log joint probability...\n",
      "2025-06-27 09:51:30,532 - INFO - Epoch 000 | Log Joint: -656612.5625 | Loss: 656612.5625\n",
      "2025-06-27 09:51:30,561 - INFO - Epoch 050 | Log Joint: -501612.7812 | Loss: 501612.7812\n",
      "2025-06-27 09:51:30,586 - INFO - Epoch 100 | Log Joint: -378928.0938 | Loss: 378928.0938\n",
      "2025-06-27 09:51:30,610 - INFO - Epoch 150 | Log Joint: -284216.7500 | Loss: 284216.7500\n",
      "2025-06-27 09:51:30,636 - INFO - Epoch 200 | Log Joint: -211949.2500 | Loss: 211949.2500\n",
      "2025-06-27 09:51:30,663 - INFO - Epoch 250 | Log Joint: -157169.8281 | Loss: 157169.8281\n",
      "2025-06-27 09:51:30,690 - INFO - Epoch 300 | Log Joint: -115754.8281 | Loss: 115754.8281\n",
      "2025-06-27 09:51:30,716 - INFO - Epoch 350 | Log Joint: -84482.3047 | Loss: 84482.3047\n",
      "2025-06-27 09:51:30,740 - INFO - Epoch 400 | Log Joint: -60937.7461 | Loss: 60937.7461\n",
      "2025-06-27 09:51:30,765 - INFO - Epoch 450 | Log Joint: -43330.7422 | Loss: 43330.7422\n",
      "2025-06-27 09:51:30,790 - INFO - Epoch 500 | Log Joint: -30309.3086 | Loss: 30309.3086\n",
      "2025-06-27 09:51:30,791 - INFO - Training finished.\n",
      "2025-06-27 09:51:30,792 - INFO - Learned theta:\n",
      "tensor([[ 2.5497, -0.9796,  2.3753, -0.1633, -2.7242]])\n",
      "2025-06-27 09:51:30,792 - INFO - True theta:\n",
      "tensor([[ 2.5000, -1.0000,  3.3000,  0.0000, -4.1000]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Using a logger is good practice for research code instead of print statements.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Model & Probability Functions ---\n",
    "\n",
    "def log_gaussian_likelihood(x: torch.Tensor, pred_mean: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the log-likelihood using the stable torch.distributions module.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): The observed data (ground truth).\n",
    "        pred_mean (torch.Tensor): The model's predictions (the mean of the Gaussian).\n",
    "        sigma (float): The standard deviation of the Gaussian.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The total log-likelihood of the data.\n",
    "    \"\"\"\n",
    "    # Create a tensor for sigma with the correct dtype and device\n",
    "    sigma_tensor = torch.tensor(sigma, dtype=pred_mean.dtype, device=pred_mean.device)\n",
    "    \n",
    "    # Create a Normal distribution object. `loc` is the mean, `scale` is the standard deviation.\n",
    "    normal_dist = Normal(loc=pred_mean, scale=sigma_tensor)\n",
    "    \n",
    "    # Calculate the log probability of the data 'x' under this distribution.\n",
    "    # This is done element-wise, so we sum them to get the total log probability.\n",
    "    log_prob = normal_dist.log_prob(x).sum()\n",
    "    \n",
    "    return log_prob\n",
    "\n",
    "def log_gaussian_prior(theta: torch.Tensor, sigma_prior: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the log-prior probability of the weights `theta`.\n",
    "    Assumes a Gaussian prior centered at 0.\n",
    "    \n",
    "    Args:\n",
    "        theta (torch.Tensor): The model parameters (weights).\n",
    "        sigma_prior (float): The standard deviation of the Gaussian prior.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The log-prior probability of the parameters.\n",
    "    \"\"\"\n",
    "    # Using the stable distributions module is best practice.\n",
    "    prior_dist = Normal(loc=0.0, scale=sigma_prior)\n",
    "    return prior_dist.log_prob(theta).sum()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the experiment.\"\"\"\n",
    "    # --- 1. Generate Synthetic Data ---\n",
    "    # We will model a simple linear relationship: y = X @ theta_true + noise\n",
    "    n_samples = 200\n",
    "    n_features = 5\n",
    "    noise_std_dev = 0.1  # True standard deviation of the noise in data generation\n",
    "\n",
    "    # Create a \"true\" set of weights we want our model to discover\n",
    "    theta_true = torch.tensor([2.5, -1.0, 3.3, 0.0, -4.1]).unsqueeze(1)\n",
    "    \n",
    "    # Generate random input data X\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Generate the output y using the linear model and add Gaussian noise\n",
    "    noise = torch.randn(n_samples, 1) * noise_std_dev\n",
    "    y = X @ theta_true + noise\n",
    "\n",
    "    logging.info(f\"Generated data with {n_samples} samples and {n_features} features.\")\n",
    "    logging.info(f\"True theta:\\n{theta_true.T}\")\n",
    "\n",
    "    # --- 2. Setup the Model and Optimizer ---\n",
    "    # Initialize model parameters `theta` randomly. This is what we will learn.\n",
    "    theta_model = torch.randn(n_features, 1, requires_grad=True)\n",
    "\n",
    "    # Setup the optimizer. We use Adam, a standard choice.\n",
    "    # We pass the parameter we want to optimize: `theta_model`.\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.Adam([theta_model], lr=learning_rate)\n",
    "    \n",
    "    # --- 3. Perform Gradient Ascent ---\n",
    "    # We want to *maximize* the log joint. PyTorch optimizers *minimize* a loss.\n",
    "    # So, our \"loss\" will be the *negative* log joint probability.\n",
    "    epochs = 501\n",
    "    log_joint_history = []\n",
    "    \n",
    "    logging.info(\"Starting gradient ascent to maximize log joint probability...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Zero out gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Forward Pass ---\n",
    "        # 1. Get the model's prediction for y\n",
    "        y_pred = X @ theta_model\n",
    "        \n",
    "        # 2. Calculate the log likelihood using the provided function\n",
    "        # We pass the true noise std dev for this test.\n",
    "        log_likelihood = log_gaussian_likelihood(y, y_pred, sigma=noise_std_dev)\n",
    "        #log_likelihood_manual = log_gaussian_likelihood_manual(y, y_pred, sigma=noise_std_dev)\n",
    "        # print the difference between the two\n",
    "        #logging.info(f\"Difference between the two log likelihoods: {log_likelihood - log_likelihood_manual}\")\n",
    "        \n",
    "        # 3. Calculate the log prior on the weights\n",
    "        log_prior = log_gaussian_prior(theta_model)\n",
    "        \n",
    "        # 4. Calculate the log joint probability (the objective we want to maximize)\n",
    "        log_joint = log_likelihood + log_prior\n",
    "        \n",
    "        # --- Backward Pass ---\n",
    "        # 5. Define the loss as the negative log joint\n",
    "        loss = -log_joint\n",
    "        \n",
    "        # 6. Backpropagate to compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 7. Update the parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Logging ---\n",
    "        log_joint_history.append(log_joint.item())\n",
    "        if epoch % 50 == 0:\n",
    "            logging.info(f\"Epoch {epoch:03d} | Log Joint: {log_joint.item():.4f} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    logging.info(\"Training finished.\")\n",
    "    logging.info(f\"Learned theta:\\n{theta_model.T.detach()}\")\n",
    "    logging.info(f\"True theta:\\n{theta_true.T}\")\n",
    "    \n",
    "    # --- 4. Plot the results ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_joint_history)\n",
    "    plt.title(\"Log Joint Probability during Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Log Joint P(y, θ | X)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dibs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
